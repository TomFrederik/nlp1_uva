{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "NLP1 2020 Practical 2 (student version)",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptMZZMQlfn7c"
      },
      "source": [
        "------\n",
        "**You cannot apply any changes to this file, so please make sure to save it on your Google Colab drive or download it as a .ipynb file.**\n",
        "\n",
        "------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jILqpPLlE9r0"
      },
      "source": [
        "# Practical 2: Representing Sentences with Neural Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JXOZ5uhQ8Qq"
      },
      "source": [
        "In this second practical, we will train neural network models to obtain sentence representations. We can then use these sentence representations for a downstream task such as sentiment classification. \n",
        "\n",
        "In this notebook, we will help you to develop models for your experiments. But this time, next to completing the notebook, **you are expected to write a four-page scientific report with your findings**. Please still submit the notebook together with your scientific report so that we can reproduce your experiments. (Note: if you find it useful, you can split this notebook into multiple notebooks. If you do so, keep it mind that it should be possible for your TAs to reproduce the entire content of the notebooks without having to ask for clarifications or to copy and paste functions from one sub-notebook to another.)\n",
        "\n",
        "**Important!** The main purpose of this lab is for you to learn how to answer research questions by experimenting and then writing a scientific report.\n",
        "So you will be *judged by the quality of your report* but will lose points if your experiments are not reproducible.\n",
        "You can find the requirements for the report at the end of this notebook.\n",
        "\n",
        "\n",
        "### Data set\n",
        "We will use the [Stanford Sentiment Treebank](https://nlp.stanford.edu/sentiment/) (SST), which provides sentences, their binary tree structure, and fine-grained sentiment scores.\n",
        "This dataset is different from the one we used in the first practical. \n",
        "In Practical 1, a review consisted of several sentences, and we had one sentiment score for the whole review. Now, a review consists of a single sentence, and we have a sentiment score for each node in the binary tree that makes up the sentence, including the root node (i.e., we still have an overall sentiment score for the entire review). We will look at an example below.\n",
        "\n",
        "In the first part of this practical we will only make use of the sentence tokens whereas in the second part we will also exploit the tree structure that is provided by the SST.\n",
        "\n",
        "We will cover the following approaches:\n",
        "\n",
        "- Bag-of-words (BOW)\n",
        "- Continuous bag-of-words (CBOW)\n",
        "- Deep continuous bag-of-words (Deep CBOW)\n",
        "- LSTM\n",
        "- Tree-LSTM\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbNKef3lymaj"
      },
      "source": [
        "## Preparing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jxTkpg59FlU"
      },
      "source": [
        "Let's first download the data set and take a look."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZp53HmMP3F2"
      },
      "source": [
        "import re\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('default')"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TovFkDTgE_d6",
        "outputId": "241e20d3-6914-42a8-f246-5be88e695fa3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# un-comment if you have not downloaded the data yet\n",
        "!wget http://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip\n",
        "!unzip trainDevTestTrees_PTB.zip"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-01 15:14:23--  http://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip [following]\n",
            "--2020-12-01 15:14:23--  https://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 789539 (771K) [application/zip]\n",
            "Saving to: ‘trainDevTestTrees_PTB.zip’\n",
            "\n",
            "trainDevTestTrees_P 100%[===================>] 771.03K  2.53MB/s    in 0.3s    \n",
            "\n",
            "2020-12-01 15:14:23 (2.53 MB/s) - ‘trainDevTestTrees_PTB.zip’ saved [789539/789539]\n",
            "\n",
            "Archive:  trainDevTestTrees_PTB.zip\n",
            "   creating: trees/\n",
            "  inflating: trees/dev.txt           \n",
            "  inflating: trees/test.txt          \n",
            "  inflating: trees/train.txt         \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IpAphkBO5eW"
      },
      "source": [
        "# this function reads in a textfile and fixes an issue with \"\\\\\"\n",
        "def filereader(path): \n",
        "  with open(path, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "      yield line.strip().replace(\"\\\\\",\"\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yP_jpquiprH8"
      },
      "source": [
        "Let's look at a data point. It is a **flattened binary tree**, with sentiment scores at every node, and words as the leaves (or *terminal nodes*)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylkIopm0QJML",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fb24e57-f060-4def-f491-1b60aff657a4"
      },
      "source": [
        "s = next(filereader(\"trees/dev.txt\"))\n",
        "print(s)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3 (2 It) (4 (4 (2 's) (4 (3 (2 a) (4 (3 lovely) (2 film))) (3 (2 with) (4 (3 (3 lovely) (2 performances)) (2 (2 by) (2 (2 (2 Buy) (2 and)) (2 Accorsi))))))) (2 .)))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_U7HTFwdrWt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e26547c5-bfd3-4e6f-efe8-5640dc8de210"
      },
      "source": [
        "# We can use NLTK to better visualise the tree structure of the sentence\n",
        "from nltk import Tree\n",
        "from nltk.treeprettyprinter import TreePrettyPrinter\n",
        "tree = Tree.fromstring(s)\n",
        "print(TreePrettyPrinter(tree))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              3                                                                     \n",
            "  ____________|____________________                                                  \n",
            " |                                 4                                                \n",
            " |        _________________________|______________________________________________   \n",
            " |       4                                                                        | \n",
            " |    ___|______________                                                          |  \n",
            " |   |                  4                                                         | \n",
            " |   |         _________|__________                                               |  \n",
            " |   |        |                    3                                              | \n",
            " |   |        |               _____|______________________                        |  \n",
            " |   |        |              |                            4                       | \n",
            " |   |        |              |            ________________|_______                |  \n",
            " |   |        |              |           |                        2               | \n",
            " |   |        |              |           |                 _______|___            |  \n",
            " |   |        3              |           |                |           2           | \n",
            " |   |    ____|_____         |           |                |        ___|_____      |  \n",
            " |   |   |          4        |           3                |       2         |     | \n",
            " |   |   |     _____|___     |      _____|_______         |    ___|___      |     |  \n",
            " 2   2   2    3         2    2     3             2        2   2       2     2     2 \n",
            " |   |   |    |         |    |     |             |        |   |       |     |     |  \n",
            " It  's  a  lovely     film with lovely     performances  by Buy     and Accorsi  . \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekAWKsji9t93"
      },
      "source": [
        "The sentiment scores range from 0 (very negative) to 5 (very positive). Again, as you can see, every node in the tree is labeled with a sentiment score. For now, we will only use the score at the **root node**, i.e., the sentiment score for the complete sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKynLm0xPKr2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a62848c0-e3e6-49ae-d097-1b7dbafc20c2"
      },
      "source": [
        "# Let's first make a function that extracts the tokens (the leaves).\n",
        "\n",
        "def tokens_from_treestring(s):\n",
        "  \"\"\"extract the tokens from a sentiment tree\"\"\"\n",
        "  return re.sub(r\"\\([0-9] |\\)\", \"\", s).split()\n",
        " \n",
        "# let's try it on our example tree\n",
        "tokens = tokens_from_treestring(s)\n",
        "print(tokens)\n",
        "print(len(tokens))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['It', \"'s\", 'a', 'lovely', 'film', 'with', 'lovely', 'performances', 'by', 'Buy', 'and', 'Accorsi', '.']\n",
            "13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8vFkeqN-NLP"
      },
      "source": [
        "> *Warning: you could also parse a treestring using NLTK and ask it to return the leaves, but there seems to be an issue with NLTK not always correctly parsing the input, so do not rely on it.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Akr9K_Mv4dym"
      },
      "source": [
        "# We will also need the following function, but you can ignore this for now.\n",
        "# It is explained later on.\n",
        "\n",
        "SHIFT = 0\n",
        "REDUCE = 1\n",
        "\n",
        "\n",
        "def transitions_from_treestring(s):\n",
        "  s = re.sub(\"\\([0-5] ([^)]+)\\)\", \"0\", s)\n",
        "  s = re.sub(\"\\)\", \" )\", s)\n",
        "  s = re.sub(\"\\([0-4] \", \"\", s)\n",
        "  s = re.sub(\"\\([0-4] \", \"\", s)\n",
        "  s = re.sub(\"\\)\", \"1\", s)\n",
        "  return list(map(int, s.split()))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNtPdlwPgRat",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fc8413c-c334-4c6d-f096-485348cedc3c"
      },
      "source": [
        "# Now let's first see how large our data sets are.\n",
        "for path in (\"trees/train.txt\", \"trees/dev.txt\", \"trees/test.txt\"):\n",
        "  print(\"{:16s} {:4d}\".format(path, sum(1 for _ in filereader(path))))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trees/train.txt  8544\n",
            "trees/dev.txt    1101\n",
            "trees/test.txt   2210\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HexlSqTR_UrY"
      },
      "source": [
        "You can see that the number of sentences is not very large. That's probably because the data set required so much manual annotation. However, it is large enough to train a neural network on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfRjelOcsXuC"
      },
      "source": [
        "It will be useful to store each data example in an `Example` object,\n",
        "containing everything that we may need for each data point.\n",
        "It will contain the tokens, the tree, the top-level sentiment label, and \n",
        "the transitions (explained later)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4I07Hb_-q8wg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cf338f4-ab22-4e20-b874-6f91d6b8ba66"
      },
      "source": [
        "from collections import namedtuple\n",
        "from nltk import Tree\n",
        "\n",
        "# A simple way to define a class is using namedtuple.\n",
        "Example = namedtuple(\"Example\", [\"tokens\", \"tree\", \"label\", \"transitions\"])\n",
        "\n",
        "   \n",
        "def examplereader(path, lower=False):\n",
        "  \"\"\"Returns all examples in a file one by one.\"\"\"\n",
        "  for line in filereader(path):\n",
        "    line = line.lower() if lower else line\n",
        "    tokens = tokens_from_treestring(line)\n",
        "    tree = Tree.fromstring(line)  # use NLTK's Tree\n",
        "    label = int(line[1])\n",
        "    trans = transitions_from_treestring(line)\n",
        "    yield Example(tokens=tokens, tree=tree, label=label, transitions=trans)\n",
        "  \n",
        "\n",
        "# Let's load the data into memory.\n",
        "LOWER = False  # we will keep the original casing\n",
        "train_data = list(examplereader(\"trees/train.txt\", lower=LOWER))\n",
        "dev_data = list(examplereader(\"trees/dev.txt\", lower=LOWER))\n",
        "test_data = list(examplereader(\"trees/test.txt\", lower=LOWER))\n",
        "\n",
        "print(\"train\", len(train_data))\n",
        "print(\"dev\", len(dev_data))\n",
        "print(\"test\", len(test_data))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train 8544\n",
            "dev 1101\n",
            "test 2210\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KM0bDyeVZtP"
      },
      "source": [
        "Let's check out an `Example` object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8mwcaZwxP1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8ed223e-6986-4846-bf05-68b11ebaaefd"
      },
      "source": [
        "example = dev_data[0]\n",
        "print(\"First example:\", example)\n",
        "print(\"First example tokens:\", example.tokens)\n",
        "print(\"First example label:\",  example.label)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First example: Example(tokens=['It', \"'s\", 'a', 'lovely', 'film', 'with', 'lovely', 'performances', 'by', 'Buy', 'and', 'Accorsi', '.'], tree=Tree('3', [Tree('2', ['It']), Tree('4', [Tree('4', [Tree('2', [\"'s\"]), Tree('4', [Tree('3', [Tree('2', ['a']), Tree('4', [Tree('3', ['lovely']), Tree('2', ['film'])])]), Tree('3', [Tree('2', ['with']), Tree('4', [Tree('3', [Tree('3', ['lovely']), Tree('2', ['performances'])]), Tree('2', [Tree('2', ['by']), Tree('2', [Tree('2', [Tree('2', ['Buy']), Tree('2', ['and'])]), Tree('2', ['Accorsi'])])])])])])]), Tree('2', ['.'])])]), label=3, transitions=[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1])\n",
            "First example tokens: ['It', \"'s\", 'a', 'lovely', 'film', 'with', 'lovely', 'performances', 'by', 'Buy', 'and', 'Accorsi', '.']\n",
            "First example label: 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WDSprDBVcr-"
      },
      "source": [
        "#### Vocabulary \n",
        "A first step in most NLP tasks is collecting all the word types that appear in the data into a vocabulary, and counting the frequency of their occurrences. On the one hand, this will give us an overview of the word distribution of the data set (what are the most frequent words, how many rare words are there, ...). On the other hand, we will also use the vocabulary to map each word to a unique numeric ID, which is a more handy index than a string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvNgKx7usRSt"
      },
      "source": [
        "# Here we first define a class that can map a word to an ID (w2i)\n",
        "# and back (i2w).\n",
        "\n",
        "from collections import Counter, OrderedDict, defaultdict\n",
        "\n",
        "\n",
        "class OrderedCounter(Counter, OrderedDict):\n",
        "  \"\"\"Counter that remembers the order elements are first seen\"\"\"\n",
        "  def __repr__(self):\n",
        "    return '%s(%r)' % (self.__class__.__name__,\n",
        "                      OrderedDict(self))\n",
        "  def __reduce__(self):\n",
        "    return self.__class__, (OrderedDict(self),)\n",
        "\n",
        "\n",
        "class Vocabulary:\n",
        "  \"\"\"A vocabulary, assigns IDs to tokens\"\"\"\n",
        "  \n",
        "  def __init__(self):\n",
        "    self.freqs = OrderedCounter()\n",
        "    self.w2i = {}\n",
        "    self.i2w = []\n",
        "\n",
        "  def count_token(self, t):\n",
        "    self.freqs[t] += 1\n",
        "    \n",
        "  def add_token(self, t):\n",
        "    self.w2i[t] = len(self.w2i)\n",
        "    self.i2w.append(t)    \n",
        "    \n",
        "  def build(self, min_freq=0):\n",
        "    '''\n",
        "    min_freq: minimum number of occurrences for a word to be included  \n",
        "              in the vocabulary\n",
        "    '''\n",
        "    self.add_token(\"<unk>\")  # reserve 0 for <unk> (unknown words)\n",
        "    self.add_token(\"<pad>\")  # reserve 1 for <pad> (discussed later)   \n",
        "    \n",
        "    tok_freq = list(self.freqs.items())\n",
        "    tok_freq.sort(key=lambda x: x[1], reverse=True)\n",
        "    for tok, freq in tok_freq:\n",
        "      if freq >= min_freq:\n",
        "        self.add_token(tok)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOvkH_llVsoW"
      },
      "source": [
        "The vocabulary has by default an `<unk>` token and a `<pad>` token. The `<unk>` token is reserved for all words which do not appear in the training data (and for which, therefore, we cannot learn word representations). The function of the `<pad>` token will be explained later.\n",
        "\n",
        "\n",
        "Let's build the vocabulary!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwGQgQQBNUSq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86a3f656-3882-4274-c9e9-c076cdd8b93b"
      },
      "source": [
        "# This process should be deterministic and should have the same result \n",
        "# if run multiple times on the same data set.\n",
        "\n",
        "v = Vocabulary()\n",
        "for data_set in (train_data,):\n",
        "  for ex in data_set:\n",
        "    for token in ex.tokens:\n",
        "      v.count_token(token)\n",
        "\n",
        "v.build()\n",
        "print(\"Vocabulary size:\", len(v.w2i))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size: 18280\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UNIedPrPdCw"
      },
      "source": [
        "Let's have a closer look at the properties of our vocabulary. Having a good idea of what it is like can facilitate data analysis and debugging later on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJyuogmh0CA7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4041907-2e92-462c-cc85-72802c51126a"
      },
      "source": [
        "# What is the ID for \"century?\"\n",
        "print(v.w2i['century'])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1973\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8OkPQ8Zv-rI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b5a4664-e3d3-4e9f-8e7d-dd01766cde6d"
      },
      "source": [
        "# What are the first 10 words in the vocabulary (based on their IDs)?\n",
        "print([v.i2w[i] for i in range(10)])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<unk>', '<pad>', '.', ',', 'the', 'and', 'a', 'of', 'to', \"'s\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmXwu02lOLWI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5cd1a99-c69e-457d-cb01-baa7cda1a22b"
      },
      "source": [
        "# What are the 10 most common words?\n",
        "print([w for (w,freq) in Counter(v.freqs).most_common(10)])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['.', ',', 'the', 'and', 'a', 'of', 'to', \"'s\", 'is', 'that']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__NDPaCeOT_m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa7b3387-e23f-4b42-bb5e-522e5ed3ea9b"
      },
      "source": [
        "# And how many words are there with frequency 1?\n",
        "# (A fancy name for these is hapax legomena.)\n",
        "print(len([w for w in v.freqs if v.freqs[w]==1]))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9543\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKHocugctZGM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1faf020b-2bf7-4d3b-8e70-4455c9f75ec7"
      },
      "source": [
        "# Finally 20 random words from the vocabulary.\n",
        "# This is a simple way to get a feeling for the data. \n",
        "# You could use the `choice` function from the already imported `random` package\n",
        "print([random.choice(list(v.w2i.keys())) for _ in range(20)])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Performances', 'hungry', 'straight-to-video', 'Terrible', 'communicating', 'investigator', 'uppity', 'frustratingly', 'Goldbacher', 'Returning', 'Wow', 'military', 'thesis', 'Everlyn', 'sense', 'drizzle', 'Cliffhanger', 'participant', 'Sorcerer', 'emotionally']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGWaZahKV_dH"
      },
      "source": [
        "#### Sentiment label vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmTC-rvQelpl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c52778f-d13a-4583-cdb8-7a749aff6b2a"
      },
      "source": [
        "# Now let's map the sentiment labels 0-4 to a more readable form\n",
        "i2t = [\"very negative\", \"negative\", \"neutral\", \"positive\", \"very positive\"]\n",
        "print(i2t)\n",
        "print(i2t[4])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['very negative', 'negative', 'neutral', 'positive', 'very positive']\n",
            "very positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7UI26DP2dr2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fbfa54a-06fb-4b27-9daf-d8e3e7a12ddb"
      },
      "source": [
        "# And let's also create the opposite mapping.\n",
        "# We won't use a Vocabulary for this (although we could), since the labels\n",
        "# are already numeric.\n",
        "t2i = OrderedDict({p : i for p, i in zip(i2t, range(len(i2t)))})\n",
        "print(t2i)\n",
        "print(t2i['very positive'])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OrderedDict([('very negative', 0), ('negative', 1), ('neutral', 2), ('positive', 3), ('very positive', 4)])\n",
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0067ax54-rd"
      },
      "source": [
        "## PyTorch\n",
        "\n",
        "In Colab notebooks, the last available version of PyTorch is already installed.The current stable version is 1.7.\n",
        "\n",
        "*For installing PyTorch in your own computer, follow the instructions on [pytorch.org](pytorch.org) instead. This is for Google Colab only.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKQMGtkR5KWr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f9c4523-5f3a-488e-e2d7-3075cda40415"
      },
      "source": [
        "import torch\n",
        "print(\"Using torch\", torch.__version__) # should say 1.7.0+cu101"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using torch 1.7.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnvPcd_E1xH8"
      },
      "source": [
        "# Let's also import torch.nn, a PyTorch package that  \n",
        "# makes building neural networks more convenient.\n",
        "from torch import nn"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYt8uTyGCKc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01784123-4911-41bf-be15-b6d9d7ce734d"
      },
      "source": [
        "# PyTorch can run on CPU or on Nvidia GPU (video card) using CUDA\n",
        "# This cell selects the GPU if one is available.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2d1VMOOYx1Bw"
      },
      "source": [
        "# Seed manually to make runs reproducible\n",
        "# You need to set this again if you do multiple runs of the same model\n",
        "torch.manual_seed(0)\n",
        "\n",
        "random.seed(0)\n",
        "\n",
        "# When running on the CuDNN backend two further options must be set for reproducibility\n",
        "if torch.cuda.is_available():\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWBTzkuE3CtZ"
      },
      "source": [
        "# BOW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBAjYYySOA5W"
      },
      "source": [
        "Our first model is a rather simple neural **bag-of-words (BOW) model**.\n",
        "Unlike the bag-of-words model that you used in the previous lab, where we would look at the presence / frequency of words in a text, here we associate each word with a multi-dimensional vector which expresses what sentiment is conveyed by the word. In particular, our BOW vectors will be of size 5, exactly our number of sentiment classes. \n",
        "\n",
        "To classify a sentence, we **sum** the vectors of the words in the sentence and a bias vector. Because we sum the vectors, we lose word order: that's why we call this a neural bag-of-words model.\n",
        "\n",
        "```\n",
        "this   [0.0, 0.1, 0.1, 0.1, 0.0]\n",
        "movie  [0.0, 0.1, 0.1, 0.2, 0.1]\n",
        "is     [0.0, 0.1, 0.0, 0.0, 0.0]\n",
        "stupid [0.9, 0.5, 0.1, 0.0, 0.0]\n",
        "\n",
        "bias   [0.0, 0.0, 0.0, 0.0, 0.0]\n",
        "--------------------------------\n",
        "sum    [0.9, 0.8, 0.3, 0.3, 0.1]\n",
        "\n",
        "argmax: 0 (very negative)\n",
        "```\n",
        "\n",
        "The **argmax** of this sum is our predicted label.\n",
        "\n",
        "We initialize all vectors *randomly* and train them using cross-entropy loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLtBAIQGynkB"
      },
      "source": [
        "#### Model class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZfNklWf3tvs"
      },
      "source": [
        "class BOW(nn.Module):\n",
        "  \"\"\"A simple bag-of-words model\"\"\"\n",
        "\n",
        "  def __init__(self, vocab_size, embedding_dim, vocab):\n",
        "    super(BOW, self).__init__()\n",
        "    self.vocab = vocab\n",
        "    \n",
        "    # this is a trainable look-up table with word embeddings\n",
        "    self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
        "    \n",
        "    # this is a trainable bias term\n",
        "    self.bias = nn.Parameter(torch.zeros(embedding_dim), requires_grad=True)        \n",
        "\n",
        "  def forward(self, inputs):\n",
        "    # this is the forward pass of the neural network\n",
        "    # it applies a function to the input and returns the output\n",
        "\n",
        "    # this looks up the embeddings for each word ID in inputs\n",
        "    # the result is a sequence of word embeddings\n",
        "    embeds = self.embed(inputs)\n",
        "    \n",
        "    # the output is the sum across the time dimension (1)\n",
        "    # with the bias term added\n",
        "    logits = embeds.sum(1) + self.bias\n",
        "\n",
        "    return logits"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKHvBnoBAr6z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e3973f3-760c-4179-8689-6fb03c8b03df"
      },
      "source": [
        "# Let's create a model.\n",
        "vocab_size = len(v.w2i)\n",
        "n_classes = len(t2i)\n",
        "bow_model = BOW(vocab_size, n_classes, v)\n",
        "print(bow_model)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BOW(\n",
            "  (embed): Embedding(18280, 5)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfCx-HvMH1qQ"
      },
      "source": [
        "> **Hey, wait, where is the bias vector?**\n",
        "> PyTorch does not print Parameters, only Modules!\n",
        "\n",
        "> We can print it ourselves though, to check that it is there."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fhvk5HenAroT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e1af47b-6d1f-4991-a9e9-d50c74966d35"
      },
      "source": [
        "\n",
        "# Here we print each parameter name, shape, and if it is trainable.\n",
        "def print_parameters(model):\n",
        "  total = 0\n",
        "  for name, p in model.named_parameters():\n",
        "    total += np.prod(p.shape)\n",
        "    print(\"{:24s} {:12s} requires_grad={}\".format(name, str(list(p.shape)), p.requires_grad))\n",
        "  print(\"\\nTotal number of parameters: {}\\n\".format(total))\n",
        "    \n",
        "\n",
        "print_parameters(bow_model)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bias                     [5]          requires_grad=True\n",
            "embed.weight             [18280, 5]   requires_grad=True\n",
            "\n",
            "Total number of parameters: 91405\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSAw292WxuP4"
      },
      "source": [
        "#### Preparing an example for input\n",
        "\n",
        "To feed sentences to our PyTorch model, we need to convert a sequence of tokens to a sequence of IDs. The `prepare_example` function below takes care of this for us. We then use these IDs as indices for the word embedding table. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWeGTC_OGReV"
      },
      "source": [
        "def prepare_example(example, vocab):\n",
        "  \"\"\"\n",
        "  Map tokens to their IDs for a single example\n",
        "  \"\"\"\n",
        "  \n",
        "  # vocab returns 0 if the word is not there (i2w[0] = <unk>)\n",
        "  x = [vocab.w2i.get(t, 0) for t in example.tokens]\n",
        "  \n",
        "  x = torch.LongTensor([x])\n",
        "  x = x.to(device)\n",
        "  \n",
        "  y = torch.LongTensor([example.label])\n",
        "  y = y.to(device)\n",
        "  \n",
        "  return x, y"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfbdv9px3uFF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74da0b0a-e7c2-4869-d07b-727288d03d50"
      },
      "source": [
        "x, y = prepare_example(dev_data[0], v)\n",
        "print('x:', x)\n",
        "print('y:', y)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x: tensor([[  28,    9,    6,  998,   16,   18,  998,  135,   32, 7688,    5,    0,\n",
            "            2]], device='cuda:0')\n",
            "y: tensor([3], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKNQjEc0yXnJ"
      },
      "source": [
        "#### Evaluation\n",
        "We now need to define an evaluation metric.\n",
        "How many predictions do we get right? The accuracy will tell us.\n",
        "Make sure that you understand this code block.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGmQLcVYKZsh"
      },
      "source": [
        "def simple_evaluate(model, data, prep_fn=prepare_example, **kwargs):\n",
        "  \"\"\"Accuracy of a model on given data set.\"\"\"\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  model.eval()  # disable dropout (explained later)\n",
        "\n",
        "  for example in data:\n",
        "    \n",
        "    # convert the example input and label to PyTorch tensors\n",
        "    x, target = prep_fn(example, model.vocab)\n",
        "\n",
        "    # forward pass without backpropagation (no_grad)\n",
        "    # get the output from the neural network for input x\n",
        "    with torch.no_grad():\n",
        "      logits = model(x)\n",
        "    \n",
        "    # get the prediction\n",
        "    prediction = logits.argmax(dim=-1)\n",
        "    \n",
        "    # add the number of correct predictions to the total correct\n",
        "    correct += (prediction == target).sum().item()\n",
        "    total += 1\n",
        "\n",
        "  return correct, total, correct / float(total)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KlIGFXllWWm"
      },
      "source": [
        "We are using accuracy as a handy evaluation metric. Please consider using [alternative metrics](https://scikit-learn.org/stable/modules/classes.html#classification-metrics) for your experiments if that makes more theoretical sense (see, e.g., Q3.3 in Practical 1). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIk6OtSdzGRP"
      },
      "source": [
        "#### Example feed\n",
        "For stochastic gradient descent (SGD) we will need a random training example for every update.\n",
        "We implement this by shuffling the training data and returning examples one by one using `yield`.\n",
        "\n",
        "Shuffling is optional so that we get to use this function to get validation and test examples, too."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxDFOZLfCXvJ"
      },
      "source": [
        "def get_examples(data, shuffle=True, **kwargs):\n",
        "  \"\"\"Shuffle data set and return 1 example at a time (until nothing left)\"\"\"\n",
        "  if shuffle:\n",
        "    print(\"Shuffling training data\")\n",
        "    random.shuffle(data)  # shuffle training data each epoch\n",
        "  for example in data:\n",
        "    yield example"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g09SM8yb2cjx"
      },
      "source": [
        "#### Exercise: Training function\n",
        "\n",
        "Your task is now to complete the training loop below.\n",
        "Before you do so, please read the section about optimisation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVfUukVdM_1c"
      },
      "source": [
        "**Optimisation**\n",
        "\n",
        "As mentioned in the \"Intro to PyTorch\" notebook, one of the perks of using PyTorch is automatic differentiation. We will use it to train our BOW model. \n",
        "\n",
        "We train our model by feeding it an input, performing a **forward** pass, obtaining an output prediction, and calculating a **loss** with our loss function.\n",
        "After the gradients are computed in the **backward** pass, we can take a step on the surface of the loss function towards more optimal parameter settings (gradient descent). \n",
        "\n",
        "The package we will use to do this optimisation is [torch.optim](https://pytorch.org/docs/stable/optim.html). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhQigDrQ--YU"
      },
      "source": [
        "from torch import optim"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGIvcTZU_Cez"
      },
      "source": [
        "Besides implementations of stochastic gradient descent (SGD), this package also implements the optimisation algorithm Adam, which we'll be using in this practical. \n",
        "For the purposes of this assignment you do not need to know what Adam does besides that it uses gradient information to update our model parameters by calling: \n",
        "\n",
        "```\n",
        "optimizer.step()\n",
        "```\n",
        "Remember when we updated our parameters in the PyTorch tutorial in a loop?\n",
        "\n",
        "\n",
        "```python\n",
        "# update weights\n",
        "learning_rate = 0.5\n",
        "for f in net.parameters():\n",
        "    # for each parameter, take a small step in the opposite dir of the gradient\n",
        "    p.data = p.data - p.grad.data * learning_rate\n",
        "\n",
        "```\n",
        "The function call optimizer.step() does effectively the same thing.\n",
        "\n",
        "*(If you want to know more about optimisation algorithms using gradient information, [this blog](http://ruder.io/optimizing-gradient-descent/.) gives a nice intuitive overview.)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktFnKBux25lD"
      },
      "source": [
        "def train_model(model, optimizer, num_iterations=10000, \n",
        "                print_every=1000, eval_every=1000,\n",
        "                batch_fn=get_examples, \n",
        "                prep_fn=prepare_example,\n",
        "                eval_fn=simple_evaluate,\n",
        "                batch_size=1, eval_batch_size=None,\n",
        "                patience=0):\n",
        "    \"\"\"Train a model.\n",
        "    \n",
        "    The patience parameter is the number of evaluations after which\n",
        "    training is stopped if the model hasn't set a new highscore (so training\n",
        "    is stopped after than eval_every * patience steps without a highscore).\n",
        "    If patience = 0, this feature is disabled. In any case, training is\n",
        "    stopped after num_iterations. Set num_iterations to zero to treat it as infinite\"\"\"  \n",
        "    iter_i = 0\n",
        "    train_loss = 0.\n",
        "    print_num = 0\n",
        "    start = time.time()\n",
        "    criterion = nn.CrossEntropyLoss() # loss function\n",
        "    best_eval = 0.\n",
        "    best_iter = 0\n",
        "\n",
        "    # store train loss and validation accuracy during training\n",
        "    # so we can plot them afterwards\n",
        "    losses = []\n",
        "    accuracies = []  \n",
        "\n",
        "    if eval_batch_size is None:\n",
        "        eval_batch_size = batch_size\n",
        "  \n",
        "    while True:  # when we run out of examples, shuffle and continue\n",
        "        for batch in batch_fn(train_data, batch_size=batch_size):\n",
        "\n",
        "            # forward pass\n",
        "            model.train()\n",
        "            x, targets = prep_fn(batch, model.vocab)\n",
        "            logits = model(x)\n",
        "\n",
        "            B = targets.size(0)  # later we will use B examples per update\n",
        "\n",
        "            # compute cross-entropy loss (our criterion)\n",
        "            # note that the cross entropy loss function computes the softmax for us\n",
        "            loss = criterion(logits.view([B, -1]), targets.view(-1))\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            # backward pass (tip: check the Introduction to PyTorch notebook)\n",
        "\n",
        "            # erase previous gradients\n",
        "            optimizer.zero_grad()\n",
        "            # YOUR CODE HERE\n",
        "\n",
        "            # compute gradients\n",
        "            # YOUR CODE HERE\n",
        "            loss.backward()\n",
        "\n",
        "            # update weights - take a small step in the opposite dir of the gradient\n",
        "            # YOUR CODE HERE\n",
        "            optimizer.step()\n",
        "\n",
        "            print_num += 1\n",
        "            iter_i += 1\n",
        "\n",
        "            # print info\n",
        "            if iter_i % print_every == 0:\n",
        "                print(\"Iter %r: loss=%.4f, time=%.2fs\" % \n",
        "                      (iter_i, train_loss, time.time()-start))\n",
        "                losses.append(train_loss)\n",
        "                print_num = 0        \n",
        "                train_loss = 0.\n",
        "\n",
        "            # evaluate\n",
        "            if iter_i % eval_every == 0:\n",
        "                _, _, accuracy = eval_fn(model, dev_data, batch_size=eval_batch_size,\n",
        "                                         batch_fn=batch_fn, prep_fn=prep_fn)\n",
        "                accuracies.append(accuracy)\n",
        "                print(\"iter %r: dev acc=%.4f\" % (iter_i, accuracy))       \n",
        "        \n",
        "                # save best model parameters\n",
        "                if accuracy > best_eval:\n",
        "                    print(\"new highscore\")\n",
        "                    best_eval = accuracy\n",
        "                    best_iter = iter_i\n",
        "                    path = \"{}.pt\".format(model.__class__.__name__)\n",
        "                    ckpt = {\n",
        "                      \"state_dict\": model.state_dict(),\n",
        "                      \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                      \"best_eval\": best_eval,\n",
        "                      \"best_iter\": best_iter\n",
        "                    }\n",
        "                    torch.save(ckpt, path)\n",
        "\n",
        "            # done training\n",
        "            if iter_i == num_iterations or ((patience > 0) and iter_i - best_iter >= patience * eval_every):\n",
        "                if patience > 0 and (iter_i - best_iter >= patience * eval_every):\n",
        "                    print(\"Stopping early because there was no improvement for {} steps\".format(iter_i - best_iter))\n",
        "                print(\"Done training\")\n",
        "\n",
        "                # evaluate on train, dev, and test with best model\n",
        "                print(\"Loading best model\")\n",
        "                path = \"{}.pt\".format(model.__class__.__name__)        \n",
        "                ckpt = torch.load(path)\n",
        "                model.load_state_dict(ckpt[\"state_dict\"])\n",
        "\n",
        "                _, _, train_acc = eval_fn(\n",
        "                    model, train_data, batch_size=eval_batch_size, \n",
        "                    batch_fn=batch_fn, prep_fn=prep_fn)\n",
        "                _, _, dev_acc = eval_fn(\n",
        "                    model, dev_data, batch_size=eval_batch_size,\n",
        "                    batch_fn=batch_fn, prep_fn=prep_fn)\n",
        "                _, _, test_acc = eval_fn(\n",
        "                    model, test_data, batch_size=eval_batch_size, \n",
        "                    batch_fn=batch_fn, prep_fn=prep_fn)\n",
        "\n",
        "                print(\"best model iter {:d}: \"\n",
        "                      \"train acc={:.4f}, dev acc={:.4f}, test acc={:.4f}\".format(\n",
        "                          best_iter, train_acc, dev_acc, test_acc))\n",
        "\n",
        "                return (losses, accuracies), (train_acc, dev_acc, test_acc)\n",
        "\n",
        "def train_loop(model_generator, optimizer_generator,\n",
        "               num_iterations=10000, \n",
        "               print_every=1000, eval_every=1000,\n",
        "               batch_fn=get_examples, \n",
        "               prep_fn=prepare_example,\n",
        "               eval_fn=simple_evaluate,\n",
        "               batch_size=1, eval_batch_size=None,\n",
        "               patience=0,\n",
        "               num_seeds=3):\n",
        "    \"\"\"Train a model with multiple seeds.\n",
        "    \n",
        "    model_generator should be a function that returns a model instance.\n",
        "    optimizer_generator should be a function that takes a model instance\n",
        "    and return an optimizer.\n",
        "    \n",
        "    Returns three lists: losses, accuracies, best_accuracies.\n",
        "    For the first two, the first axis enumerates the different seeds and\n",
        "    the second axis is time. For best_accuracies, the first axis has length\n",
        "    three and contains the train, validation and test accuracies (in that order).\n",
        "    The second axis enumerates seeds.\"\"\"\n",
        "    losses = []\n",
        "    accuracies = []\n",
        "    best_accuracies = []\n",
        "    for seed in range(num_seeds):\n",
        "        print(\"Training new model with seed {}\".format(seed))\n",
        "        print(\"=\" * 50)\n",
        "        torch.manual_seed(seed)\n",
        "        random.seed(seed)\n",
        "        model = model_generator()\n",
        "        optimizer = optimizer_generator(model)\n",
        "        (new_losses, new_accuracies), best_accs = train_model(model, optimizer, num_iterations,\n",
        "                    print_every, eval_every, batch_fn, prep_fn, eval_fn,\n",
        "                    batch_size, eval_batch_size, patience)\n",
        "        losses.append(new_losses)\n",
        "        accuracies.append(new_accuracies)\n",
        "        best_accuracies.append(best_accs)\n",
        "    # transpose accuracy list\n",
        "    best_accuracies = list(zip(*best_accuracies))\n",
        "\n",
        "    print(\"Test accuracy:\", np.mean(best_accuracies[2]), \"+-\", np.std(best_accuracies[2]))\n",
        "    return losses, accuracies, best_accuracies\n",
        "  "
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEPsLvI-3D5b"
      },
      "source": [
        "### Training the BOW model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9luJnNuN_d3q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bc6c2bb-bbe1-4f9d-d255-3c070a24c0d0"
      },
      "source": [
        "# If everything is in place we can now train our first model!\n",
        "#bow_model = BOW(len(v.w2i), len(t2i), vocab=v)\n",
        "#print(bow_model)\n",
        "\n",
        "#bow_model = bow_model.to(device)\n",
        "\n",
        "#optimizer = optim.Adam(bow_model.parameters(), lr=0.0005)\n",
        "bow_losses, bow_accuracies, best_accs = train_loop(\n",
        "    lambda: BOW(len(v.w2i), len(t2i), vocab=v).to(device),\n",
        "    lambda model: optim.Adam(model.parameters(), lr=0.0005),\n",
        "    num_iterations=0, print_every=1000, eval_every=1000, patience=20)\n"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training new model with seed 0\n",
            "==================================================\n",
            "Shuffling training data\n",
            "Iter 1000: loss=6167.9235, time=1.00s\n",
            "iter 1000: dev acc=0.1898\n",
            "new highscore\n",
            "Iter 2000: loss=5419.3502, time=2.26s\n",
            "iter 2000: dev acc=0.1889\n",
            "Iter 3000: loss=4952.3615, time=3.51s\n",
            "iter 3000: dev acc=0.2035\n",
            "new highscore\n",
            "Iter 4000: loss=4920.7039, time=4.88s\n",
            "iter 4000: dev acc=0.2153\n",
            "new highscore\n",
            "Iter 5000: loss=4829.9561, time=6.22s\n",
            "iter 5000: dev acc=0.2134\n",
            "Iter 6000: loss=4538.4709, time=7.58s\n",
            "iter 6000: dev acc=0.2153\n",
            "Iter 7000: loss=4303.5353, time=8.83s\n",
            "iter 7000: dev acc=0.2162\n",
            "new highscore\n",
            "Iter 8000: loss=4632.1829, time=10.08s\n",
            "iter 8000: dev acc=0.2216\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 9000: loss=4310.0499, time=11.37s\n",
            "iter 9000: dev acc=0.2180\n",
            "Iter 10000: loss=4129.0018, time=12.61s\n",
            "iter 10000: dev acc=0.2207\n",
            "Iter 11000: loss=4093.0056, time=13.87s\n",
            "iter 11000: dev acc=0.2198\n",
            "Iter 12000: loss=3819.5645, time=15.12s\n",
            "iter 12000: dev acc=0.2271\n",
            "new highscore\n",
            "Iter 13000: loss=4150.0263, time=16.45s\n",
            "iter 13000: dev acc=0.2289\n",
            "new highscore\n",
            "Iter 14000: loss=4011.7559, time=17.73s\n",
            "iter 14000: dev acc=0.2325\n",
            "new highscore\n",
            "Iter 15000: loss=4095.9985, time=19.01s\n",
            "iter 15000: dev acc=0.2316\n",
            "Iter 16000: loss=3931.2745, time=20.28s\n",
            "iter 16000: dev acc=0.2307\n",
            "Iter 17000: loss=3679.6757, time=21.53s\n",
            "iter 17000: dev acc=0.2325\n",
            "Shuffling training data\n",
            "Iter 18000: loss=3495.1895, time=22.81s\n",
            "iter 18000: dev acc=0.2334\n",
            "new highscore\n",
            "Iter 19000: loss=3495.5633, time=24.08s\n",
            "iter 19000: dev acc=0.2334\n",
            "Iter 20000: loss=3477.9166, time=25.48s\n",
            "iter 20000: dev acc=0.2361\n",
            "new highscore\n",
            "Iter 21000: loss=3420.7189, time=26.75s\n",
            "iter 21000: dev acc=0.2371\n",
            "new highscore\n",
            "Iter 22000: loss=3475.2385, time=28.00s\n",
            "iter 22000: dev acc=0.2389\n",
            "new highscore\n",
            "Iter 23000: loss=3571.2851, time=29.26s\n",
            "iter 23000: dev acc=0.2389\n",
            "Iter 24000: loss=3490.8771, time=30.51s\n",
            "iter 24000: dev acc=0.2389\n",
            "Iter 25000: loss=3473.4623, time=31.80s\n",
            "iter 25000: dev acc=0.2425\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 26000: loss=3497.0730, time=33.11s\n",
            "iter 26000: dev acc=0.2398\n",
            "Iter 27000: loss=3129.7310, time=34.50s\n",
            "iter 27000: dev acc=0.2398\n",
            "Iter 28000: loss=3164.2260, time=35.88s\n",
            "iter 28000: dev acc=0.2389\n",
            "Iter 29000: loss=3129.8491, time=37.16s\n",
            "iter 29000: dev acc=0.2425\n",
            "Iter 30000: loss=3090.9884, time=38.41s\n",
            "iter 30000: dev acc=0.2443\n",
            "new highscore\n",
            "Iter 31000: loss=3088.8299, time=39.73s\n",
            "iter 31000: dev acc=0.2434\n",
            "Iter 32000: loss=3093.9545, time=41.13s\n",
            "iter 32000: dev acc=0.2470\n",
            "new highscore\n",
            "Iter 33000: loss=2902.7559, time=42.47s\n",
            "iter 33000: dev acc=0.2480\n",
            "new highscore\n",
            "Iter 34000: loss=3241.0073, time=43.80s\n",
            "iter 34000: dev acc=0.2498\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 35000: loss=2854.2010, time=45.21s\n",
            "iter 35000: dev acc=0.2516\n",
            "new highscore\n",
            "Iter 36000: loss=2751.0539, time=46.53s\n",
            "iter 36000: dev acc=0.2534\n",
            "new highscore\n",
            "Iter 37000: loss=2762.1272, time=47.77s\n",
            "iter 37000: dev acc=0.2516\n",
            "Iter 38000: loss=2854.1026, time=49.07s\n",
            "iter 38000: dev acc=0.2480\n",
            "Iter 39000: loss=2660.0774, time=50.45s\n",
            "iter 39000: dev acc=0.2498\n",
            "Iter 40000: loss=2582.2368, time=51.73s\n",
            "iter 40000: dev acc=0.2516\n",
            "Iter 41000: loss=2881.0860, time=52.98s\n",
            "iter 41000: dev acc=0.2480\n",
            "Iter 42000: loss=2687.5738, time=54.30s\n",
            "iter 42000: dev acc=0.2543\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 43000: loss=2700.9636, time=55.64s\n",
            "iter 43000: dev acc=0.2507\n",
            "Iter 44000: loss=2597.2779, time=56.91s\n",
            "iter 44000: dev acc=0.2534\n",
            "Iter 45000: loss=2423.8674, time=58.21s\n",
            "iter 45000: dev acc=0.2507\n",
            "Iter 46000: loss=2720.1404, time=59.52s\n",
            "iter 46000: dev acc=0.2516\n",
            "Iter 47000: loss=2586.2870, time=60.84s\n",
            "iter 47000: dev acc=0.2570\n",
            "new highscore\n",
            "Iter 48000: loss=2403.1958, time=62.09s\n",
            "iter 48000: dev acc=0.2543\n",
            "Iter 49000: loss=2481.6836, time=63.34s\n",
            "iter 49000: dev acc=0.2516\n",
            "Iter 50000: loss=2474.9802, time=64.59s\n",
            "iter 50000: dev acc=0.2534\n",
            "Iter 51000: loss=2342.3408, time=65.90s\n",
            "iter 51000: dev acc=0.2543\n",
            "Shuffling training data\n",
            "Iter 52000: loss=2203.3748, time=67.21s\n",
            "iter 52000: dev acc=0.2507\n",
            "Iter 53000: loss=2422.4040, time=68.46s\n",
            "iter 53000: dev acc=0.2507\n",
            "Iter 54000: loss=2289.4982, time=69.81s\n",
            "iter 54000: dev acc=0.2570\n",
            "Iter 55000: loss=2209.4174, time=71.07s\n",
            "iter 55000: dev acc=0.2552\n",
            "Iter 56000: loss=2234.6936, time=72.33s\n",
            "iter 56000: dev acc=0.2534\n",
            "Iter 57000: loss=2194.7163, time=73.68s\n",
            "iter 57000: dev acc=0.2516\n",
            "Iter 58000: loss=2374.5212, time=74.93s\n",
            "iter 58000: dev acc=0.2589\n",
            "new highscore\n",
            "Iter 59000: loss=2148.9403, time=76.24s\n",
            "iter 59000: dev acc=0.2570\n",
            "Shuffling training data\n",
            "Iter 60000: loss=2317.6588, time=77.54s\n",
            "iter 60000: dev acc=0.2589\n",
            "Iter 61000: loss=2107.3899, time=78.83s\n",
            "iter 61000: dev acc=0.2570\n",
            "Iter 62000: loss=2032.5929, time=80.15s\n",
            "iter 62000: dev acc=0.2561\n",
            "Iter 63000: loss=2134.0455, time=81.51s\n",
            "iter 63000: dev acc=0.2579\n",
            "Iter 64000: loss=1991.2844, time=82.79s\n",
            "iter 64000: dev acc=0.2570\n",
            "Iter 65000: loss=2117.8265, time=84.10s\n",
            "iter 65000: dev acc=0.2552\n",
            "Iter 66000: loss=2037.1145, time=85.34s\n",
            "iter 66000: dev acc=0.2598\n",
            "new highscore\n",
            "Iter 67000: loss=2049.7117, time=86.63s\n",
            "iter 67000: dev acc=0.2616\n",
            "new highscore\n",
            "Iter 68000: loss=2047.4085, time=87.91s\n",
            "iter 68000: dev acc=0.2598\n",
            "Shuffling training data\n",
            "Iter 69000: loss=1918.1093, time=89.19s\n",
            "iter 69000: dev acc=0.2534\n",
            "Iter 70000: loss=1928.7975, time=90.59s\n",
            "iter 70000: dev acc=0.2579\n",
            "Iter 71000: loss=1964.9413, time=92.01s\n",
            "iter 71000: dev acc=0.2607\n",
            "Iter 72000: loss=1881.5234, time=93.43s\n",
            "iter 72000: dev acc=0.2525\n",
            "Iter 73000: loss=1926.5447, time=94.70s\n",
            "iter 73000: dev acc=0.2561\n",
            "Iter 74000: loss=1880.3864, time=95.96s\n",
            "iter 74000: dev acc=0.2534\n",
            "Iter 75000: loss=1841.9484, time=97.27s\n",
            "iter 75000: dev acc=0.2598\n",
            "Iter 76000: loss=1942.3950, time=98.61s\n",
            "iter 76000: dev acc=0.2670\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 77000: loss=1712.1042, time=99.91s\n",
            "iter 77000: dev acc=0.2625\n",
            "Iter 78000: loss=1696.4599, time=101.27s\n",
            "iter 78000: dev acc=0.2743\n",
            "new highscore\n",
            "Iter 79000: loss=1734.9360, time=102.52s\n",
            "iter 79000: dev acc=0.2670\n",
            "Iter 80000: loss=1711.4035, time=103.85s\n",
            "iter 80000: dev acc=0.2679\n",
            "Iter 81000: loss=1736.6102, time=105.20s\n",
            "iter 81000: dev acc=0.2661\n",
            "Iter 82000: loss=1721.5452, time=106.69s\n",
            "iter 82000: dev acc=0.2679\n",
            "Iter 83000: loss=1708.6073, time=108.24s\n",
            "iter 83000: dev acc=0.2716\n",
            "Iter 84000: loss=1707.6392, time=109.58s\n",
            "iter 84000: dev acc=0.2688\n",
            "Iter 85000: loss=1857.2631, time=110.99s\n",
            "iter 85000: dev acc=0.2670\n",
            "Shuffling training data\n",
            "Iter 86000: loss=1622.5192, time=112.37s\n",
            "iter 86000: dev acc=0.2661\n",
            "Iter 87000: loss=1710.7863, time=113.70s\n",
            "iter 87000: dev acc=0.2652\n",
            "Iter 88000: loss=1570.0832, time=115.04s\n",
            "iter 88000: dev acc=0.2679\n",
            "Iter 89000: loss=1630.9656, time=116.51s\n",
            "iter 89000: dev acc=0.2716\n",
            "Iter 90000: loss=1538.3474, time=117.83s\n",
            "iter 90000: dev acc=0.2688\n",
            "Iter 91000: loss=1613.7129, time=119.13s\n",
            "iter 91000: dev acc=0.2670\n",
            "Iter 92000: loss=1640.6599, time=120.59s\n",
            "iter 92000: dev acc=0.2698\n",
            "Iter 93000: loss=1566.3588, time=121.99s\n",
            "iter 93000: dev acc=0.2788\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 94000: loss=1562.8203, time=123.28s\n",
            "iter 94000: dev acc=0.2734\n",
            "Iter 95000: loss=1492.5623, time=124.66s\n",
            "iter 95000: dev acc=0.2734\n",
            "Iter 96000: loss=1406.5027, time=125.92s\n",
            "iter 96000: dev acc=0.2752\n",
            "Iter 97000: loss=1431.2231, time=127.29s\n",
            "iter 97000: dev acc=0.2761\n",
            "Iter 98000: loss=1442.0969, time=128.68s\n",
            "iter 98000: dev acc=0.2707\n",
            "Iter 99000: loss=1423.1589, time=129.97s\n",
            "iter 99000: dev acc=0.2752\n",
            "Iter 100000: loss=1468.3801, time=131.48s\n",
            "iter 100000: dev acc=0.2761\n",
            "Iter 101000: loss=1563.3962, time=132.76s\n",
            "iter 101000: dev acc=0.2725\n",
            "Iter 102000: loss=1540.6651, time=133.99s\n",
            "iter 102000: dev acc=0.2734\n",
            "Shuffling training data\n",
            "Iter 103000: loss=1428.2833, time=135.22s\n",
            "iter 103000: dev acc=0.2770\n",
            "Iter 104000: loss=1259.9850, time=136.67s\n",
            "iter 104000: dev acc=0.2752\n",
            "Iter 105000: loss=1377.7057, time=137.92s\n",
            "iter 105000: dev acc=0.2770\n",
            "Iter 106000: loss=1393.4380, time=139.25s\n",
            "iter 106000: dev acc=0.2779\n",
            "Iter 107000: loss=1503.5100, time=140.75s\n",
            "iter 107000: dev acc=0.2779\n",
            "Iter 108000: loss=1331.5587, time=142.00s\n",
            "iter 108000: dev acc=0.2779\n",
            "Iter 109000: loss=1304.6072, time=143.24s\n",
            "iter 109000: dev acc=0.2779\n",
            "Iter 110000: loss=1435.7159, time=144.47s\n",
            "iter 110000: dev acc=0.2797\n",
            "new highscore\n",
            "Iter 111000: loss=1357.6924, time=145.71s\n",
            "iter 111000: dev acc=0.2816\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 112000: loss=1227.6905, time=146.96s\n",
            "iter 112000: dev acc=0.2816\n",
            "Iter 113000: loss=1315.6198, time=148.20s\n",
            "iter 113000: dev acc=0.2816\n",
            "Iter 114000: loss=1310.9346, time=149.54s\n",
            "iter 114000: dev acc=0.2770\n",
            "Iter 115000: loss=1260.6686, time=150.78s\n",
            "iter 115000: dev acc=0.2752\n",
            "Iter 116000: loss=1270.0407, time=152.03s\n",
            "iter 116000: dev acc=0.2825\n",
            "new highscore\n",
            "Iter 117000: loss=1251.2747, time=153.28s\n",
            "iter 117000: dev acc=0.2816\n",
            "Iter 118000: loss=1207.4691, time=154.51s\n",
            "iter 118000: dev acc=0.2825\n",
            "Iter 119000: loss=1295.5228, time=155.85s\n",
            "iter 119000: dev acc=0.2734\n",
            "Shuffling training data\n",
            "Iter 120000: loss=1183.1586, time=157.11s\n",
            "iter 120000: dev acc=0.2770\n",
            "Iter 121000: loss=1227.3410, time=158.47s\n",
            "iter 121000: dev acc=0.2797\n",
            "Iter 122000: loss=1171.6043, time=159.79s\n",
            "iter 122000: dev acc=0.2779\n",
            "Iter 123000: loss=1198.7594, time=161.20s\n",
            "iter 123000: dev acc=0.2834\n",
            "new highscore\n",
            "Iter 124000: loss=1184.1507, time=162.53s\n",
            "iter 124000: dev acc=0.2843\n",
            "new highscore\n",
            "Iter 125000: loss=1153.1315, time=163.81s\n",
            "iter 125000: dev acc=0.2788\n",
            "Iter 126000: loss=1174.0997, time=165.07s\n",
            "iter 126000: dev acc=0.2834\n",
            "Iter 127000: loss=1138.9433, time=166.34s\n",
            "iter 127000: dev acc=0.2861\n",
            "new highscore\n",
            "Iter 128000: loss=1134.7214, time=167.60s\n",
            "iter 128000: dev acc=0.2807\n",
            "Shuffling training data\n",
            "Iter 129000: loss=1116.8399, time=168.89s\n",
            "iter 129000: dev acc=0.2816\n",
            "Iter 130000: loss=1101.8630, time=170.20s\n",
            "iter 130000: dev acc=0.2761\n",
            "Iter 131000: loss=1056.8748, time=171.62s\n",
            "iter 131000: dev acc=0.2797\n",
            "Iter 132000: loss=1110.8394, time=172.87s\n",
            "iter 132000: dev acc=0.2843\n",
            "Iter 133000: loss=1130.3011, time=174.22s\n",
            "iter 133000: dev acc=0.2870\n",
            "new highscore\n",
            "Iter 134000: loss=1132.6365, time=175.53s\n",
            "iter 134000: dev acc=0.2888\n",
            "new highscore\n",
            "Iter 135000: loss=1111.3819, time=176.94s\n",
            "iter 135000: dev acc=0.2861\n",
            "Iter 136000: loss=1013.2108, time=178.27s\n",
            "iter 136000: dev acc=0.2788\n",
            "Shuffling training data\n",
            "Iter 137000: loss=1061.0575, time=179.69s\n",
            "iter 137000: dev acc=0.2852\n",
            "Iter 138000: loss=974.9212, time=181.07s\n",
            "iter 138000: dev acc=0.2816\n",
            "Iter 139000: loss=1025.2238, time=182.35s\n",
            "iter 139000: dev acc=0.2852\n",
            "Iter 140000: loss=1083.8476, time=183.59s\n",
            "iter 140000: dev acc=0.2861\n",
            "Iter 141000: loss=970.4998, time=184.84s\n",
            "iter 141000: dev acc=0.2816\n",
            "Iter 142000: loss=1078.4492, time=186.27s\n",
            "iter 142000: dev acc=0.2879\n",
            "Iter 143000: loss=985.7569, time=187.75s\n",
            "iter 143000: dev acc=0.2870\n",
            "Iter 144000: loss=1023.1138, time=188.99s\n",
            "iter 144000: dev acc=0.2906\n",
            "new highscore\n",
            "Iter 145000: loss=1068.6079, time=190.25s\n",
            "iter 145000: dev acc=0.2870\n",
            "Shuffling training data\n",
            "Iter 146000: loss=900.6989, time=191.59s\n",
            "iter 146000: dev acc=0.2879\n",
            "Iter 147000: loss=937.0252, time=192.84s\n",
            "iter 147000: dev acc=0.2897\n",
            "Iter 148000: loss=951.3056, time=194.07s\n",
            "iter 148000: dev acc=0.2861\n",
            "Iter 149000: loss=952.6855, time=195.34s\n",
            "iter 149000: dev acc=0.2897\n",
            "Iter 150000: loss=917.3658, time=196.76s\n",
            "iter 150000: dev acc=0.2906\n",
            "Iter 151000: loss=974.3644, time=198.03s\n",
            "iter 151000: dev acc=0.2870\n",
            "Iter 152000: loss=931.9187, time=199.29s\n",
            "iter 152000: dev acc=0.2852\n",
            "Iter 153000: loss=988.8478, time=200.56s\n",
            "iter 153000: dev acc=0.2843\n",
            "Shuffling training data\n",
            "Iter 154000: loss=957.7200, time=201.87s\n",
            "iter 154000: dev acc=0.2897\n",
            "Iter 155000: loss=921.1251, time=203.23s\n",
            "iter 155000: dev acc=0.2906\n",
            "Iter 156000: loss=922.8852, time=204.50s\n",
            "iter 156000: dev acc=0.2879\n",
            "Iter 157000: loss=930.4020, time=205.86s\n",
            "iter 157000: dev acc=0.2916\n",
            "new highscore\n",
            "Iter 158000: loss=771.1396, time=207.10s\n",
            "iter 158000: dev acc=0.2934\n",
            "new highscore\n",
            "Iter 159000: loss=817.1709, time=208.36s\n",
            "iter 159000: dev acc=0.2897\n",
            "Iter 160000: loss=895.3591, time=209.58s\n",
            "iter 160000: dev acc=0.2888\n",
            "Iter 161000: loss=877.6636, time=210.97s\n",
            "iter 161000: dev acc=0.2952\n",
            "new highscore\n",
            "Iter 162000: loss=881.8649, time=212.24s\n",
            "iter 162000: dev acc=0.2879\n",
            "Shuffling training data\n",
            "Iter 163000: loss=886.6188, time=213.47s\n",
            "iter 163000: dev acc=0.2897\n",
            "Iter 164000: loss=846.4554, time=214.79s\n",
            "iter 164000: dev acc=0.2916\n",
            "Iter 165000: loss=799.4516, time=216.08s\n",
            "iter 165000: dev acc=0.2916\n",
            "Iter 166000: loss=814.4361, time=217.44s\n",
            "iter 166000: dev acc=0.2961\n",
            "new highscore\n",
            "Iter 167000: loss=805.2231, time=218.72s\n",
            "iter 167000: dev acc=0.2934\n",
            "Iter 168000: loss=827.1943, time=219.97s\n",
            "iter 168000: dev acc=0.2916\n",
            "Iter 169000: loss=895.9005, time=221.20s\n",
            "iter 169000: dev acc=0.2952\n",
            "Iter 170000: loss=869.5455, time=222.44s\n",
            "iter 170000: dev acc=0.2970\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 171000: loss=823.7429, time=223.69s\n",
            "iter 171000: dev acc=0.2970\n",
            "Iter 172000: loss=747.5868, time=224.94s\n",
            "iter 172000: dev acc=0.2970\n",
            "Iter 173000: loss=728.9925, time=226.25s\n",
            "iter 173000: dev acc=0.3006\n",
            "new highscore\n",
            "Iter 174000: loss=820.0200, time=227.49s\n",
            "iter 174000: dev acc=0.2970\n",
            "Iter 175000: loss=803.8005, time=228.73s\n",
            "iter 175000: dev acc=0.2997\n",
            "Iter 176000: loss=770.1000, time=230.15s\n",
            "iter 176000: dev acc=0.3015\n",
            "new highscore\n",
            "Iter 177000: loss=756.0379, time=231.42s\n",
            "iter 177000: dev acc=0.3043\n",
            "new highscore\n",
            "Iter 178000: loss=804.6441, time=232.80s\n",
            "iter 178000: dev acc=0.3015\n",
            "Iter 179000: loss=790.8539, time=234.04s\n",
            "iter 179000: dev acc=0.3043\n",
            "Shuffling training data\n",
            "Iter 180000: loss=736.3523, time=235.30s\n",
            "iter 180000: dev acc=0.2988\n",
            "Iter 181000: loss=788.9295, time=236.61s\n",
            "iter 181000: dev acc=0.3052\n",
            "new highscore\n",
            "Iter 182000: loss=671.0184, time=237.87s\n",
            "iter 182000: dev acc=0.3061\n",
            "new highscore\n",
            "Iter 183000: loss=717.5935, time=239.11s\n",
            "iter 183000: dev acc=0.3025\n",
            "Iter 184000: loss=739.5738, time=240.35s\n",
            "iter 184000: dev acc=0.3079\n",
            "new highscore\n",
            "Iter 185000: loss=707.8767, time=241.59s\n",
            "iter 185000: dev acc=0.3034\n",
            "Iter 186000: loss=719.9711, time=242.84s\n",
            "iter 186000: dev acc=0.3043\n",
            "Iter 187000: loss=751.7767, time=244.11s\n",
            "iter 187000: dev acc=0.3052\n",
            "Shuffling training data\n",
            "Iter 188000: loss=782.8887, time=245.36s\n",
            "iter 188000: dev acc=0.3061\n",
            "Iter 189000: loss=670.0209, time=246.59s\n",
            "iter 189000: dev acc=0.3034\n",
            "Iter 190000: loss=680.7022, time=248.05s\n",
            "iter 190000: dev acc=0.3025\n",
            "Iter 191000: loss=565.8041, time=249.49s\n",
            "iter 191000: dev acc=0.3070\n",
            "Iter 192000: loss=743.6398, time=250.86s\n",
            "iter 192000: dev acc=0.3034\n",
            "Iter 193000: loss=719.3770, time=252.09s\n",
            "iter 193000: dev acc=0.3025\n",
            "Iter 194000: loss=701.1957, time=253.34s\n",
            "iter 194000: dev acc=0.3079\n",
            "Iter 195000: loss=740.4303, time=254.57s\n",
            "iter 195000: dev acc=0.3070\n",
            "Iter 196000: loss=667.3044, time=255.87s\n",
            "iter 196000: dev acc=0.3079\n",
            "Shuffling training data\n",
            "Iter 197000: loss=653.4691, time=257.13s\n",
            "iter 197000: dev acc=0.3106\n",
            "new highscore\n",
            "Iter 198000: loss=638.5340, time=258.44s\n",
            "iter 198000: dev acc=0.3052\n",
            "Iter 199000: loss=633.0845, time=259.68s\n",
            "iter 199000: dev acc=0.3079\n",
            "Iter 200000: loss=641.8684, time=260.91s\n",
            "iter 200000: dev acc=0.3043\n",
            "Iter 201000: loss=607.7106, time=262.16s\n",
            "iter 201000: dev acc=0.3070\n",
            "Iter 202000: loss=659.2482, time=263.39s\n",
            "iter 202000: dev acc=0.3070\n",
            "Iter 203000: loss=668.7448, time=264.74s\n",
            "iter 203000: dev acc=0.3061\n",
            "Iter 204000: loss=653.7443, time=265.99s\n",
            "iter 204000: dev acc=0.3070\n",
            "Iter 205000: loss=696.9555, time=267.22s\n",
            "iter 205000: dev acc=0.3088\n",
            "Shuffling training data\n",
            "Iter 206000: loss=571.8709, time=268.46s\n",
            "iter 206000: dev acc=0.3061\n",
            "Iter 207000: loss=571.8409, time=269.68s\n",
            "iter 207000: dev acc=0.3052\n",
            "Iter 208000: loss=619.7911, time=270.90s\n",
            "iter 208000: dev acc=0.3070\n",
            "Iter 209000: loss=660.9597, time=272.17s\n",
            "iter 209000: dev acc=0.3079\n",
            "Iter 210000: loss=656.4298, time=273.41s\n",
            "iter 210000: dev acc=0.3097\n",
            "Iter 211000: loss=649.8487, time=274.65s\n",
            "iter 211000: dev acc=0.3152\n",
            "new highscore\n",
            "Iter 212000: loss=543.0311, time=275.96s\n",
            "iter 212000: dev acc=0.3134\n",
            "Iter 213000: loss=625.9347, time=277.21s\n",
            "iter 213000: dev acc=0.3106\n",
            "Shuffling training data\n",
            "Iter 214000: loss=593.0808, time=278.56s\n",
            "iter 214000: dev acc=0.3143\n",
            "Iter 215000: loss=571.9924, time=279.81s\n",
            "iter 215000: dev acc=0.3124\n",
            "Iter 216000: loss=556.3619, time=281.12s\n",
            "iter 216000: dev acc=0.3097\n",
            "Iter 217000: loss=542.2078, time=282.40s\n",
            "iter 217000: dev acc=0.3097\n",
            "Iter 218000: loss=630.7486, time=283.66s\n",
            "iter 218000: dev acc=0.3088\n",
            "Iter 219000: loss=534.4005, time=284.90s\n",
            "iter 219000: dev acc=0.3152\n",
            "Iter 220000: loss=615.2247, time=286.14s\n",
            "iter 220000: dev acc=0.3161\n",
            "new highscore\n",
            "Iter 221000: loss=593.2569, time=287.39s\n",
            "iter 221000: dev acc=0.3161\n",
            "Iter 222000: loss=581.2582, time=288.64s\n",
            "iter 222000: dev acc=0.3115\n",
            "Shuffling training data\n",
            "Iter 223000: loss=518.3275, time=289.87s\n",
            "iter 223000: dev acc=0.3143\n",
            "Iter 224000: loss=527.3018, time=291.10s\n",
            "iter 224000: dev acc=0.3115\n",
            "Iter 225000: loss=535.3228, time=292.36s\n",
            "iter 225000: dev acc=0.3134\n",
            "Iter 226000: loss=528.9423, time=293.60s\n",
            "iter 226000: dev acc=0.3115\n",
            "Iter 227000: loss=543.0169, time=294.83s\n",
            "iter 227000: dev acc=0.3124\n",
            "Iter 228000: loss=583.4236, time=296.20s\n",
            "iter 228000: dev acc=0.3124\n",
            "Iter 229000: loss=567.5138, time=297.54s\n",
            "iter 229000: dev acc=0.3152\n",
            "Iter 230000: loss=522.5171, time=298.76s\n",
            "iter 230000: dev acc=0.3188\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 231000: loss=530.4446, time=300.07s\n",
            "iter 231000: dev acc=0.3170\n",
            "Iter 232000: loss=495.4421, time=301.35s\n",
            "iter 232000: dev acc=0.3215\n",
            "new highscore\n",
            "Iter 233000: loss=463.1680, time=302.59s\n",
            "iter 233000: dev acc=0.3215\n",
            "Iter 234000: loss=532.1099, time=303.88s\n",
            "iter 234000: dev acc=0.3179\n",
            "Iter 235000: loss=498.3461, time=305.11s\n",
            "iter 235000: dev acc=0.3134\n",
            "Iter 236000: loss=470.1967, time=306.51s\n",
            "iter 236000: dev acc=0.3161\n",
            "Iter 237000: loss=543.2599, time=307.87s\n",
            "iter 237000: dev acc=0.3197\n",
            "Iter 238000: loss=534.3318, time=309.13s\n",
            "iter 238000: dev acc=0.3134\n",
            "Iter 239000: loss=602.0351, time=310.45s\n",
            "iter 239000: dev acc=0.3152\n",
            "Shuffling training data\n",
            "Iter 240000: loss=481.8197, time=311.70s\n",
            "iter 240000: dev acc=0.3188\n",
            "Iter 241000: loss=448.1516, time=313.03s\n",
            "iter 241000: dev acc=0.3124\n",
            "Iter 242000: loss=489.3795, time=314.31s\n",
            "iter 242000: dev acc=0.3152\n",
            "Iter 243000: loss=476.2828, time=315.61s\n",
            "iter 243000: dev acc=0.3179\n",
            "Iter 244000: loss=496.2683, time=316.88s\n",
            "iter 244000: dev acc=0.3179\n",
            "Iter 245000: loss=499.4498, time=318.16s\n",
            "iter 245000: dev acc=0.3197\n",
            "Iter 246000: loss=527.7432, time=319.41s\n",
            "iter 246000: dev acc=0.3179\n",
            "Iter 247000: loss=476.3387, time=320.64s\n",
            "iter 247000: dev acc=0.3179\n",
            "Shuffling training data\n",
            "Iter 248000: loss=493.0322, time=321.94s\n",
            "iter 248000: dev acc=0.3206\n",
            "Iter 249000: loss=419.0283, time=323.30s\n",
            "iter 249000: dev acc=0.3197\n",
            "Iter 250000: loss=402.0444, time=324.54s\n",
            "iter 250000: dev acc=0.3170\n",
            "Iter 251000: loss=489.0610, time=325.89s\n",
            "iter 251000: dev acc=0.3170\n",
            "Iter 252000: loss=452.3000, time=327.19s\n",
            "iter 252000: dev acc=0.3206\n",
            "Stopping early because there was no improvement for 20000 steps\n",
            "Done training\n",
            "Loading best model\n",
            "best model iter 232000: train acc=0.8233, dev acc=0.3215, test acc=0.3208\n",
            "Training new model with seed 1\n",
            "==================================================\n",
            "Shuffling training data\n",
            "Iter 1000: loss=5499.2644, time=0.98s\n",
            "iter 1000: dev acc=0.1771\n",
            "new highscore\n",
            "Iter 2000: loss=5292.3595, time=2.34s\n",
            "iter 2000: dev acc=0.1798\n",
            "new highscore\n",
            "Iter 3000: loss=4823.3092, time=3.64s\n",
            "iter 3000: dev acc=0.1889\n",
            "new highscore\n",
            "Iter 4000: loss=4711.5403, time=5.04s\n",
            "iter 4000: dev acc=0.1935\n",
            "new highscore\n",
            "Iter 5000: loss=4806.5508, time=6.47s\n",
            "iter 5000: dev acc=0.2053\n",
            "new highscore\n",
            "Iter 6000: loss=4610.2164, time=7.80s\n",
            "iter 6000: dev acc=0.2071\n",
            "new highscore\n",
            "Iter 7000: loss=4321.0934, time=9.12s\n",
            "iter 7000: dev acc=0.2098\n",
            "new highscore\n",
            "Iter 8000: loss=4482.5668, time=10.43s\n",
            "iter 8000: dev acc=0.2162\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 9000: loss=4142.5974, time=11.73s\n",
            "iter 9000: dev acc=0.2180\n",
            "new highscore\n",
            "Iter 10000: loss=4154.6569, time=13.09s\n",
            "iter 10000: dev acc=0.2271\n",
            "new highscore\n",
            "Iter 11000: loss=4076.0541, time=14.46s\n",
            "iter 11000: dev acc=0.2216\n",
            "Iter 12000: loss=4169.8879, time=15.75s\n",
            "iter 12000: dev acc=0.2252\n",
            "Iter 13000: loss=4136.2652, time=17.03s\n",
            "iter 13000: dev acc=0.2280\n",
            "new highscore\n",
            "Iter 14000: loss=3871.7367, time=18.38s\n",
            "iter 14000: dev acc=0.2298\n",
            "new highscore\n",
            "Iter 15000: loss=3915.7156, time=19.67s\n",
            "iter 15000: dev acc=0.2307\n",
            "new highscore\n",
            "Iter 16000: loss=3786.9445, time=20.96s\n",
            "iter 16000: dev acc=0.2298\n",
            "Iter 17000: loss=3954.0682, time=22.25s\n",
            "iter 17000: dev acc=0.2334\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 18000: loss=3649.8283, time=23.54s\n",
            "iter 18000: dev acc=0.2352\n",
            "new highscore\n",
            "Iter 19000: loss=3438.0283, time=24.99s\n",
            "iter 19000: dev acc=0.2352\n",
            "Iter 20000: loss=3667.0962, time=26.42s\n",
            "iter 20000: dev acc=0.2416\n",
            "new highscore\n",
            "Iter 21000: loss=3481.3862, time=27.72s\n",
            "iter 21000: dev acc=0.2425\n",
            "new highscore\n",
            "Iter 22000: loss=3547.3066, time=28.95s\n",
            "iter 22000: dev acc=0.2470\n",
            "new highscore\n",
            "Iter 23000: loss=3406.0839, time=30.26s\n",
            "iter 23000: dev acc=0.2434\n",
            "Iter 24000: loss=3586.9463, time=31.53s\n",
            "iter 24000: dev acc=0.2489\n",
            "new highscore\n",
            "Iter 25000: loss=3292.2357, time=32.86s\n",
            "iter 25000: dev acc=0.2461\n",
            "Shuffling training data\n",
            "Iter 26000: loss=3317.0041, time=34.23s\n",
            "iter 26000: dev acc=0.2470\n",
            "Iter 27000: loss=3045.2630, time=35.53s\n",
            "iter 27000: dev acc=0.2489\n",
            "Iter 28000: loss=3319.6807, time=36.92s\n",
            "iter 28000: dev acc=0.2452\n",
            "Iter 29000: loss=3207.3432, time=38.34s\n",
            "iter 29000: dev acc=0.2525\n",
            "new highscore\n",
            "Iter 30000: loss=3045.4587, time=39.72s\n",
            "iter 30000: dev acc=0.2543\n",
            "new highscore\n",
            "Iter 31000: loss=3061.3180, time=41.00s\n",
            "iter 31000: dev acc=0.2552\n",
            "new highscore\n",
            "Iter 32000: loss=3226.1187, time=42.35s\n",
            "iter 32000: dev acc=0.2607\n",
            "new highscore\n",
            "Iter 33000: loss=3117.3982, time=43.61s\n",
            "iter 33000: dev acc=0.2570\n",
            "Iter 34000: loss=2971.4786, time=44.98s\n",
            "iter 34000: dev acc=0.2607\n",
            "Shuffling training data\n",
            "Iter 35000: loss=2857.3811, time=46.32s\n",
            "iter 35000: dev acc=0.2643\n",
            "new highscore\n",
            "Iter 36000: loss=2832.3580, time=47.59s\n",
            "iter 36000: dev acc=0.2643\n",
            "Iter 37000: loss=2988.3324, time=48.98s\n",
            "iter 37000: dev acc=0.2643\n",
            "Iter 38000: loss=2770.7629, time=50.23s\n",
            "iter 38000: dev acc=0.2670\n",
            "new highscore\n",
            "Iter 39000: loss=2627.1894, time=51.46s\n",
            "iter 39000: dev acc=0.2707\n",
            "new highscore\n",
            "Iter 40000: loss=2775.8069, time=52.70s\n",
            "iter 40000: dev acc=0.2661\n",
            "Iter 41000: loss=2784.0615, time=53.97s\n",
            "iter 41000: dev acc=0.2652\n",
            "Iter 42000: loss=2811.5933, time=55.19s\n",
            "iter 42000: dev acc=0.2652\n",
            "Shuffling training data\n",
            "Iter 43000: loss=2596.1162, time=56.45s\n",
            "iter 43000: dev acc=0.2643\n",
            "Iter 44000: loss=2610.1917, time=57.69s\n",
            "iter 44000: dev acc=0.2652\n",
            "Iter 45000: loss=2612.4857, time=58.92s\n",
            "iter 45000: dev acc=0.2688\n",
            "Iter 46000: loss=2484.8562, time=60.16s\n",
            "iter 46000: dev acc=0.2734\n",
            "new highscore\n",
            "Iter 47000: loss=2558.3473, time=61.54s\n",
            "iter 47000: dev acc=0.2752\n",
            "new highscore\n",
            "Iter 48000: loss=2313.6898, time=62.76s\n",
            "iter 48000: dev acc=0.2725\n",
            "Iter 49000: loss=2427.9295, time=63.99s\n",
            "iter 49000: dev acc=0.2761\n",
            "new highscore\n",
            "Iter 50000: loss=2404.8898, time=65.30s\n",
            "iter 50000: dev acc=0.2761\n",
            "Iter 51000: loss=2749.9564, time=66.57s\n",
            "iter 51000: dev acc=0.2770\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 52000: loss=2283.3326, time=67.82s\n",
            "iter 52000: dev acc=0.2788\n",
            "new highscore\n",
            "Iter 53000: loss=2218.2870, time=69.06s\n",
            "iter 53000: dev acc=0.2734\n",
            "Iter 54000: loss=2134.0617, time=70.30s\n",
            "iter 54000: dev acc=0.2770\n",
            "Iter 55000: loss=2568.2936, time=71.55s\n",
            "iter 55000: dev acc=0.2752\n",
            "Iter 56000: loss=2277.4174, time=72.78s\n",
            "iter 56000: dev acc=0.2797\n",
            "new highscore\n",
            "Iter 57000: loss=2314.3296, time=74.13s\n",
            "iter 57000: dev acc=0.2816\n",
            "new highscore\n",
            "Iter 58000: loss=2351.0910, time=75.36s\n",
            "iter 58000: dev acc=0.2770\n",
            "Iter 59000: loss=2206.3896, time=76.61s\n",
            "iter 59000: dev acc=0.2761\n",
            "Shuffling training data\n",
            "Iter 60000: loss=2327.3917, time=77.86s\n",
            "iter 60000: dev acc=0.2770\n",
            "Iter 61000: loss=2205.7447, time=79.09s\n",
            "iter 61000: dev acc=0.2807\n",
            "Iter 62000: loss=2013.0258, time=80.33s\n",
            "iter 62000: dev acc=0.2788\n",
            "Iter 63000: loss=2090.9928, time=81.57s\n",
            "iter 63000: dev acc=0.2825\n",
            "new highscore\n",
            "Iter 64000: loss=2077.6304, time=82.89s\n",
            "iter 64000: dev acc=0.2807\n",
            "Iter 65000: loss=2031.5083, time=84.12s\n",
            "iter 65000: dev acc=0.2852\n",
            "new highscore\n",
            "Iter 66000: loss=2012.6991, time=85.55s\n",
            "iter 66000: dev acc=0.2879\n",
            "new highscore\n",
            "Iter 67000: loss=2139.0685, time=86.99s\n",
            "iter 67000: dev acc=0.2861\n",
            "Iter 68000: loss=2117.8340, time=88.33s\n",
            "iter 68000: dev acc=0.2852\n",
            "Shuffling training data\n",
            "Iter 69000: loss=1966.3728, time=89.67s\n",
            "iter 69000: dev acc=0.2879\n",
            "Iter 70000: loss=1954.3913, time=91.10s\n",
            "iter 70000: dev acc=0.2943\n",
            "new highscore\n",
            "Iter 71000: loss=1965.5922, time=92.46s\n",
            "iter 71000: dev acc=0.2952\n",
            "new highscore\n",
            "Iter 72000: loss=1940.4477, time=93.86s\n",
            "iter 72000: dev acc=0.2943\n",
            "Iter 73000: loss=1892.1565, time=95.19s\n",
            "iter 73000: dev acc=0.2916\n",
            "Iter 74000: loss=1865.5039, time=96.46s\n",
            "iter 74000: dev acc=0.2952\n",
            "Iter 75000: loss=1890.5615, time=97.70s\n",
            "iter 75000: dev acc=0.2916\n",
            "Iter 76000: loss=1949.9652, time=99.01s\n",
            "iter 76000: dev acc=0.2943\n",
            "Shuffling training data\n",
            "Iter 77000: loss=1849.9436, time=100.25s\n",
            "iter 77000: dev acc=0.2925\n",
            "Iter 78000: loss=1819.9819, time=101.51s\n",
            "iter 78000: dev acc=0.2961\n",
            "new highscore\n",
            "Iter 79000: loss=1673.3656, time=102.74s\n",
            "iter 79000: dev acc=0.2988\n",
            "new highscore\n",
            "Iter 80000: loss=1756.6261, time=103.99s\n",
            "iter 80000: dev acc=0.2961\n",
            "Iter 81000: loss=1748.0113, time=105.27s\n",
            "iter 81000: dev acc=0.3006\n",
            "new highscore\n",
            "Iter 82000: loss=1819.7579, time=106.55s\n",
            "iter 82000: dev acc=0.2952\n",
            "Iter 83000: loss=1763.0686, time=107.86s\n",
            "iter 83000: dev acc=0.2997\n",
            "Iter 84000: loss=1835.0622, time=109.20s\n",
            "iter 84000: dev acc=0.2979\n",
            "Iter 85000: loss=1737.4623, time=110.48s\n",
            "iter 85000: dev acc=0.2979\n",
            "Shuffling training data\n",
            "Iter 86000: loss=1623.8427, time=111.84s\n",
            "iter 86000: dev acc=0.3025\n",
            "new highscore\n",
            "Iter 87000: loss=1645.1914, time=113.11s\n",
            "iter 87000: dev acc=0.2961\n",
            "Iter 88000: loss=1672.0357, time=114.47s\n",
            "iter 88000: dev acc=0.3015\n",
            "Iter 89000: loss=1567.7983, time=115.81s\n",
            "iter 89000: dev acc=0.3043\n",
            "new highscore\n",
            "Iter 90000: loss=1714.3766, time=117.15s\n",
            "iter 90000: dev acc=0.2961\n",
            "Iter 91000: loss=1571.9319, time=118.49s\n",
            "iter 91000: dev acc=0.3034\n",
            "Iter 92000: loss=1572.4815, time=119.73s\n",
            "iter 92000: dev acc=0.3061\n",
            "new highscore\n",
            "Iter 93000: loss=1610.0389, time=121.00s\n",
            "iter 93000: dev acc=0.3070\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 94000: loss=1642.4541, time=122.36s\n",
            "iter 94000: dev acc=0.3079\n",
            "new highscore\n",
            "Iter 95000: loss=1499.3382, time=123.63s\n",
            "iter 95000: dev acc=0.3088\n",
            "new highscore\n",
            "Iter 96000: loss=1489.5299, time=124.94s\n",
            "iter 96000: dev acc=0.3106\n",
            "new highscore\n",
            "Iter 97000: loss=1409.3306, time=126.18s\n",
            "iter 97000: dev acc=0.3097\n",
            "Iter 98000: loss=1539.4400, time=127.51s\n",
            "iter 98000: dev acc=0.3097\n",
            "Iter 99000: loss=1496.3251, time=128.79s\n",
            "iter 99000: dev acc=0.3152\n",
            "new highscore\n",
            "Iter 100000: loss=1563.0211, time=130.03s\n",
            "iter 100000: dev acc=0.3161\n",
            "new highscore\n",
            "Iter 101000: loss=1465.6880, time=131.25s\n",
            "iter 101000: dev acc=0.3170\n",
            "new highscore\n",
            "Iter 102000: loss=1519.3864, time=132.48s\n",
            "iter 102000: dev acc=0.3124\n",
            "Shuffling training data\n",
            "Iter 103000: loss=1415.9460, time=133.88s\n",
            "iter 103000: dev acc=0.3115\n",
            "Iter 104000: loss=1397.5971, time=135.11s\n",
            "iter 104000: dev acc=0.3124\n",
            "Iter 105000: loss=1427.8924, time=136.35s\n",
            "iter 105000: dev acc=0.3124\n",
            "Iter 106000: loss=1318.4887, time=137.61s\n",
            "iter 106000: dev acc=0.3088\n",
            "Iter 107000: loss=1520.8711, time=139.01s\n",
            "iter 107000: dev acc=0.3097\n",
            "Iter 108000: loss=1349.4820, time=140.23s\n",
            "iter 108000: dev acc=0.3124\n",
            "Iter 109000: loss=1361.0055, time=141.50s\n",
            "iter 109000: dev acc=0.3097\n",
            "Iter 110000: loss=1409.0278, time=142.89s\n",
            "iter 110000: dev acc=0.3124\n",
            "Iter 111000: loss=1308.3726, time=144.24s\n",
            "iter 111000: dev acc=0.3143\n",
            "Shuffling training data\n",
            "Iter 112000: loss=1336.3798, time=145.57s\n",
            "iter 112000: dev acc=0.3115\n",
            "Iter 113000: loss=1293.5558, time=147.07s\n",
            "iter 113000: dev acc=0.3124\n",
            "Iter 114000: loss=1292.9066, time=148.44s\n",
            "iter 114000: dev acc=0.3115\n",
            "Iter 115000: loss=1214.8843, time=149.69s\n",
            "iter 115000: dev acc=0.3134\n",
            "Iter 116000: loss=1202.8685, time=151.09s\n",
            "iter 116000: dev acc=0.3079\n",
            "Iter 117000: loss=1303.5050, time=152.39s\n",
            "iter 117000: dev acc=0.3124\n",
            "Iter 118000: loss=1339.9861, time=153.64s\n",
            "iter 118000: dev acc=0.3152\n",
            "Iter 119000: loss=1301.1948, time=154.87s\n",
            "iter 119000: dev acc=0.3088\n",
            "Shuffling training data\n",
            "Iter 120000: loss=1306.6343, time=156.11s\n",
            "iter 120000: dev acc=0.3170\n",
            "Iter 121000: loss=1141.8041, time=157.35s\n",
            "iter 121000: dev acc=0.3152\n",
            "Stopping early because there was no improvement for 20000 steps\n",
            "Done training\n",
            "Loading best model\n",
            "best model iter 101000: train acc=0.5490, dev acc=0.3170, test acc=0.3222\n",
            "Training new model with seed 2\n",
            "==================================================\n",
            "Shuffling training data\n",
            "Iter 1000: loss=5087.9889, time=0.97s\n",
            "iter 1000: dev acc=0.2007\n",
            "new highscore\n",
            "Iter 2000: loss=4643.9402, time=2.28s\n",
            "iter 2000: dev acc=0.2007\n",
            "Iter 3000: loss=4827.9722, time=3.60s\n",
            "iter 3000: dev acc=0.2134\n",
            "new highscore\n",
            "Iter 4000: loss=4551.0075, time=4.88s\n",
            "iter 4000: dev acc=0.2180\n",
            "new highscore\n",
            "Iter 5000: loss=4632.2668, time=6.13s\n",
            "iter 5000: dev acc=0.2189\n",
            "new highscore\n",
            "Iter 6000: loss=4345.8743, time=7.45s\n",
            "iter 6000: dev acc=0.2207\n",
            "new highscore\n",
            "Iter 7000: loss=4528.3902, time=8.90s\n",
            "iter 7000: dev acc=0.2271\n",
            "new highscore\n",
            "Iter 8000: loss=4503.7135, time=10.15s\n",
            "iter 8000: dev acc=0.2234\n",
            "Shuffling training data\n",
            "Iter 9000: loss=4508.6420, time=11.55s\n",
            "iter 9000: dev acc=0.2234\n",
            "Iter 10000: loss=4110.3444, time=12.88s\n",
            "iter 10000: dev acc=0.2234\n",
            "Iter 11000: loss=4001.9668, time=14.11s\n",
            "iter 11000: dev acc=0.2289\n",
            "new highscore\n",
            "Iter 12000: loss=4021.6519, time=15.44s\n",
            "iter 12000: dev acc=0.2307\n",
            "new highscore\n",
            "Iter 13000: loss=3894.3240, time=16.70s\n",
            "iter 13000: dev acc=0.2307\n",
            "Iter 14000: loss=3885.1459, time=17.97s\n",
            "iter 14000: dev acc=0.2325\n",
            "new highscore\n",
            "Iter 15000: loss=3766.2234, time=19.22s\n",
            "iter 15000: dev acc=0.2361\n",
            "new highscore\n",
            "Iter 16000: loss=4049.2556, time=20.45s\n",
            "iter 16000: dev acc=0.2407\n",
            "new highscore\n",
            "Iter 17000: loss=3802.2135, time=21.69s\n",
            "iter 17000: dev acc=0.2425\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 18000: loss=3525.7120, time=22.93s\n",
            "iter 18000: dev acc=0.2416\n",
            "Iter 19000: loss=3504.3586, time=24.20s\n",
            "iter 19000: dev acc=0.2461\n",
            "new highscore\n",
            "Iter 20000: loss=3590.0943, time=25.49s\n",
            "iter 20000: dev acc=0.2434\n",
            "Iter 21000: loss=3577.3689, time=26.77s\n",
            "iter 21000: dev acc=0.2425\n",
            "Iter 22000: loss=3446.1875, time=28.07s\n",
            "iter 22000: dev acc=0.2498\n",
            "new highscore\n",
            "Iter 23000: loss=3511.5171, time=29.33s\n",
            "iter 23000: dev acc=0.2525\n",
            "new highscore\n",
            "Iter 24000: loss=3427.4513, time=30.60s\n",
            "iter 24000: dev acc=0.2552\n",
            "new highscore\n",
            "Iter 25000: loss=3364.9880, time=32.06s\n",
            "iter 25000: dev acc=0.2589\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 26000: loss=3082.7774, time=33.60s\n",
            "iter 26000: dev acc=0.2589\n",
            "Iter 27000: loss=3056.7389, time=35.01s\n",
            "iter 27000: dev acc=0.2534\n",
            "Iter 28000: loss=3062.9213, time=36.35s\n",
            "iter 28000: dev acc=0.2570\n",
            "Iter 29000: loss=3018.9341, time=37.67s\n",
            "iter 29000: dev acc=0.2552\n",
            "Iter 30000: loss=3137.3549, time=38.92s\n",
            "iter 30000: dev acc=0.2616\n",
            "new highscore\n",
            "Iter 31000: loss=3016.0651, time=40.27s\n",
            "iter 31000: dev acc=0.2698\n",
            "new highscore\n",
            "Iter 32000: loss=3192.5623, time=41.63s\n",
            "iter 32000: dev acc=0.2698\n",
            "Iter 33000: loss=3115.7970, time=42.92s\n",
            "iter 33000: dev acc=0.2743\n",
            "new highscore\n",
            "Iter 34000: loss=2966.1961, time=44.27s\n",
            "iter 34000: dev acc=0.2725\n",
            "Shuffling training data\n",
            "Iter 35000: loss=2832.0149, time=45.70s\n",
            "iter 35000: dev acc=0.2725\n",
            "Iter 36000: loss=2833.7531, time=47.09s\n",
            "iter 36000: dev acc=0.2743\n",
            "Iter 37000: loss=2898.5262, time=48.34s\n",
            "iter 37000: dev acc=0.2752\n",
            "new highscore\n",
            "Iter 38000: loss=2823.1578, time=49.57s\n",
            "iter 38000: dev acc=0.2816\n",
            "new highscore\n",
            "Iter 39000: loss=2633.5678, time=50.81s\n",
            "iter 39000: dev acc=0.2861\n",
            "new highscore\n",
            "Iter 40000: loss=2747.3455, time=52.06s\n",
            "iter 40000: dev acc=0.2852\n",
            "Iter 41000: loss=2745.9904, time=53.31s\n",
            "iter 41000: dev acc=0.2834\n",
            "Iter 42000: loss=2574.1866, time=54.56s\n",
            "iter 42000: dev acc=0.2852\n",
            "Shuffling training data\n",
            "Iter 43000: loss=2631.1669, time=55.89s\n",
            "iter 43000: dev acc=0.2852\n",
            "Iter 44000: loss=2623.2192, time=57.13s\n",
            "iter 44000: dev acc=0.2825\n",
            "Iter 45000: loss=2424.9483, time=58.46s\n",
            "iter 45000: dev acc=0.2870\n",
            "new highscore\n",
            "Iter 46000: loss=2488.3970, time=59.76s\n",
            "iter 46000: dev acc=0.2843\n",
            "Iter 47000: loss=2588.7750, time=61.10s\n",
            "iter 47000: dev acc=0.2888\n",
            "new highscore\n",
            "Iter 48000: loss=2550.2537, time=62.51s\n",
            "iter 48000: dev acc=0.2934\n",
            "new highscore\n",
            "Iter 49000: loss=2325.8793, time=63.77s\n",
            "iter 49000: dev acc=0.2934\n",
            "Iter 50000: loss=2374.1278, time=65.10s\n",
            "iter 50000: dev acc=0.2979\n",
            "new highscore\n",
            "Iter 51000: loss=2425.5643, time=66.43s\n",
            "iter 51000: dev acc=0.2979\n",
            "Shuffling training data\n",
            "Iter 52000: loss=2308.9945, time=67.68s\n",
            "iter 52000: dev acc=0.2979\n",
            "Iter 53000: loss=2206.0053, time=69.01s\n",
            "iter 53000: dev acc=0.3043\n",
            "new highscore\n",
            "Iter 54000: loss=2259.4365, time=70.32s\n",
            "iter 54000: dev acc=0.3097\n",
            "new highscore\n",
            "Iter 55000: loss=2323.7497, time=71.68s\n",
            "iter 55000: dev acc=0.3079\n",
            "Iter 56000: loss=2091.0777, time=73.02s\n",
            "iter 56000: dev acc=0.3079\n",
            "Iter 57000: loss=2256.7523, time=74.25s\n",
            "iter 57000: dev acc=0.3106\n",
            "new highscore\n",
            "Iter 58000: loss=2264.0628, time=75.47s\n",
            "iter 58000: dev acc=0.3134\n",
            "new highscore\n",
            "Iter 59000: loss=2286.2607, time=76.75s\n",
            "iter 59000: dev acc=0.3124\n",
            "Shuffling training data\n",
            "Iter 60000: loss=2219.1409, time=77.99s\n",
            "iter 60000: dev acc=0.3124\n",
            "Iter 61000: loss=2109.4427, time=79.25s\n",
            "iter 61000: dev acc=0.3124\n",
            "Iter 62000: loss=2040.1549, time=80.48s\n",
            "iter 62000: dev acc=0.3152\n",
            "new highscore\n",
            "Iter 63000: loss=2103.2999, time=81.76s\n",
            "iter 63000: dev acc=0.3115\n",
            "Iter 64000: loss=2092.3784, time=83.13s\n",
            "iter 64000: dev acc=0.3088\n",
            "Iter 65000: loss=2096.2786, time=84.45s\n",
            "iter 65000: dev acc=0.3079\n",
            "Iter 66000: loss=2048.7877, time=85.79s\n",
            "iter 66000: dev acc=0.3115\n",
            "Iter 67000: loss=1871.0599, time=87.14s\n",
            "iter 67000: dev acc=0.3097\n",
            "Iter 68000: loss=2023.9145, time=88.47s\n",
            "iter 68000: dev acc=0.3079\n",
            "Shuffling training data\n",
            "Iter 69000: loss=2014.4275, time=89.81s\n",
            "iter 69000: dev acc=0.3106\n",
            "Iter 70000: loss=1843.3419, time=91.09s\n",
            "iter 70000: dev acc=0.3088\n",
            "Iter 71000: loss=1856.8599, time=92.43s\n",
            "iter 71000: dev acc=0.3143\n",
            "Iter 72000: loss=1857.4524, time=93.79s\n",
            "iter 72000: dev acc=0.3188\n",
            "new highscore\n",
            "Iter 73000: loss=1771.7404, time=95.02s\n",
            "iter 73000: dev acc=0.3152\n",
            "Iter 74000: loss=1892.7361, time=96.33s\n",
            "iter 74000: dev acc=0.3143\n",
            "Iter 75000: loss=1926.4628, time=97.72s\n",
            "iter 75000: dev acc=0.3161\n",
            "Iter 76000: loss=1844.6261, time=99.04s\n",
            "iter 76000: dev acc=0.3224\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 77000: loss=1964.5827, time=100.29s\n",
            "iter 77000: dev acc=0.3288\n",
            "new highscore\n",
            "Iter 78000: loss=1665.0879, time=101.60s\n",
            "iter 78000: dev acc=0.3261\n",
            "Iter 79000: loss=1766.1141, time=102.90s\n",
            "iter 79000: dev acc=0.3233\n",
            "Iter 80000: loss=1759.2008, time=104.15s\n",
            "iter 80000: dev acc=0.3270\n",
            "Iter 81000: loss=1826.6837, time=105.51s\n",
            "iter 81000: dev acc=0.3324\n",
            "new highscore\n",
            "Iter 82000: loss=1722.5923, time=106.84s\n",
            "iter 82000: dev acc=0.3261\n",
            "Iter 83000: loss=1689.0863, time=108.12s\n",
            "iter 83000: dev acc=0.3270\n",
            "Iter 84000: loss=1615.2339, time=109.39s\n",
            "iter 84000: dev acc=0.3333\n",
            "new highscore\n",
            "Iter 85000: loss=1708.5323, time=110.73s\n",
            "iter 85000: dev acc=0.3224\n",
            "Shuffling training data\n",
            "Iter 86000: loss=1710.0623, time=112.14s\n",
            "iter 86000: dev acc=0.3243\n",
            "Iter 87000: loss=1628.9270, time=113.41s\n",
            "iter 87000: dev acc=0.3324\n",
            "Iter 88000: loss=1502.5178, time=114.67s\n",
            "iter 88000: dev acc=0.3342\n",
            "new highscore\n",
            "Iter 89000: loss=1576.1861, time=115.92s\n",
            "iter 89000: dev acc=0.3361\n",
            "new highscore\n",
            "Iter 90000: loss=1541.5525, time=117.25s\n",
            "iter 90000: dev acc=0.3406\n",
            "new highscore\n",
            "Iter 91000: loss=1672.6537, time=118.49s\n",
            "iter 91000: dev acc=0.3288\n",
            "Iter 92000: loss=1607.1504, time=119.72s\n",
            "iter 92000: dev acc=0.3261\n",
            "Iter 93000: loss=1566.2720, time=121.18s\n",
            "iter 93000: dev acc=0.3270\n",
            "Shuffling training data\n",
            "Iter 94000: loss=1612.3327, time=122.49s\n",
            "iter 94000: dev acc=0.3261\n",
            "Iter 95000: loss=1385.7244, time=123.73s\n",
            "iter 95000: dev acc=0.3315\n",
            "Iter 96000: loss=1434.1842, time=125.05s\n",
            "iter 96000: dev acc=0.3324\n",
            "Iter 97000: loss=1515.1596, time=126.39s\n",
            "iter 97000: dev acc=0.3297\n",
            "Iter 98000: loss=1476.5007, time=127.63s\n",
            "iter 98000: dev acc=0.3333\n",
            "Iter 99000: loss=1427.1491, time=128.86s\n",
            "iter 99000: dev acc=0.3315\n",
            "Iter 100000: loss=1531.9677, time=130.11s\n",
            "iter 100000: dev acc=0.3306\n",
            "Iter 101000: loss=1445.3010, time=131.43s\n",
            "iter 101000: dev acc=0.3315\n",
            "Iter 102000: loss=1519.4819, time=132.65s\n",
            "iter 102000: dev acc=0.3333\n",
            "Shuffling training data\n",
            "Iter 103000: loss=1404.4551, time=133.88s\n",
            "iter 103000: dev acc=0.3342\n",
            "Iter 104000: loss=1357.8430, time=135.20s\n",
            "iter 104000: dev acc=0.3388\n",
            "Iter 105000: loss=1360.8229, time=136.45s\n",
            "iter 105000: dev acc=0.3361\n",
            "Iter 106000: loss=1304.6703, time=137.67s\n",
            "iter 106000: dev acc=0.3406\n",
            "Iter 107000: loss=1401.3762, time=138.97s\n",
            "iter 107000: dev acc=0.3370\n",
            "Iter 108000: loss=1393.9590, time=140.21s\n",
            "iter 108000: dev acc=0.3361\n",
            "Iter 109000: loss=1308.4058, time=141.44s\n",
            "iter 109000: dev acc=0.3388\n",
            "Iter 110000: loss=1332.1746, time=142.87s\n",
            "iter 110000: dev acc=0.3379\n",
            "Stopping early because there was no improvement for 20000 steps\n",
            "Done training\n",
            "Loading best model\n",
            "best model iter 90000: train acc=0.5324, dev acc=0.3406, test acc=0.3303\n",
            "Test accuracy: 0.3244343891402715 +- 0.00419620746402519\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBhQ4oR78xYe"
      },
      "source": [
        "def plot_results(losses, accuracies, filename=None):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    for run in losses:\n",
        "        plt.plot(run)\n",
        "    plt.xlabel(\"Steps (in thousand)\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.subplot(1, 2, 2)\n",
        "    for run in accuracies:\n",
        "        plt.plot(run)\n",
        "    plt.xlabel(\"Steps (in thousand)\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "\n",
        "    if filename is not None:\n",
        "        os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "        plt.savefig(filename)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvYLj8LIAzfS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "8e28c70c-74ac-4c31-dd40-d4fa46f72567"
      },
      "source": [
        "# This will plot the validation accuracies across time.\n",
        "plot_results(bow_losses, bow_accuracies, \"fig/bow.pdf\")"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1200x600 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Psxh-Le1BMDQ"
      },
      "source": [
        "Please note that we set the number of iterations to 30K as an indicative value, after which we simply stop training without checking for convergence. You should choose an appropriate number of iterations and motivate your decision. **This holds for all pre-set numbers of iteration in the following code blocks.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9mB1_XhMPNN"
      },
      "source": [
        "# CBOW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWk78FvNMw4o"
      },
      "source": [
        "We now continue with a **continuous bag-of-words (CBOW)** model. (*This is not the same as the word2vec CBOW model!*)\n",
        "\n",
        "It is similar to the BOW model above, but now embeddings can have a dimension of *arbitrary size*. \n",
        "This means that we can choose a higher dimensionality and learn more aspects of each word. We will still sum word vectors to get a sentence representation, but now the size of the resulting vector will no longer correspond to the number of sentiment classes. \n",
        "\n",
        "So to turn the size of our summed vector into the number of output classes, we can *learn* a parameter matrix $W$ and multiply it by the sum vector $x$: $$Wx$$\n",
        "If the size of $x$ is `d x 1`, we can set $W$ to be `5 x d`, so that the output of the matrix multiplication will be the of the desired size, `5 x 1`. Then, just like for the BOW model, we can obtain a prediction using the argmax function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIjrCPfCwsXI"
      },
      "source": [
        "## Exercise: implement and train the CBOW model\n",
        "\n",
        "Write a class `CBOW` that:\n",
        "\n",
        "- has word embeddings with size 300\n",
        "- sums the word vectors for the input words (just like in `BOW`)\n",
        "- projects the resulting vector down to 5 units using a linear layer and a bias term (check out `nn.Linear`)\n",
        "\n",
        "Train your CBOW model and plot the validation accuracy and training loss over time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEV22aR2MP0Q"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "class CBOW(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim, vocab, num_classes):\n",
        "        super(CBOW, self).__init__()\n",
        "        \n",
        "        self.vocab = vocab\n",
        "\n",
        "        # this is a trainable look-up table with word embeddings\n",
        "        self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # linear projection layer\n",
        "        self.linear = nn.Linear(embedding_dim, num_classes)\n",
        "        \n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        # this is the forward pass of the neural network\n",
        "        # it applies a function to the input and returns the output\n",
        "\n",
        "        # this looks up the embeddings for each word ID in inputs\n",
        "        # the result is a sequence of word embeddings\n",
        "        embeds = self.embed(inputs)\n",
        "\n",
        "        # project from embedding to output space\n",
        "        out = self.linear(embeds)\n",
        "        \n",
        "        # the output is the sum across the time dimension (1)\n",
        "        # with the bias term added\n",
        "        logits = out.sum(1)\n",
        "\n",
        "        return logits\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ui2g8jStFqnT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0ebe71d-a7ac-487c-b17e-8b5d69403efa"
      },
      "source": [
        "NUM_CLASSES = 5\n",
        "EMBED_DIM = 300\n",
        "# train CBOW\n",
        "\n",
        "cbow_losses, cbow_accuracies, cbow_best_accs = train_loop(\n",
        "    lambda: CBOW(len(v.w2i), EMBED_DIM, vocab=v, num_classes=NUM_CLASSES).to(device),\n",
        "    lambda model: optim.Adam(model.parameters(), lr=0.0005),\n",
        "    num_iterations=0, print_every=1000, eval_every=1000, patience=20)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CBOW(\n",
            "  (embed): Embedding(18280, 300)\n",
            "  (linear): Linear(in_features=300, out_features=5, bias=True)\n",
            ")\n",
            "Shuffling training data\n",
            "Iter 1000: loss=2792.6084, time=3.00s\n",
            "iter 1000: dev acc=0.2361\n",
            "new highscore\n",
            "Iter 2000: loss=2305.9903, time=6.50s\n",
            "iter 2000: dev acc=0.2607\n",
            "new highscore\n",
            "Iter 3000: loss=2107.6914, time=10.04s\n",
            "iter 3000: dev acc=0.2570\n",
            "Iter 4000: loss=2034.0056, time=13.37s\n",
            "iter 4000: dev acc=0.2770\n",
            "new highscore\n",
            "Iter 5000: loss=1920.1800, time=16.92s\n",
            "iter 5000: dev acc=0.3006\n",
            "new highscore\n",
            "Iter 6000: loss=1943.7141, time=20.48s\n",
            "iter 6000: dev acc=0.3152\n",
            "new highscore\n",
            "Iter 7000: loss=1859.6490, time=23.95s\n",
            "iter 7000: dev acc=0.2825\n",
            "Iter 8000: loss=1843.9017, time=27.37s\n",
            "iter 8000: dev acc=0.2834\n",
            "Shuffling training data\n",
            "Iter 9000: loss=1734.3594, time=30.78s\n",
            "iter 9000: dev acc=0.2943\n",
            "Iter 10000: loss=1606.4291, time=34.17s\n",
            "iter 10000: dev acc=0.3143\n",
            "Iter 11000: loss=1641.6246, time=37.53s\n",
            "iter 11000: dev acc=0.3170\n",
            "new highscore\n",
            "Iter 12000: loss=1599.6880, time=41.05s\n",
            "iter 12000: dev acc=0.3097\n",
            "Iter 13000: loss=1652.7661, time=44.42s\n",
            "iter 13000: dev acc=0.2970\n",
            "Iter 14000: loss=1669.9496, time=47.82s\n",
            "iter 14000: dev acc=0.2916\n",
            "Iter 15000: loss=1606.0017, time=51.19s\n",
            "iter 15000: dev acc=0.2934\n",
            "Iter 16000: loss=1612.8594, time=54.51s\n",
            "iter 16000: dev acc=0.3143\n",
            "Iter 17000: loss=1630.8504, time=57.88s\n",
            "iter 17000: dev acc=0.3079\n",
            "Shuffling training data\n",
            "Iter 18000: loss=1321.8288, time=61.21s\n",
            "iter 18000: dev acc=0.3043\n",
            "Iter 19000: loss=1315.5358, time=64.62s\n",
            "iter 19000: dev acc=0.3297\n",
            "new highscore\n",
            "Iter 20000: loss=1291.7773, time=68.24s\n",
            "iter 20000: dev acc=0.3179\n",
            "Iter 21000: loss=1372.1981, time=71.62s\n",
            "iter 21000: dev acc=0.3106\n",
            "Iter 22000: loss=1426.1335, time=74.96s\n",
            "iter 22000: dev acc=0.3406\n",
            "new highscore\n",
            "Iter 23000: loss=1342.3396, time=78.42s\n",
            "iter 23000: dev acc=0.3197\n",
            "Iter 24000: loss=1363.5574, time=81.73s\n",
            "iter 24000: dev acc=0.2970\n",
            "Iter 25000: loss=1341.3090, time=85.02s\n",
            "iter 25000: dev acc=0.3433\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 26000: loss=1161.7568, time=88.50s\n",
            "iter 26000: dev acc=0.2988\n",
            "Iter 27000: loss=913.5062, time=91.87s\n",
            "iter 27000: dev acc=0.3088\n",
            "Iter 28000: loss=1062.1197, time=95.20s\n",
            "iter 28000: dev acc=0.3270\n",
            "Iter 29000: loss=945.9639, time=98.59s\n",
            "iter 29000: dev acc=0.3115\n",
            "Iter 30000: loss=1105.0460, time=101.91s\n",
            "iter 30000: dev acc=0.3097\n",
            "Iter 31000: loss=1087.6839, time=105.28s\n",
            "iter 31000: dev acc=0.3442\n",
            "new highscore\n",
            "Iter 32000: loss=1029.9491, time=108.76s\n",
            "iter 32000: dev acc=0.3215\n",
            "Iter 33000: loss=1040.5410, time=112.13s\n",
            "iter 33000: dev acc=0.3079\n",
            "Iter 34000: loss=1183.8160, time=115.43s\n",
            "iter 34000: dev acc=0.3161\n",
            "Shuffling training data\n",
            "Iter 35000: loss=728.1967, time=118.76s\n",
            "iter 35000: dev acc=0.3115\n",
            "Iter 36000: loss=721.3108, time=122.11s\n",
            "iter 36000: dev acc=0.3043\n",
            "Iter 37000: loss=758.6424, time=125.43s\n",
            "iter 37000: dev acc=0.2879\n",
            "Iter 38000: loss=796.5958, time=128.76s\n",
            "iter 38000: dev acc=0.3152\n",
            "Iter 39000: loss=883.5222, time=132.06s\n",
            "iter 39000: dev acc=0.3361\n",
            "Iter 40000: loss=768.2118, time=135.35s\n",
            "iter 40000: dev acc=0.3279\n",
            "Iter 41000: loss=903.8823, time=138.68s\n",
            "iter 41000: dev acc=0.3288\n",
            "Iter 42000: loss=823.1140, time=142.02s\n",
            "iter 42000: dev acc=0.3061\n",
            "Shuffling training data\n",
            "Iter 43000: loss=832.7794, time=145.37s\n",
            "iter 43000: dev acc=0.3270\n",
            "Iter 44000: loss=546.3686, time=148.68s\n",
            "iter 44000: dev acc=0.3006\n",
            "Iter 45000: loss=554.3492, time=152.07s\n",
            "iter 45000: dev acc=0.3124\n",
            "Iter 46000: loss=531.8669, time=155.41s\n",
            "iter 46000: dev acc=0.3306\n",
            "Iter 47000: loss=582.7734, time=158.73s\n",
            "iter 47000: dev acc=0.3152\n",
            "Iter 48000: loss=644.4054, time=162.02s\n",
            "iter 48000: dev acc=0.3179\n",
            "Iter 49000: loss=615.0673, time=165.33s\n",
            "iter 49000: dev acc=0.3297\n",
            "Iter 50000: loss=681.6961, time=168.66s\n",
            "iter 50000: dev acc=0.3106\n",
            "Iter 51000: loss=699.7057, time=172.02s\n",
            "iter 51000: dev acc=0.3143\n",
            "Stopping early because there was no improvement for 20000 steps\n",
            "Done training\n",
            "Loading best model\n",
            "best model iter 31000: train acc=0.6993, dev acc=0.3442, test acc=0.3679\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frXis2bS4vZz",
        "outputId": "1042edf8-7e59-466c-e5f4-939dbbf9a532",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        }
      },
      "source": [
        "plot_results(cbow_losses, cbow_accuracies, \"fig/cbow.pdf\")"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Accuracy')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAGwCAYAAABB4NqyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd3xUdb4//teZmt57gQABQg81goqoFFdWBVdF7yq7rKu7ulgubAPbXUVRv/byW3fZq7LoKupFFxsuRBBROoQaQk8vpE2SSTIzmTm/P2bOJIGUKWdyJpnX8/HI43FJJief7EXyzvvzLoIoiiKIiIiIAohK6QMQERER9TUGQERERBRwGAARERFRwGEARERERAGHARAREREFHAZAREREFHAYABEREVHA0Sh9AH9ks9lQVlaG8PBwCIKg9HGIiIjIBaIoorGxESkpKVCpes7xMADqQllZGdLT05U+BhEREXmguLgYaWlpPb6GAVAXwsPDAdj/B4yIiFD4NEREROSKhoYGpKenO3+O94QBUBeka6+IiAgGQERERP2MK+UrLIImIiKigMMAiIiIiAIOAyAiIiIKOAyAiIiIKOAwACIiIqKAwwCIiIiIAg4DICIiIgo4DICIiIgo4DAAIiIiooDDAIiIiIgCDgMgIiIiCjgMgIiIiCjgMAAiIiKfarVYYbOJSh+DqBMGQERE5DP1zWZctjoXS97dq/RRiDphAERERD5zuMSA+mYLdp6pgSgyC0T+gwEQERH5TFFtMwDAbLWhxmhW+DSXarPa8PG+YlQ2tCp9FOpjDICIiMhnpAAIAMrr/S/I2HCgFH/45DDuWLMLzeY2pY9DfYgBEBER+UxhjdH5f5cbWhQ8Sdd2nK4GAJy9YMRTX+QrfBrqSwyAiIjIZ4pq24OecoN/ZYBEUcTe87XOP3+wpwibjlYoeCLqSwyAiIjIJ0RRRFGnDJB/BUAldS0oN7RCoxLwi+mDAQB/3nAYFX52TvINBkBEROQTNUYzjGar88/+dgUmZX/GpkbikfmjMS41EvXNFiz7KI9ziwIAAyAiIvKJjgXQgP9lgKQAaNqQGOg0Krx6ezaCtWr8eKYGf//+rMKnI19jAERERD5RVGMPgIK09h81/pYB2nPOHgBNzYgBAAyND8P/3DgaAPDCNwU4XFKv2NnI9xgAERGRTxQ6AqBJg6IBAJUGk99cLVU3mXDmgr0+acrgaOf7b5uSjp+MTUKbTcRDH+bBaGJr/EDFAIiIiHxCugKbmhEDQfCvYYj7HNdfIxLDEB2qc75fEASsvnkckiODcK7aiCc/P67UEcnHGAAREZFPFNXaMyzDEsIQH6YHAL/psNpzrg5A+/VXR1EhOrx0WzYEAVi/rxhfHSnv6+NRH2AAREREPiFdgQ2KCUFyZBAAoMxP6oA6FkB3ZfqwWNx31TAAwJ//7zDK6v3j3CQfBkBERCS7FrMVVY0mAMDgmBAkRwYD8I8MUJOpDcfKDAC6zgBJ/nvOCExIi0RDaxv+e30erH5Sv0TyYABERESyK66zZ3/CgzSICtEiyY8yQAcK62ATgbToYKREBXf7Oq1ahVdvn4gQnRq7z9Xire/O9OEpydcYABERkeyk66/BsSEQBAEpUfYAyB8yQM7rrx6yP5KMuFD85cYxAICXN59EXjFb4wcKBkBERCQ7aQnqoJgQAECS4wrMHzbCO+f/dFP/c7FbJqdh/vhkR2v8QTSxNX5AYABERESyK66VCqBDAQApjiuw8gZlr8BMbVZnFqen+p+OBEHAMwvGISUyCIU1zfifjcd8eUTqIwyAiIhIdoW17VdgAJAc1V4EreQwxKOlBpjabIgN1WFYfKjLnxcZosUrt0+ESgA+2V+Czw+V+fCU1BcYABERkeyKOrTAA0BCuB6CAFisoqLDEHc7rr+mZERDEAS3PnfakBj87upMAMDKT4+gpK65l88gf8YAiIhk9dZ3Z3DjGztQ6ycTf6nvWW0iSursV11SAKRVq5AQbh+GqOROsL0X7f9y14PXDkd2ehQaW9uw/KNDEEXfZrO+PFyOmc9vxcGiOp9+nUDEAIiIZCOKIv6+/SwOlxiw+XiF0schhVQ0tMJstUGjEjq1mTsLoRXqBLPaROwrtAcS3Q1A7I1WrcJrt09EkFaF3edqsf1UtZxH7KTVYsX/fH4MRbXNWPvjeZ99nUDFAIiIZFNU2+zM/BwpNSh8GlKK1AGWFh0Mtar9mslZCK3QVOWCikY0trYhVKfG6OQIj58zKDYEd+YMBmBvjfdVFui9XYW44Bgmue3kBQ5ilBkDICKSTccZKUdKGxQ8CSnJ2QEW27nIOMnZCaZMBkia/zNpcDQ0au9+/P3mqmEI0qqQV1yP705ekON4nbSYrXjru7POP9c3W3gNJjMGQEQkm4NF7QFQfnkDLFabgqchpTiHIDrqfyQpCs8C2uPGAMTexIfrcddljizQllOyZ4He312I6iYT0qKDcf24JABA7okqWb9GoGMARESy6fgbqrnNhpOVjQqehpRSWNu5A0wiZYCUmAYtimJ7AbSH9T8Xu3emPQt0qLge22TMAtmzP/a1Gw9ck4l5Y+wB0FYGQLJiAEREsmi1WHGszH7tJc1+Oco6oIDUfgV2UQYoSrl9YEW1zahqNEGrFpCdHiXLMztmgV6RMQtkz/6YkR4TjJsnpeGqEfFQCcCJikaUciu9bPwiAHrzzTeRkZGBoKAg5OTkYM+ePd2+dsOGDZgyZQqioqIQGhqK7OxsrFu3rtvX//a3v4UgCHjllVd8cXQicjhWZkCbTURcmA7XOX5jPVzCACgQddwD1pHUBVbZ0PfDEKX1F+PTohCkVcv2XKkW6FBxPbYVeJ8Faja3tWd/rh4OrVqFqBAdJg+OBgB8yyyQbBQPgNavX49ly5bhiSeewIEDBzBhwgTMmzcPVVVd/z85JiYGjzzyCHbu3InDhw9jyZIlWLJkCb755ptLXvvpp59i165dSElJ8fW3QRTwpPqf7PRojE2NBMAMUCAyNFtgaLEAANKjOwdACeF6qBzDEKuNpj491x4v5/90Jy5Mj8XTMwAAr2zxviPs/V1FzuzPwkmpzvdfnZUAAPg2v9Kr51M7xQOgl156Cffccw+WLFmC0aNH46233kJISAjefvvtLl8/a9YsLFy4EKNGjcKwYcPw0EMPYfz48dixY0en15WWluKBBx7A+++/D61W2xffClFAO+joAJs4KArj0+wBUH5FI8xtLIQOJEWO66+4MD1C9ZpOH9OqVYh3DEPs6zog5wb4IdGyP/vemUMRrFXjUInBqyxQs7kNf9veOfsjuTYrEQDw45katJit3h2YACgcAJnNZuzfvx+zZ892vk+lUmH27NnYuXNnr58viiJyc3NRUFCAmTNnOt9vs9lw11134Q9/+APGjBnT63NMJhMaGho6vRGRe/KK2gOgQTEhiAjSsBA6ABXW2mcAXXz9JUl2XIOV9WEnWFVjK87XNEMQgMmD5c0AAVIWSOoI8zwL9N4ue+3PoJiQTtkfABiRGIbUqGCY2mz48Yzvhi8GEkUDoOrqalitViQmJnZ6f2JiIioqup8iazAYEBYWBp1Oh/nz5+P111/HnDlznB9/7rnnoNFo8OCDD7p0jtWrVyMyMtL5lp6e7tk3RBSgqhpaUVrfAkGw11gIgsBrsABVWNN1B5gk2dkJ1nfFvHvP2bsTs5IiEBnsmxuBexxZoMMlBmwtcL9Op9nchr855v4svSazU/YHsG+kv0a6BmMdkCwUvwLzRHh4OPLy8rB37148/fTTWLZsGbZt2wYA2L9/P1599VW8++67Li+6W7FiBQwGg/OtuLjYh6cnGnik66+RieEIc1x7jHNcg3EidGAp7qYFXpKswDoM5/VXhvzXX5K4MD0Wz/C8I+y9XYWoMTqyPxNTu3xNxwDI1zvIAoGiAVBcXBzUajUqKzsXdVVWViIpKanbz1OpVMjMzER2djaWL1+OW265BatXrwYAfP/996iqqsKgQYOg0Wig0WhQWFiI5cuXIyMjo8vn6fV6REREdHojItcd7HD9JRmXygAoEHXXASaRMkB9GQDtkXn+T3fuvbI9C+ROlqZj9ueBLrI/kunDYhGkVaHc0IoTFbxa9paiAZBOp8PkyZORm5vrfJ/NZkNubi6mT5/u8nNsNhtMJntHwV133YXDhw8jLy/P+ZaSkoI//OEPXXaKEZH3pAGIE9Pbf8OWAqAT5SyEDiRFvWWAoqQAqG+uwBpaLcivsNd1yjEBuiexHmaB1u20Z38Gx3af/QGAIK0alw+LA8BrMDkofgW2bNkyrFmzBmvXrkV+fj7uu+8+GI1GLFmyBACwePFirFixwvn61atXY/PmzTh79izy8/Px4osvYt26dbjzzjsBALGxsRg7dmynN61Wi6SkJIwcOVKR75FoIGuz2pzzfjpmgJyF0FYWQgcKc5vNOeTw4iGIkr7OAO0vrIMo2jNSCRFBPv969145FCE6NY6UupYFsnd+OWp/rs7sdUfZNaNYByQXTe8v8a1FixbhwoULePzxx1FRUYHs7Gxs2rTJWRhdVFQElar9L4TRaMT999+PkpISBAcHIysrC++99x4WLVqk1LdAFNAKKhvRYrEiXK/BsPgw5/sFQcC4tEj8cLoGR0oNzqJoGrhK6pohikCITo34MH2Xr0m+aBiiSuVaraan9vpo/k93Yh1zgd767gxe2XIK12Ql9FiP+s+dhah1IfsjuXqkPQA6WFSHWqMZMaE62c4eaBQPgABg6dKlWLp0aZcfk4qbJatWrcKqVavcev758+c9PBkR9UbaAD8hPeqSH2ZjU9sDoDuUOBz1qY47wLr7od9pGGKTyedZGan+x9fXXx3dO3Mo/rnzPI6UGpCbX4XZoxO7fJ3R1Ia/b5dqf4a7tKE+JSoYWUnhOFHRiO9OVmHhxDQ5jx5QFL8CI6L+rasCaMn4VPv72AofGKQOsPRu6n8AQKNWISG8b67BWi1W5/WsrwugO4oJ1eEXMzIAAK/kdj8XaN0ue/YnIzYEC7Jd31hwrfMaTL4FrIGIARARecVZAN1FAMRC6MDi7ADrIQAC2rfC+7oQ+lBxPcxWG+LD9cjopibJV+5x1AIdLW3AlvxL63U8yf5IpHb47wqq0Gblf1eeYgBERB4zNFtw5oJ98m92+qUzVtJjghEZrGUhdIDorQVekhLVNxmg9vk/MS7PhZNLpyxQF9OhpdqfIXGhuMmN7A9g/28tOkSLhtY27C+sk+vIAYcBEFEAMLRYsPRfB7DpaLmsz80rsV9/ZcSGdFmMKQgC5wEFEFeuwAAgKaJvhiHuOW8PDqb6cABiT+65cihCdWocK+ucBbJnfxw7v67pvfPrYmqVgFkj2Q3mLQZARAFg/d4ifHG4HE9sPAabTb4JsnnODfCXXn9JpO4vqRaDBiZRFJ0zgAbHhvb42r7IAFltIg44siN9Wf/TUXdZoLU7z6Ou2YIhcaG4cYJ72R8J12J4jwEQUQD45ph92nplgwn7i+RLmR8slup/uv8Nexx3ggWEC40mtFisUAlAalRwj6911gDV+64GKL+8AU2mNoTrNchKUm66f8cs0ObjlWgytWHN9vapz+5mfyQzR8RDrRJwqqrJmXkj9zAAIhrgqhpbcaBD0PPlYXmuwURR7LEDTDLesRPsREUDTG1WWb42+R8p+5McGQydpucfLX2xD0xqf5+cEQ21j2cN9SQ6VIdfXp4BwD4deu2P9uzPUC+yPwAQGazFlMH2XzyYBfIMAyCiAS43vwqiCARr1QCAr4+Wy3INdq7aCEOLBXqNqsffsNOi7YXQFquIkxVNXn9d8k+uFkAD7VdglQ2tsMp4JduRVADdVwMQe/LrK+xZoOPlDXhly0kAwAPXep79kUjXYLkMgDzCAIhogPvPsQoAwD0zhyJcr5HtGkwagDguNbLH3/hZCB0YCmtdD4Diw+zDENtsImqaTLKfRRTF9gGICtX/dNQxC2SxihgaF4obxnue/ZFI84B2namB0dTm9fMCDQMgogGsydSGH07XAABuGJ+MOWPsE2nluAY76EIBtGRcGgOggc7VDjDAPgwx0TEBuswH12Bnq42oMZqh06icV7BK+/UVQxGmty9fePBa9+b+dGdYfBjSY4Jhttrww+lqr58XaBgAEQ1g2wqqYLbaMCQuFJkJYZg/LhkA8NUR76/BXCmAlrRngOq9+prkvwpr7POgBsf03AEmkQqhK3wwDFHa/5WdHgW9Ri378z0RHarD3xdPxl9uHONV7U9HgiDg2iz7LzVbC3gN5i4GQEQD2H8c3V9zRydCEARcMTwO4UEaVDWasM+LAWotZivyy+2DDXsqgJZIAVBBRSMLoQeoIjeuwAAgxVEIXVYvfwZoz/m+3//lihnD4vCLGRmyLoC9ukM7fHcrN6hrDICIBihzmw1bHcWRcx1XX3qNGnNGS9dgZR4/+0ipAVabiMQIPZIje19mmRYdjKgQFkIPVEZTG6qbzABcuwIDOmSAGuQPgJwF0H5Q/+NrOUNiEKxVo7LBhGNlDUofp19hAEQ0QO06W4NGUxviwvSY2GFNxU/H26/Bvj5a4XEHTp50/ZUe7dKKgY6F0IdlvAarbjKh1cKMktKk7E9UiBaRwVqXPkcKnMtkngVUYWhFcW0LVAIwyYXsZH8XpFXjiuFxANgO7y4GQEQD1H+O27u/5oxO7JRyvyIzvv0azPGbsrucBdBu/IAZK/NAxKOlBsxY/S0e+vCgLM8jz7m6BLUjaRZQhcxF0NL11+iUCIQHuRaM9XecCu0ZBkBEA5DNJrbX/ziuvyQ6jQpzRycBsBdDe8I5ANGFDjDJeJlb4dftLITZasPm45U+aaUm17nTASZJ9tE6DKkA2h/m//SVqx17wQ6V1KOa/y24jAEQ0QB0qKQeVY0mhOrUmDEs9pKPS9dgX3lwDVZuaEFFQyvUKsHZ3u6KsTIWQhtNbfjCUcNkEzkITmmFtY4OMBcLoIH2KzC5hyHu9dMCaF9KigzCmJQIiCKwreCC0sfpNxgAEQ1A/zluz/7Mykrosg348sw4RARpcMGDazBpAWpWUjhCdBqXP69jIXRBRaNbX/NiXx4uh9HcHkRJ2S5SRvsVmGst8ACQEB4EtUpAm02ULWtR32zGCcffrSkBFAABwLXOazD+t+AqBkBEA5A0/XnemKQuP67TqDDX8bEv3bwGO1jc+/6vrsg5EXr9vmIAcM5T+f7UBTSbOQlXKZ5cgalVAhLC9QDkuwbbd95enD80LhTxjmcHCqkd/vuT1TC32RQ+Tf/AAIhogDld1YQzF4zQqgXMGhnf7eukoYjudoMddKzRyE7vfQDixZwBUInnAdDpqibsL6yDWiXgkfmjkB4TDFObDdtPMvWvhDarDSV19k4ud67AgPZrMLm2wjuvvwKg/f1iE9KiEBuqQ6OpzePmhkDDAIhogJG6v6YPi0NED10wHa/B9rr4D6bFasNhR/DibgYIgCwZoI8d2Z9ZI+KRGBHkLOjmNZgyyg2taLOJ0GlUSIrofSZUR3JvhT/gCM4nD3Y/OO/vVCoBs0ayG8wdDICIBpiO05970vEazNVusBPljTC12RAZrMWQWNfrPSRSIfTJykaP5vdYrDb834ESAMBtU9MBtH+fuSeqYLEy9d/XpBlA6dHBbk84dmaAZFiH0TE4nxSAARDQvhyVAZBrGAARDSCVDa3OLe1zegmAAGC+1A12xLVrMGkAYnZ6lEfj/NOigxHtRSH01hNVqG4yIy5M55x9MiUjBjGhOhhaLM4WaOo7UgH0IDfqfyRJkfK1wnsbnA8EVwyPg0Yl4Gy1EeeqjUofx+8xACIaQDY7ur8mDopybtvuyeXD7Ndg1U0m7HEheHDO//Fwwq4gCM4skCfXYB85rr9unpQGrWObtlolYLbjN1+p+436TnsLvPtBR0qUfFdgB70MzgeCiCCts/6JWaDeMQAiGkCkAECqi+mNTqNydoq5cg0mdYBluzEA8WLjPJwIXdXQiq2OGSe3TUnv9LH2OqAKLoTsY0UyZIDkmAbtnE7uxd/NgUDKjG5lANQrBkBEA0RDqwU7z1QDuHT6c0/mu7gbrM5odqbVvfkhM94xPPGwm51gnxwogdUmYvLgaGQmhHX62BXD4xCiU6PM0IqjpVwI2ZekGiBPAiBpI3yFDMMQ8zwczzDQSO3wu8/VoMnE0RA9YQBENEBsPVEFi1XEsPhQDIsP6/0THC7PjENksLbXa7C8EvsPmKHxoYgK0Xl8Tk8KoUVRxMf77MXPiy7K/gD2hZBXjbC3/EtdcOR7oig6M0DutsADQHy4HmqVAKuXwxDlCs4HgqFxociIDYHFKmLHKY6G6AkDIKIBwnn91c3ww+5o1SrMc2SMvjxS1u3r2vd/eddhkxplL4Rus7leCL33fB3OVRsRolPjekfG6mJS1ovt8H2nrtmCRkeWwZ0hiBK1SkCiY2ChN1vhpeyPt8H5QCAIgjMLtOkofxnoCQMgogHA1GbFNsedf3fTn3tyvWMo4qYersGkAYjeXjEIgoBxafZnHHaxDmj9Xnvx80/HJyNM3/X6jWtGJkKtElBQ2Yjz7IDpE9L1V2KEHkHaS1euuEKOOiDn300vg/OB4qfj7RPS/32ozBkc0qUYABENAD+eqYHRbEVihN65dd0d7ddgZuw+V3PJx2020fkPqRxXDONSIwAAR12oA2pstTgLtBdNvfT6SxIZosVlQ+0dMJvZDdYnCmscHWBu7AC7WLKjE6zMmwBI+rsZ4PU/ksmDo7FwYipEEVix4QjaOB+rSwyAiAYA6dpnzuhEj1qAO16DddUNdra6CY2tbQjSqpCVFO7dYeHeROgvDpejxWLF0PhQTBrU82/4UjfYN8eY+u8Lzg4wD+p/JMkRUgbIsyuwjsH5xACv/+nokfmjEBWiRX55A97+4ZzSx/FLDICI+jmrTXRmPFxtf+/KfEfavKtrMKn+Z3xaFDRq7//ZkK7AXCmElq6/Fk1JhyD0HNxJwx/3F9XhQqM8G8ape950gEm8zQDJHZwPFHFheqz8ySgAwMubTzkX1lI7BkBE/VxecR2qm0wID9LgsqGxHj9nxrBYRIV0fQ3m6Qb47qREBiEmVIc2m4gTPRRCn6xsRF5xPdQqATdPSuv9uVHBGJ8WCVEEcvN5DeZrhbWed4BJkr2sATogc3A+kNw6JQ3ThsSgxWLF4/8+yhlZF+HfFqJ+Trr+uiYrATqN5/9Ja9UqzHNkkL483PkaTK4OMImrE6E/cmR/rslKQLyjW6g30m4wToX2PW+GIEq83Qjv7XTygUwQBDyzcBy0agFbCy7gqyO8Gu6IARBRPyaKorPexZvrL4nUYr7paIWzcNJoakNBhX24oJw/ZKRi7SMlXXepmNts2HCwFEDXs3+6I40B2HG6moPgfKjVYkVFgz1r410AZL8Cq2w0eTQMsb0DjAFQVzITwnDfrEwAwP98fgwNrRaFT+Q/GAAR9WOnq5pwvqYZOrUKV42M9/p50jVYjdHsHIp4uMQAm2i/tnJlv5ir2jNAXU9uzs2vRK3RjIRwPWa58b0NTwjDkLhQmNts+K6Ag+B8paTOnv0J02sQE+r57J34cD00jmGI7tZtGU1tOFlpv0Kd2EuBfCC7f9YwDI0LxYVGE57fdELp4/gNBkBE/ZiU/bk8M7bb+Tju0KpVuM6RQfnC0Q3WvmJA3h8w4xwrMU51UwgtLT792eQ0t2o7BEHocA3GlL+vdNwC31txek/UKsEZWJe52Qnmq+B8oAnSqrFq4VgAwPu7i7C/sE7hE/kHBkBE/Zin0597Ig1F/MZxDSbXAMSLpUQGIdZRCJ1f3jkLVGFoxXcn7dmbWyf3Xvx8MWkq9LcnqmBu4wwUX5CjA0zi6TBEaQM8sz+9mzEsDj+blAZRBFZuOAILZwMxACLqr8rqW3C4xABBAGaPcn35aW+mD4tFtOMabPe5Wtk7wCQdC6Ev3gz/yf5i2ERgWkYMhrqx10ySnR6NuDA9GlvbuhzsSN4r9GIH2MWkQmh312GwANo9j8wfhegQLQoqG/GP7zkbiAEQUT+1xdHmPXlQtMsdUq6wD0W0Z5T+vv0sLjSaoFEJGJPi/oTp3nQ1ENFmE/GRY/HpbT1Mfu6JWiVgzmj7PiTuBvMNZwZIxgDInQyQKMo7nTwQxITq8Mj80QCAV3NPOrv4AhUDIKJ+SvrBLl33yGm+oxtMuoYanRLh8a6nnkh1QIc7rMTYfa4WRbXNCNNrcP04z6/2pGvBzccrYfOgu4h6JucVmNQJVu5GAFRa3+IMzsd6sP4lUP1sUiqmD41Fq8WGRwN8NhADIKJ+yNBswa6z9qudOTK0v19s+lD7NZjEVy3GUgboVFWTsxBaKn6+YUIyQnSeF3bPGBaLUJ0aFQ2tLi9dJdfYbKIzAPJmD5jEOQvIjSJo6frLV8H5QCUIAp5eOBY6tQrbT17A54cvXX0TKBgAEfVD3xZUos0mYkSiveVbbhq1CteNbQ+sfFVkmuwohLY6CqENLe2LT29zY/ZPV/QaNWZlSddg7AaTU2VjK8xtNmhUAlKivO++ktZhuJMBah/Oyesvdw2ND8PvrrbPBnry82MwNAfmbCAGQET9kHT9NU/G7q+LSd1ggO+KTAVBcF6DHSk14PNDZTC12TAiMUyWug5OhfYNqXYkNTpYlvUTUgaoqtHk8uZydoB557ezhmJYfCiqm8x4NkBnAzEAIupnWi1WZ22OHNOfuzN9aCyuGhGPeWMSZanz6I6zELrE4Lz+us2FxaeuuDorAVq1gNNVTThzocnr55FdoYz1P4B9cadzGGJT78MQTW1WHCuTfzp5INFr1Hhm4TgAwAd7irD3fK3CJ+p7DICI+pkfTlej2WxFcmQQxqZG+OzraNQqrP3VNPztrimyBCPdkQpYN+dX4nCJAVq1gIUTU2V5dkSQFtOHxdmfzyyQbOTYAdZRx2GIrlyD5Zc3wtxmQ0yozqfB+UCXMzTWuWZm5YYjbs3MEkUR56uN+PxQGb46Ut4vi6m9Hx1LRH1KWlQ6d3SiTwOTvjLecQVW76hDmD0qEbFh8rX1zx2diO0nL+CbYxX47VXDZHtuICuSYQv8xZIjg1Ba34Ly+lZgUE6qSvkAACAASURBVM+vlYZzZqdHDYj/BpS04vosbMmvxKmqJqz5/qyzNqgjURRRWNOMw6UGHC014EiJAUfLDGhsbd+197+/mIJrZZxH1hf8IgP05ptvIiMjA0FBQcjJycGePXu6fe2GDRswZcoUREVFITQ0FNnZ2Vi3bp3z4xaLBX/6058wbtw4hIaGIiUlBYsXL0ZZWVlffCtEPtXQasFXR+0B0E0yZUmUlhQRhLiw9l1S3hY/X2yOow7oYFE9qhrcmzRMXZP7CgxonwbtSicYC6DlExWiw6M/HQUAeDX3FM5VG3Gu2oiNh8rwzFf5uOPvuzD+L//BrBe24cEPDuLv289i59kaNLa2QadRIcExg+zDvcVKfhseUTwDtH79eixbtgxvvfUWcnJy8Morr2DevHkoKChAQkLCJa+PiYnBI488gqysLOh0OnzxxRdYsmQJEhISMG/ePDQ3N+PAgQN47LHHMGHCBNTV1eGhhx7CjTfeiH379inwHRLJ5/NDZWi12DA8IWzA/OMvTYTeVnABSRFBmDnC+6WuHSVGBCE7PQp5xfXYnF+Jn+cMlvX5gaioxggAGCRDC7wkxY1OMBZAy2tBdir+b38pdpyuxrUvbkNXY7N0GhVGJUdgXGoExqVGYmxqJEYkhuN8tRFzXt6Ob09UoaqxFQnh/Wcnm+IB0EsvvYR77rkHS5YsAQC89dZb+PLLL/H222/jz3/+8yWvnzVrVqc/P/TQQ1i7di127NiBefPmITIyEps3b+70mjfeeAPTpk1DUVERBg3qJbdK5Mc+2itvkbC/mDUiHtsKLmDxjMFQq+T/vuaNSUJecT3+c4wBkLcaWi2oc1xXyjEFWpIU4do06OomE4prWyAIwPh0DkCUgyAIWLVgLK5/7Xs0m62dgp3xqVEYmxqJ4Ylh0HbR8Tc8MRwTB0XhYFE9Nhwo7VfXzIoGQGazGfv378eKFSuc71OpVJg9ezZ27tzZ6+eLoohvv/0WBQUFeO6557p9ncFggCAIiIrq+jdmk8kEk6m986ChoaHL1xEp6URFAw6VGKBRCVg4aWBcf0nump6B6cPiMCLR/b1frpg7JhHPbTqBH89Uo7HVgvAgbe+fRF2SCqBjQ3UI08v3I0SaJ9TbRvg8x/VXZnwYIvj/R9lkxIVi87Kr0NBiQWZC18FOdxZNScfBonp8tLcYv5k5tN/8cqZoDVB1dTWsVisSEzsXTiUmJqKiovvBZQaDAWFhYdDpdJg/fz5ef/11zJkzp8vXtra24k9/+hPuuOMORER03TGzevVqREZGOt/S0+WtQSCSw0d77fuxZo9KRJyMRcL+QK0SMDIp3Gf/cA6LD8Ow+FBYrCK2FlzwydcIFHLuAOsoybEOo7cMUPv118C4AvYnqVHBGJUc4VbwAwA/nZCCYK0aZ6uN2F9Y56PTyc8viqDdFR4ejry8POzduxdPP/00li1bhm3btl3yOovFgttuuw2iKOKvf/1rt89bsWIFDAaD8624uP8Vc9HAZmqz4tOD0oLQNIVP0z9Ju8E4Fdo77Ssw5A2AUhxF0JUNrT0OQ2zfAM/6H38Rptc49weu70fF0IoGQHFxcVCr1ais7Dyfo7KyEklJ3Q94U6lUyMzMRHZ2NpYvX45bbrkFq1ev7vQaKfgpLCzE5s2bu83+AIBer0dERESnNyJ/suV4FeqaLUiM0GPmcHmLhAOFNBV6W8EFmNqsCp+m/yqUeQaQJNYxDNEmotthiFabiEPFUgDEDJA/WTTVfnPy5ZFyNJnaenm1f1A0ANLpdJg8eTJyc3Od77PZbMjNzcX06dNdfo7NZutUwyMFP6dOncKWLVsQGxsr67mJ+po0IfmWyWmyrB4IRBPSopAYoUeTqQ07z9QofZx+q6jW0QEWK+8Ouo7DEMvqu74GO13VBKPZilCdGsMTwmX9+uSdKYOjMTQ+FM1mK7483D/Gzij+L+myZcuwZs0arF27Fvn5+bjvvvtgNBqdXWGLFy/uVCS9evVqbN68GWfPnkV+fj5efPFFrFu3DnfeeScAe/Bzyy23YN++fXj//fdhtVpRUVGBiooKmM1mRb5HIm+U1bdg+yl73cqtk1mf5imVSnDOBOJuMM/5YgiiRNoJ1l0dkDQAcXxalE+6BclzgiA4Z3j1l2swxdvgFy1ahAsXLuDxxx9HRUUFsrOzsWnTJmdhdFFREVSq9jjNaDTi/vvvR0lJCYKDg5GVlYX33nsPixYtAgCUlpZi48aNAIDs7OxOX2vr1q2XtNET+btP9pdAFIGcITHI8MHm90Ayd3QS3ttVhM3HK/GXG8e4XezpjgNFdXjy8+OYPSoB98wcCr1G7bOv1VcsVpszO+OLFRTJUcFAYV23wxDb6394/eWPbp6Uiv/3TQEOFNXjdFUjMv08S6d4AAQAS5cuxdKlS7v82MXFzatWrcKqVau6fVZGRka/3ElC1BWbTcTH++2/TUl37OS5y4bGIjZUhwuNJvzj+3O4b5ZvZpa0Wqz47/V5KKxpRl5xPT7ZX4InbhyDq0deOty1Pymta4HVJiJI2z4BWE7JkT3vA+MARP+WEB6Eq0cmYEt+JdbvLcYj80crfaQeKX4FRkTd23W2BsW1LQjXa/CTsclKH6ff02lUWHm9NPb/pHOmjdze+PY0CmuaERemR0K4HudrmrHknb2455/7UFzrm6/ZF4o6rMDwxciC5B7WYTS2WnCqqgmAfQcY+SfpF7UNB0ph6aGbzx8wACLyY+sdxc83ZKcgWNf/r1D8wc2TUjF9aCxaLTY8+u+jsmeMT1Y24q3vzgAAVi0Yg9zlV+GeK4dArRKw+XglZr/0HV7LPYVWS//rRDvtCEB8tYG9pwzQ4RIDRBFIjwlGvA+yTySPq0fGIz5cjxqjGbn5VUofp0cMgIj8lKHZgq+P2mfWLJJ5QWggEwQBTy8cC51Ghe0nL+Dzw+WyPdtmE7FywxG02UTMHpWAeWOSEB6kxSPzR+Prh67EZUNjYGqz4aXNJzHvle349kT/Ksb+9yF7d0/OEN901iY7hiGWd9EF1r4Bntdf/kyjVuFmx6R6qXvVXzEAIvJTGw+VwtxmQ1ZSOManceeRnIbGh2Hp1ZkAgCc/Pw6DY7eVt9bvK8a+wjqE6NT4y01jO10TjUgMxwf3XIbX7piIxAg9Cmua8at39+HXa/vHtVhBRSMOFdf7dBWLlAGqarx0GCI3wPcfUjfYtoIqVDb0vtxWKQyAiPyUdP010Baf+ovfXDUUw+JDUd1kwrObTnj9vKrGVqz+Kh8AsGzOCKQ6tpt3JAgCbpyQgtzls/CbmUOhUQnYkm+/Fntly0m/vhaTfpu/dlSCz1axxIXpoVXbhyFWNbbPdhNFEQc5ALHfGBYfhqkZ0bCJ9i5Wf8UAiMgPHSsz4GhpA7RqAQsmDqzFp/5Cr1HjmYXjAAAf7CnCvvO1Xj1v1Rf5aGhtw9jUCPxyRkaPrw3Ta7Di+lHY9PCVuDwzFqY2G17ZcgpzXv4Oufn+dy1mbrPh04OlAHzbjajqMAyxYx1QUW0zao1m6NQqjE7hpP7+QMoCfbyv2G87sxkAEfmhj/fZf2uaOzoJMaE6hU8zcOUMjXXWV63YcATmNs+6VrYVVGHjoTKoBGD1wvEuT+vOTAjHe3fn4M3/moSkiCAU17bg7rX7cPe7e33WoeaJLfmVqDWa+2QVS1edYHmO7M+Y1IgBMU8pEFw/LhmhOjXO1zRj9znvfrnwFQZARH6m1WJ1/rZ9G2f/+NyK67MQG6rDqaomrPn+rNuf32K24rF/HwUA/HLGEIxzs15LEATMH5+M3OVX4bdXDYNWLSD3RBVmv/wdXtrsH9difbmKJbmLrfBS/Q/b3/uPUL0GN0xIAeC/xdAMgIj8zH+OV8LQYkFKZBCuyIxT+jgDXlSIDo/91D6w7bXcUzhfbXTr81/79hSKa1uQHBmEZXNHeHyOUL0Gf/5JFr5+aCauyIyDuc2G13JPYfZL32Hz8UrFrhHKDS3YfrLvVrFIGaCO+8CkDjAOQOxfpF/gvjpSjoZWeRoN5MQAiMjPfLS3/bdt7jvqGzdlp+DK4XEwtdnw6GeuzwY6UdGANdvtWaMnbxqLML33w/UzE8Kw7u5p+P9+PgkpkUEoqWvBPf/ch1+9u9ft4EwOn+wrga0PV7E494E12K/AWi1WHCtrAMAOsP5mYnoUMhPC0Gqx4fND/rcglQEQkR8prm3GD2eqAQC3cvZPnxEEAasWjIVeo8KO09X4d17v/1jbbCJWOGb+zBuT6Fy0Ktd5rh+XjC3Lr8L9s+zXYlsLLmDuy9vx4n8K0GLum2sxm03ER328iiXJcQUmZYCOlRnQZhMRF6ZHWvSlnXXkvwRBcNbYfbTP/7rBGAAR+RFp8enlmbFI99G0Xera4NhQPHjtcADAU18cR32zucfXv7+nCAeL6hGm1+AvN471yZlCdBr88bosbHp4Jq4cHgez1YbXvz2N2S99h2+OVfj8WmzXub5fxZIS1bkIuuMCVI6D6H8WTkqFRiXgUHE9CioalT5OJwyAiPyE1SY6Z2bcxuyPIu65cihGJIahxmjG6q+6nw1U1dCK57+2f/z3c0cgyXFt4yvD4sPwz19Nw1t3TkJqVDBK61vwm3X78ct39uKcD6/FpOvYvlzFkuQchmiCxWpzzv9hAXT/FBemx+xR9uzo+r3+VQzNAIjIT/x4phql9S2ICNJg3pgkpY8TkHQalXM20Pp9xdh9tqbL1/3li+NoNLVhQlok7pqe0SdnEwQB141NxpZlV2Hp1ZnQqVX47uQFzHt5O/7fNyfQbG6T9esZWpRZxRIXah+GKDqGIeYVcQBif3fb1DQAwKcHSzweNeELDICI/IT029GCiakI0nLWiVKmZMTgjmmDAAArPz0CU1vneputJ6rw5eFyqFUCnrl5XJ8Xqgfr1Pj9vJH45r9n4qoR8TBbbXhz6xnMf20HDC3yddpszCuFqc2GkYl9u4ql4zDEQ8X1KK1vgUoAxqcxAOqvZg6PR2KEHnXNFmzxo0GfDICI/ECd0Yz/HLP/w8DrL+X9+bosxIXpceaCEX/7rn02ULO5DY9+Zp/586vLMzAmRbkdbUPiQvHukqn4212TkRihx7lqI56XYaWHRCpavW1q369iSXEUQksZqBGJ4bJ02JEyNGoVbplszwL50zUYAyAiP/BZXinMVhtGJ0dgbCoXnyotMkSLx2+wzwZ6Y+tpnL3QBAB4ZcsplNa3IDUqGP89x/OZP3IRBAHzxiTh1dsnAgDe312E/YV1Xj/3eFkDjpQaoFULWKjAKhapDuhbR7aA83/6P2mG1PZTF1BW39LLq/sGAyAihYmi6PytqK9ajal3N4xPtl8xtdnwyKdHcazMgP/dcQ4A8NSCMQjR+U9G4rKhsbjV8Rv2yg1HYLF6V2chTe5VahVLsqMTzOho9+f8n/4vIy4UOUNiIIrA//nJglQGQEQKO1ragBMVjdBpVLgpO0Xp45CDNBsoSKvCzrM1WPy/e2C1iZg/LhnXZMk380cuK68fhZhQHQoqGz1a6SHpuIrl1ilpch3PLckRnbvqWAA9MEi/4H20vxg2m/ILUhkAESls/b4iAMC8MUmICuHiU3+SHhOCh661X3XVGM0I12ucV2P+JjpUh0fnjwIAvLrlFAprPGuP39xhFcuVPl582p3kqPaBh+F6DYbFhylyDpLXT8YmI1yvQXFtC3Z102HZlxgAESmo1WJ1Th3uy1Zjct2vrxyCUckRAIA//iTL2aHkjxZOTMXlmbFur/ToqOPiU6VWsSR3mKuUPSgKKq6EGRCCdWrcmO0/C1IZABEpaNPRCjS2tiE1KhgzhsUqfRzqglatwnt3T8P7v87BnTmDlD5Oj+zXduOg06jw/alqbHRz/1JJXTN2nLavYrmlDxafdkfaCA+w/megkbpcvz5aIevYBk8wACJS0P8dsBcD3joljb/l+rHYMD0uz4zrF6sYhsSF4sFrMgG4ttKjI2kVy4xhsRgUq9wqlthQHbRq+//W2az/GVDGp0UiKykcpjYbNuaVKnoWBkBECpK2XEuj4onkcO/MYchMCEN1kxnPuTgbyGYT8bFj9o/S3YgqlX0Z7NC4UEwbwszoQCIIAm6bkg6VABTXKdsO7z99nEQBptViRa3R/ts5t1yTnHQaFVbfPA63vrUTH+wpxsKJaZg2JKbHz/nBz1axvHr7RIii2C+ybuSeW6akYf74ZMXr6ZgBIlJIuaEVABCsVSMyWKvwaWigmZoRgzum2TM5Kz890usOJmkW1U3Z/rOKhcHPwBQRpFU8+AEYABEpptwxDTUlKoj/0JNP/Pm6UYgL0+F0VRP+9t2Zbl9X39y+ikXp6y+ivsIAiEghZY4MUEoUr7/INyJDtHjsp/a5Ra9vPY1z1V3PBvrsIFexUOBhAESkECkD1HHmCZHcbpyQgiuHxzlWehy5ZDaQKIpYLy0+VWjyM5ESGAARKaTMIAVAzACR7wiCgKcXjINeo8KPZ2qcay4kR0sbkF/eAJ1GhQUKLD4lUgoDICKFlNVLV2DMAJFvDYoNwUOzhwMAVn2Zjzpj+2wgaSIvV7FQoGEARKSQcmaAqA/dc+VQjEwMR63RjGe+ygdgH8XwmWMYHVexUKBhAESkkPJ6FkFT39GqVXjm5nEQBODj/SXYeaaGq1gooDEAIlJAY6sFjaY2ALwCo74zeXA0fu7YZ/bIZ0fw/u5CAFzFQoGJARCRAqQhiJHBWoToOJCd+s4f5mUhPlyPsxeM2Hu+DoJg3/xOFGgYABEpoJQt8KSQyGAtnrhhtPPPV2TGIS1aucWnREphAESkANb/kJLmj0vG3NH2Bby/nJGh7GGIFMLcO5EC2jvAmAGivicIAt78+SQU1zZjaHyY0schUgQzQEQKKGMGiBSmVasY/FBAYwBEpAApA8QOMCIiZTAAIlJAWT2HIBIRKYkBEFEfE0XR2QafwgCIiEgRDICI+lit0QxTmw0AkBipV/g0RESBiQEQUR+Tsj9xYXroNWqFT0NEFJjcDoAyMjLw5JNPoqioyBfnIRrwpPqfVBZAExEpxu0A6OGHH8aGDRswdOhQzJkzBx9++CFMJpMvzkY0IEkZIBZAExEpx6MAKC8vD3v27MGoUaPwwAMPIDk5GUuXLsWBAwd8cUaiAcXZAcYMEBGRYjyuAZo0aRJee+01lJWV4YknnsA//vEPTJ06FdnZ2Xj77bchiqKc5yQaMMrYAUZEpDiPAyCLxYKPPvoIN954I5YvX44pU6bgH//4B372s59h5cqV+PnPf+7ys958801kZGQgKCgIOTk52LNnT7ev3bBhA6ZMmYKoqCiEhoYiOzsb69at6/QaURTx+OOPIzk5GcHBwZg9ezZOnTrl6bdKJKtyZoCIiBTn9i6wAwcO4J133sEHH3wAlUqFxYsX4+WXX0ZWVpbzNQsXLsTUqVNdet769euxbNkyvPXWW8jJycErr7yCefPmoaCgAAkJCZe8PiYmBo888giysrKg0+nwxRdfYMmSJUhISMC8efMAAM8//zxee+01rF27FkOGDMFjjz2GefPm4fjx4wgK4g8dUpZzBhDXYBARKUYQ3byrUqvVmDNnDu6++24sWLAAWq32ktcYjUYsXboU77zzTq/Py8nJwdSpU/HGG28AAGw2G9LT0/HAAw/gz3/+s0tnmjRpEubPn4+nnnoKoigiJSUFy5cvx+9//3sAgMFgQGJiIt59913cfvvtl3y+yWTqVMjd0NCA9PR0GAwGREREuHQGIldYbSJGPPo1rDYRu1ZciyQuQyUikk1DQwMiIyNd+vnt9hXY2bNnsWnTJtx6661dBj8AEBoa6lLwYzabsX//fsyePbv9QCoVZs+ejZ07d/b6+aIoIjc3FwUFBZg5cyYA4Ny5c6ioqOj0zMjISOTk5HT7zNWrVyMyMtL5lp6e3uvXJvLEhUYTrDYRGpWA+HAOQSQiUorbAVBVVRV27959yft3796Nffv2ufWs6upqWK1WJCYmdnp/YmIiKioquv08g8GAsLAw6HQ6zJ8/H6+//jrmzJkDAM7Pc+eZK1asgMFgcL4VFxe79X0QuarUUf+TGBEEtUpQ+DRERIHL7QDod7/7XZcBQmlpKX73u9/JcqjehIeHIy8vD3v37sXTTz+NZcuWYdu2bR4/T6/XIyIiotMbkS9IW+CTefVFRKQot4ugjx8/jkmTJl3y/okTJ+L48eNuPSsuLg5qtRqVlZWd3l9ZWYmkpKRuP0+lUiEzMxMAkJ2djfz8fKxevRqzZs1yfl5lZSWSk5M7PTM7O9ut8xHJrbzeMQSRBdBERIpyOwOk1+svCVgAoLy8HBqNe/GUTqfD5MmTkZub63yfzWZDbm4upk+f7vJzbDabs4h5yJAhSEpK6vTMhoYG7N69261nEvlCmSMDlMIWeCIiRbkdAM2dO9dZMyOpr6/HypUrnXU47li2bBnWrFmDtWvXIj8/H/fddx+MRiOWLFkCAFi8eDFWrFjhfP3q1auxefNmnD17Fvn5+XjxxRexbt063HnnnQAAQRDw8MMPY9WqVdi4cSOOHDmCxYsXIyUlBQsWLHD7fNS/napsxFNfHEed0az0UQC0Z4A4BJGISFluX4G98MILmDlzJgYPHoyJEycCAPLy8pCYmHjJQEJXLFq0CBcuXMDjjz+OiooKZGdnY9OmTc4i5qKiIqhU7XGa0WjE/fffj5KSEgQHByMrKwvvvfceFi1a5HzNH//4RxiNRtx7772or6/HFVdcgU2bNnEGUAB6/dvT2HioDK0WK55eOE7p47AGiIjIT7g9BwiwByHvv/8+Dh06hODgYIwfPx533HFHt23x/Y07cwTIv930xg4cKjEgRKfGrpXXIiJI2b+jU1ZtQXWTCV88cAXGpkYqehYiooHGnZ/fbmeAAPucn3vvvdejwxH1peI6e8al2WzFhv0l+OXlQxQ7i6nNiuome60aM0BERMryKAAC7N1gRUVFMJs711bceOONXh+KSA5GUxtqO9T+rNtViF/MyIAgKDN/p9JgD370GhViQnWKnIGIiOzcDoDOnj2LhQsX4siRIxAEwbn1XfqhYrVa5T0hkYeK65oBAOF6DWyiiDMXjNh5pgYzMuMUOU97B1iwYkEYERHZud0F9tBDD2HIkCGoqqpCSEgIjh07hu3bt2PKlCleDSMkkltxrT3gyIgLxc2T0gAA/9xZqNh5WABNROQ/3A6Adu7ciSeffBJxcXFQqVRQqVS44oorsHr1ajz44IO+OCORR4pr7RmgtOhg3DV9MABgc36lMxDpa2XSEES2wBMRKc7tAMhqtSI8PByAfZJzWVkZAGDw4MEoKCiQ93REXpCuwNJjQjAiMRyXDY2B1SbiX7uLFDlPWT2HIBIR+Qu3A6CxY8fi0KFDAICcnBw8//zz+OGHH/Dkk09i6NChsh+QyFPSFVh6tD3jsnh6BgDggz3FMLfZ+vw85QZmgIiI/IXbAdCjjz4Km83+w+PJJ5/EuXPncOWVV+Krr77Ca6+9JvsBiTxV4sgApcWEAADmjE5EYoQe1U0mfH20vM/PI2WAkpkBIiJSnNtdYPPmzXP+35mZmThx4gRqa2sRHR3NzhbyG6IoOmuA0qPtAZBWrcJ/TRuMl7ecxLqdhbgpO7VPzyRlgFK5CJWISHFuZYAsFgs0Gg2OHj3a6f0xMTEMfsiv1DVbYDTbRzKkRbcHHHdMS4dGJWBfYR2OlzX02XmazW0wtFgAsAuMiMgfuBUAabVaDBo0iLN+yO9J118J4XoEadXO9ydEBGHe2CQAwLpd5/vsPFIHWLheg3CF13EQEZEHNUCPPPIIVq5cidraWl+ch0gWzgJoR/1PR4svs7fEf3awzJmV8TXW/xAR+Re3a4DeeOMNnD59GikpKRg8eDBCQ0M7ffzAgQOyHY7IU84W+OhL622mDYnByMRwFFQ24pP9Jbj7Ct/vB2sfgsj6HyIif+B2ALRgwQJfnINIVs4C6C4yQIIg4K7pg/HoZ0fx3q5CLJmRAZXKtzVs0hUYZwAREfkHtwOgJ554whfnIJKVtAVe6gC72MKJqXju6xM4V23EjtPVmDki3qfnkTJAKcwAERH5BbdrgIj6g5IOazC6EqrX4GeT+24/mHMIIlvgiYj8gtsBkEqlglqt7vaNSGk2m4iSuu6LoCV3Ooqhvz1R6ewa8xXnGgy2wBMR+QW3r8A+/fTTTn+2WCw4ePAg1q5di7/85S+yHYzIU1WNJpitNqhVQo8zdzITwnB5Zix+OF2Df+0uwh+vy/LJeURRbF+EygwQEZFfcDsAuummmy553y233IIxY8Zg/fr1uPvuu2U5GJGnpA6w5MggaNQ9JznvuiwDP5yuwfq9xXho9nDoNfJnMQ0tFrRYrM4zERGR8mSrAbrsssuQm5sr1+OIPHbxCoyezB6VgJTIINQYzfjqiG/2g0nZn9hQXaehjEREpBxZAqCWlha89tprSE3t291KRF1pH4LY+3WTRq3Cf+UMAuC7YmjnDCC2wBMR+Q23r8AuXnoqiiIaGxsREhKC9957T9bDEXmipM71DBAALJo6CK/mnsLBonocLTVgbGqkrOcpkzrA2AJPROQ33A6AXn755U4BkEqlQnx8PHJychAdHS3r4Yg84ZwC3UMHWEfx4XpcPy4Z/84rwz93nsfzt0yQ9TzsACMi8j9uB0C//OUvfXAMIvm4cwUmWTx9MP6dV4Z/55Vh5fWjEBWik+085c49YMwAERH5C7drgN555x18/PHHl7z/448/xtq1a2U5FJGnLFabs+bG1SswAJg0KBqjkiNgarPh430lsp6p/QqMGSAiIn/hdgC0evVqxMXFXfL+hIQEPPPMM7IcishT5fWtsImATqNCXJje5c8TBAGLp9sHI763uxA2myjfmRwBWSozQEREfsPtAKioqAhDhly6PXvw4MEoKiqS5VBEnpLqf9Kig91ecHpTdgrCx17W5AAAIABJREFUgzQorGnGd6cuyHIem01EBddgEBH5HbcDoISEBBw+fPiS9x86dAixsbGyHIrIU+7MALpYiE6DWyenAwDWydQSX200wWIVoRKAxHDXM1JERORbbgdAd9xxBx588EFs3boVVqsVVqsV3377LR566CHcfvvtvjgjkcvaO8A8y7bc5bgG21pQ5QymvCENQUwI730qNRER9R23/0V+6qmnkJOTg2uvvRbBwcEIDg7G3Llzcc0117AGiBTn7ADzIAMEAEPiQnHl8DiIor0WyFvtHWAsgCYi8iduB0A6nQ7r169HQUEB3n//fWzYsAFnzpzB22+/DZ1OvtZhIk+4OwOoK3dMs0+G3nys0uvzSB1gKRyCSETkV9yeAyQZPnw4hg8fLudZiLzmbQYIAC7PjINKAM5WG1FW34IUL4qXpQxQCjNARER+xe0M0M9+9jM899xzl7z/+eefx6233irLoYg80WqxorrJBMDzGiAAiAzWYnxaFADgh9PVXp2pnGswiIj8ktsB0Pbt23H99ddf8v6f/OQn2L59uyyHIvKEtAMsXK9BZLDWq2ddnmnvaPzxTI1XzykzMANEROSP3A6Ampqauqz10Wq1aGhokOVQRJ6Qrr/SYkI67avzxOWZ9mGfO05XQxQ9H4oo7QFjBoiIyL+4HQCNGzcO69evv+T9H374IUaPHi3LoYg80XEIorcmDYqGXqPChUYTTlc1efQMi9WGqkb7lRy7wIiI/IvbRdCPPfYYbr75Zpw5cwbXXHMNACA3Nxf/+te/8Mknn8h+QCJXeTME8WJBWjWmDYnB96eqseN0NYYnhrv9jMqGVogioFULiAvlEEQiIn/idgbohhtuwGeffYbTp0/j/vvvx/Lly1FaWopvv/0WmZmZvjgjkUs82QLfkxnD7NdgP5z2rA6oYwG0u2s5iIjItzwaTTt//nz88MMPMBqNOHv2LG677Tb8/ve/x4QJE+Q+H5HLnDOAZMgAAcAVjjqg3Wdr0Ga1uf357fU/vP4iIvI3Hs/m3759O37xi18gJSUFL774Iq655hrs2rVLzrPRAFZQ0ejcki4X5xWYF0MQOxqdEoHIYC0aTW04XGpw+/OlDJA3c4SIiMg33AqAKioq8Oyzz2L48OG49dZbERERAZPJhM8++wzPPvsspk6d6qtz0gCy73wtfvLqdvzXmt1edVh1ZGixoKG1DYA8RdAAoFYJmDHM3g7/wyn35wExA0RE5L9cDoBuuOEGjBw5EocPH8Yrr7yCsrIyvP766748Gw1A5jYbVn56BDYROFdtRGm9PFkgKfsTG6pDqN7jAeeXkNrhfzjjSQDkqAFiBoiIyO+4/JPi66+/xoMPPoj77ruPKzDIY2u+P4uTle1t5QeL6pEmQ82ONAQxTabrL4kUAB0orEezuQ0hOteDK+mKL4UZICIiv+NyBmjHjh1obGzE5MmTkZOTgzfeeAPV1d6tCaDAcr7aiNdyTwEABsfaA5WDRfWyPLukTtoBJm+2JSM2BKlRwTBbbdh7vs6tz2UNEBGR/3I5ALrsssuwZs0alJeX4ze/+Q0+/PBDpKSkwGazYfPmzWhsbPTlOamfE0URj352FKY2G67IjMPDs+1ZxIPF7gUV3ZG7AFoiCO11QD+6sRes1WJFrdEMgJvgiYj8kdtdYKGhofjVr36FHTt24MiRI1i+fDmeffZZJCQk4MYbb/TFGWkA+HdeGXacroZeo8KqBWMxaVA0AOBYaQNMbVavn1/syADJVQDd0RXD29diuErK/oTo1IgIlq8miYiI5OFxGzwAjBw5Es8//zxKSkrwwQcfyHUmGmDqm8146ovjAIAHrx2OjLhQDIoJQUyoDmarDfnl3mcP5ZwCfbHpjgzQ8fIGZ1anNx07wLzdS0ZERPLzKgCSqNVqLFiwABs3bnT7c998801kZGQgKCgIOTk52LNnT7evXbNmDa688kpER0cjOjoas2fPvuT1TU1NWLp0KdLS0hAcHIzRo0fjrbfecvtcJJ/VX51AjdGM4QlhuOfKoQDsV0vZ6VEAgINF3l2DiaLYXgMk8xUYACSEB2FkYjhEEdjp4nZ4KQBi/Q8RkX+SJQDy1Pr167Fs2TI88cQTOHDgACZMmIB58+ahqqqqy9dv27YNd9xxB7Zu3YqdO3ciPT0dc+fORWlpqfM1y5Ytw6ZNm/Dee+8hPz8fDz/8MJYuXepRcEbe2322Buv3FQMAnrl5HHSa9r9yE50BkHeF0NVNZrRYrBAEIMVHS0dnZDrmAbnYDt++BoMdYERE/kjRAOill17CPffcgyVLljgzNSEhIXj77be7fP3777+P+++/H9nZ2cjKysI//vEP2Gw25ObmOl/z448/4he/+AVmzZqFjIwM3HvvvZgwYUKPmSXyDVObFSs/PQIAuGPaIEzNiOn08YmOOiBvC6GlFRhJEUHQa9RePas70lqMH1ysA3K2wDMDRETklxQLgMxmM/bv34/Zs2e3H0alwuzZs7Fz506XntHc3AyLxYKYmPYfrDNmzMDGjRtRWloKURSxdetWnDx5EnPnzu32OSaTCQ0NDZ3eyHt/++4szlwwIi5Mjz9fl3XJx8enR0IQ7EtMq5tMHn8dX9b/SKYNiYFaJaCwptn59XoiDUFkBxgRkX9SLACqrq6G1WpFYmJip/cnJiaioqLCpWf86U9/QkpKSqcg6vXXX8fo0aORlpYGnU6H6667Dm+++SZmzpzZ7XNWr16NyMhI51t6erpn3xQ5nb3QhDe2ngYAPH7DaESGaC95TUSQFpnxYQCAPC+uwaT6nzSZtsB3JTxI66xZ+tGFazApA5Tsoys5IiLyjqJXYN549tln8eGHH+LTTz9FUFD7D5nXX38du3btwsaNG7F//368+OKL+N3vfoctW7Z0+6wVK1bAYDA434qLi/viWxiwRFHEI58ehbnNhpkj4nHD+ORuXztxkKMOyItrsL7IAAEd1mKc7r0Q2rkGgxkgIiK/pNiAkri4OKjValRWVnZ6f2VlJZKSknr83BdeeAHPPvsstmzZgvHjxzvf39LSgpUrV+LTTz/F/PnzAQDjx49HXl4eXnjhhU6Zoo70ej30er2X3xFJNhwoxc6zNQjSqrDqprE9toFPHBSNj/aVeFUILdUA+aIDrKPLh8XitdxT+OF0NWw2ESpV199XQ6sFTSb7YlZfFWUTEZF3FMsA6XQ6TJ48uVMBs1TQPH369G4/7/nnn8dTTz2FTZs2YcqUKZ0+ZrFYYLFYoFJ1/rbUajVsNpu83wB1qdZoxqov7TN/Hrp2BAbF9hyUSBmgQ8X1sNo82wzvqzUYF5s4KBrBWjVqjGYUVHY/u6jckf2JCtG6tTuMiIj6jqJXYMuWLcOaNWuwdu1a5Ofn47777oPRaMSSJUsAAIsXL8aKFSucr3/uuefw2GOP4e2330ZGRgYqKipQUVGBpib7cs2IiAhcddVV+MMf/oBt27bh3LlzePfdd/HPf/4TCxcuVOR7DDTPfJWPumYLspLC8esrh/T6+uEJ4QjVqWE0W3G6qqnX11/MahOdM3d8nQHSaVSYNsRecN9TN1iZVP/D6y8iIr+laAC0aNEivPDCC3j88ceRnZ2NvLw8bNq0yVkYXVRUhPLycufr//rXv8JsNuOWW25BcnKy8+2FF15wvubDDz/E1KlT8fOf/xyjR4/Gs88+i6effhq//e1v+/z7CzQ/nqnGJ/tLIAjA0wvHQavu/a+XWiVgfJrnAxErGlphsYrQqgUkRvj+usmVdvhyZwcYr7+IiPyV4vn5pUuXYunSpV1+bNu2bZ3+fP78+V6fl5SUhHfeeUeGk5E7Wi1WPPrpUQDAz3MGYfLgaJc/d+KgKOw8W4ODRfW4fdogt76uVACdEhUMdTc1OXKSBiLuPlcLi9XWZZDHDjAiov+/vTuPjqrM0wf+3KpKVSWp7IEsZAOUJUAgbDG4oUSCTiMiztCt3bL0wQGhdUQ9NjoQ0DMGGMWlmwZHj9ILAmKLS7cyNEhohCgDEoIKAfJLCEsqgQSyr1Xv74/k3qTIVkmqcm+o53NOnUNV3bp56+Z25/F9v+/7al+/nQXmKeob7Xhy6zF8cChP7aZ0alNGLv7f1SoM8DPh+dS2a/50pjcLIvbVDDDZyHB/BPsaUV1vQ9aF9gu3L13nEBgRkdYxAGnc8YJr+PKkFW/vO6t2Uzp0rrgSmzJyAQCrZ45CgHfbNX86I6+vc7a4EhW1Dd367AVlD7C+CRs6naRsjtrRMJgyBMYeICIizWIA0jhredMf02vVDd0OB32h0WbHi7tOot5mxz3DB+CBMZ0vYdCeAX4mRAd7Qwgg+2JZtz57sbkHKKqPeoCAruuAlG0w2ANERKRZqtcAUefkTTWBpi0j4iO717viSo02O85dqcTJi2U4eanpcaqwHLUNdnh76fFyF2v+dGZcdBAulNbgeME1ZcFBZ/TVGkCtyQHoeMF1VNU1wtfU8j8jIYTyO+M+YERE2sUApHHWVgGooLQa8ZH+ffJzG212nC2uxMlLZfjhhrBzIz+zAa/MGt2rEJIYHYgvTlzu9oKIF0r7Zg2g1qKDfRAd7I0LpTU4kleKe0YMVN4rrapHXaMdkoQ+mZVGREQ9wwCkcfJwCgCnNuHsjfMlVXj/mzycuNgUduoa24Ydi8mAUZH+GDMoAGOiAjBmUADiQnw7XBXZWS1bYlyHEMKpnqS6RhuKKpoCYl/2AAFNvUDbjlzAoXNXHQKQvAVGqMUEo4EjzEREWsUApHGte4Dk4R53WfvVaXz1Q8tGtO4KO+2Jj/SHUa9DaVU9LpTWdLmCNABculYDIQBvLz1CfI0ub1NnpgxtCkDf3FAHdFmp/2HvDxGRljEAaVzhDUNg7iSvxPxMyjDMHBvhtrDTHpNBj/hIf2RduI7jF645FYAutpoB1tPao56a0jwT7LS1Alcr6xBqadpLrpBT4ImI+gX20WtYg82OK5V1ynN3BiC7XeB88/lnJw7CkAGWPgs/MmUYzMk6ILlHrC9ngMlCLCaMjGiqxzqc27I7PAugiYj6BwYgDSuuqINotT/oxdIa2Hu4YWhXiipqUd9oh0EnqbZ+jbIgopNbYqhRAN3aHc2rQh862zIMdrmMawAREfUHDEAaZlU21TTDoJNQb7MrRb+uln9V7k3xhsGJPbzcIbF5QcSfCstR22Dr8ng1psC3NqV5Ovw3565CNCdVDoEREfUPDEAaZi1rGv6KCvJWhlTkXg9XO19SBQCIDfF1y/mdERXkjVCLCQ02gR8vl3d5vBqLILY2OS4YXnoJl67XKMOT8s703AeMiEjbGIA0TJ4CHx7gjZjmXg531QHJ9T+xThQfu4skScq2GM4Mg/X1Nhg38jUZkBjdNGx36FwJbHaBooqm0MpVoImItI0BSMPkKfARAWZlmMdtAUgDPUCA43pAnamqa0RpVT0A9YbAACirVh86dxXFFbWw2QUMOgkD/EyqtYmIiLrGAKRhhc37gIX7m5UeIHcthijXAMWp2AMEtASgrC5mgsn1PwHeXvA3q7c9yB23NhVCH869ikvNPVJh/mbo+3gGHRERdQ/XAdKw1j1AtuYiW3cEICGE0rOk5hAYACREBUInAZeu16C4vBYDO9hOQpkBptLwlywhKhC+Rj2uVTfg69PFADgDjIioP2APkIbJASg8wOzWGqCSqnpU1jVCktQrKJZZTAYMC/MD0PkwmBwEo1Vur5deh9uGNPUCffL9JQCcAUZE1B8wAGmUzS5QVC73ALUUQRdX1KGmvusp4t0h1/9EBnjD7KV36bl7wpkFEdWeAt+aPB3eKv++2ANERKR5DEAaVVJZh0a7gE4CQi1GBHh7wc/cNGJ50cV7gp0vaTpfjAbCBABlZlVnM8HkbTCiVFoEsbU7mgOQjDPAiIi0jwFIo+QtFQb6mWHQ6yBJkjLc4+pNUfObA1BcqEYCUHMPUPbFMjTa2u5ID2hnCAwAhoVZlL3AgKaaLSIi0jYGII0qbFX/I1PqgEpcG4AKNDIFXjZ0gAV+JgNqGmw4U1TZ5n0hhMNGqGqTJAm3N2+LAXAfMCKi/oABSKNa6n9aBaAQuRDatatByz1AsRoZAtPpJIxT1gNqOwx2vboBlXWNANQv2pbdPrRlGIwBiIhI+xiANKq9HiB3LYaolUUQW2tZEbptIbQ8BDjAz6SJom0AuHNYKIx6HQb6mRDko966RERE5ByuA6RRrTdClbljMcSymgZcq24AoP4aQK21zARr2wOk9i7w7YkI8MZHi5PhY9RDkrgIIhGR1jEAaVRLD1DLH3n5D/6Fa9UQQrjkD61cTxRqMcHXpJ3bYVzzTLDcK1Uoq25AQKteFS1NgW9N7rUiIiLt4xCYRlnbqQEaFOQNSQKq620oad4Hq7fym4e/1N4C40bBvkalTScuOg6DaWkGGBER9U8MQBokhGjpAWq1FYTJoEdE83NX1QG1bIGhnfofWWKMvB7QDQFIQzPAiIiof2IA0qBr1Q2ob2xa/ybshr2wol1cB5R/VS6A1l5vilIIfcNMsIvsASIiol5iANKgwuYC6FCLEUaD46/I1QFIXgVaiwFI2Rn+wnWI5s1g7XbRahVo7bWZiIj6BwYgDbK2MwVe5upNUc+Xam8KvGxEuD9MBh2uVzcoaxVdqaxDvc0OncQ9t4iIqOcYgDRILoAO929b4+LKAFRd34ii8joA2iuCBgCjQYcxgwIAtEyHl3u+IgK84aXn7UtERD3DvyAaJPcAtbenVMsQWO9Xg5ZDVIC3FwJ9jL0+nzvcuDN8yxR4FkATEVHPMQBpUHurQMvkHqDLZTVKoXRPabn+RyavByQXQrcsgqjdNhMRkfYxAGlQZz1AoRYjvL30EAK4fL13vUBa3ALjRnIP0KnCCtTU21rWANLYIohERNS/MABpkDwLrL0eIEmSlOGf3tYByYXFWqz/kUUEmBHmb4LNLvDD5TIOgRERkUswAGlM60UQIwLa/yPvqkJoeRuMGA33pkiShER5GKzgGofAiIjIJRiANKairhHV9TYAjqtAt+aqtYCUbTBCtTsEBrQMgx3Ju6b0jnEIjIiIeoMBSGPk+p8Aby94G/XtHiP3fsjDQT1R32hXaoi0XAQNtKwI/c+zV2AXTdPjB1hMKreKiIj6MwYgjSnspABa5oohsIvXqmEXgI9Rr/kwMSYqAHqdpMx6iwr0hk4nqdwqIiLqzxiANMbaSQG0LKa5x0au4emJ863qfyRJ22HCx2jAiHA/5XkUh7+IiKiXGIA0xlrWtDJzZz1A8hBYeW0jyqobevRz5CnwcRqeAt+aXAcEANFBnAFGRES9wwCkMdby5h6gdrbBkHkb9Rjg1zRs1dM6oPx+sAhia/JMMIAF0ERE1HsMQBrjTA0Q0NIL0tM6oP6wCGJr4xx6gBiAiIiodxiANKazneBb620h9PnS/tUDNDjEFyG+TfuVDR3YP0IbERFpl0HtBpAjZ3uAehOAbHahrCHUXwKQTifhD4+Nx/mSaowI91e7OURE1M8xAGlIdX0jymqaipq76gHqzWKIl6/XoMEmYNTrOlxtWouShoQgaUiI2s0gIqKbAIfANEQe/rKYDPAze3V6bG8CkNxrFBXsDT3X0yEiIg+kegDauHEj4uLiYDabkZSUhCNHjnR47Lvvvos777wTQUFBCAoKQkpKSrvHnzp1Cg8++CACAgLg6+uLSZMmoaCgwJ1fwyXkABTm3/XChPIQ2MVrNbDZRbd+Tn4/mwJPRETkaqoGoB07dmD58uVIS0vD999/j7FjxyI1NRXFxcXtHp+RkYFf/OIX2L9/PzIzMxEdHY3p06fj0qVLyjG5ubm44447MGLECGRkZCA7OxsrV66E2dz5kJIWdLUJamth/mYY9To02oWyP5azzvezKfBERESupmoA2rBhAxYtWoQFCxYgPj4emzdvho+PD95///12j9+6dSuefPJJjBs3DiNGjMB7770Hu92Offv2Kce89NJLeOCBB7B+/XokJiZi6NChePDBBzFw4MC++lo9Zi13bgYYAOh1EqJ6OBVemQLP9XSIiMhDqRaA6uvrcezYMaSkpLQ0RqdDSkoKMjMznTpHdXU1GhoaEBwcDACw2+34+9//jmHDhiE1NRUDBw5EUlISPv30007PU1dXh/LycoeHGqxOzgCTyXVAF0t72AOk8V3giYiI3EW1AHT16lXYbDaEhYU5vB4WFgar1erUOV544QVERkYqIaq4uBiVlZVYu3YtZsyYgT179mD27Nl4+OGHceDAgQ7Pk56ejoCAAOURHR3d8y/WC4VOrgEkiw7ufg+QEEIJQKwBIiIiT9Vvp8GvXbsW27dvR0ZGhlLfY7c37RY+a9YsPPPMMwCAcePG4fDhw9i8eTPuvvvuds+1YsUKLF++XHleXl6uSgiSt8FwtgeoJ2sBXamoQ02DDToJGBTYf6bAExERuZJqASg0NBR6vR5FRUUOrxcVFSE8PLzTz7722mtYu3Yt9u7di4SEBIdzGgwGxMfHOxw/cuRIfPPNNx2ez2QywWTqeuaVuymrQHeyD1hrPQlA8h5gg4K8YTSoPgmQiIhIFar9BTQajZgwYYJDAbNc0JycnNzh59avX49XXnkFu3fvxsSJE9ucc9KkScjJyXF4/cyZM4iNjXXtF3CxukYbrlbWA+hBDVA3NkTtb7vAExERuYOqQ2DLly/HvHnzMHHiREyePBlvvvkmqqqqsGDBAgDA448/jkGDBiE9PR0AsG7dOqxatQoffvgh4uLilFohi8UCi8UCAHj++ecxd+5c3HXXXbjnnnuwe/dufPHFF8jIyFDlOzqruLwOAGAy6BDo0/kiiDI5AF2trEdVXSN8TV3/OuX6nxjOACMiIg+magCaO3curly5glWrVsFqtWLcuHHYvXu3UhhdUFAAna6lk2rTpk2or6/HI4884nCetLQ0rF69GgAwe/ZsbN68Genp6XjqqacwfPhw/PWvf8Udd9zRZ9+rJ1oXQEuSc6sz+5u9EOjjhevVDbhwzbk9srgIIhERkQaKoJctW4Zly5a1+96NvTb5+flOnXPhwoVYuHBhL1vWt+TFDMP9u7dgY0ywD65Xl6HAyU1CC/rZJqhERETuwCpYjejuGkCy6G4UQgshkHe1eRFE9gAREZEHYwDSiJYhsO5NTW+9J1hXrlc3oKK20eFzREREnogBSCOKynvYAxTkfA/Q+eZjwvxN8Dbqu9lCIiKimwcDkEZ0dxVoWXfWAlL2AOPwFxEReTgGII3oaQ2QHIAulFZDCNHpsflX5S0wOPxFRESejQFIAxptdhRX9KwHKCLQDL1OQl2jHVcq6jo99nwpe4CIiIgABiBNuFJZB7sADDoJob7d25LDS69Teo26GgZTdoFnDxAREXk4BiANkOt/wvzN0OmcWwSxNWfrgLgNBhERURMGIA2w9rAAWuZMAKqsa1T2GothDxAREXk4BiAN6OkMMJkziyHKvT/Bvkb4m53ba4yIiOhmxQCkAdbmbTAiurkNhkzZFb6048UQWf9DRETUggFIA6zNO8G7cwhMCUBcAZqIiIgBSAuUHqBuboMhkwOQtbwWtQ22do/hIohEREQtGIA0oLc1QEE+XrCYDAA63hNM7gGKC2UPEBEREQOQyux20eN9wGSSJCl1QBeutT8MJvcAxQSzB4iIiIgBSGUlVfVosAnoJGCAX/cWQWwtOqhp+OxCO3VAtQ02FDaHLG6DQURExACkOnkNoAF+Jnjpe/7rUAqhS9oGoIvXqiEE4GcyINjX2OOfQUREdLNgAFJZYXMBdHgPC6Bl8uKG7c0EkzdBjQnxgSR1f6VpIiKimw0DkMqszUNT4f49H/4C0KoGqG0RdD63wCAiInLAAKQyeQZYT6fAy6KDmgNQaTWEEA7vyb1C3AKDiIioCQOQynq7D5gsqrkIurKuEdeqGxzey5enwDMAERERAWAAUp21rHdT4GVmLz3Cm7fSuLEOiIsgEhEROWIAUllLDVDvAhDQ/pYYDTY7LjXXBXEfMCIioiYMQCoSQiizwHpbAwS0KoRuFYAuX69Bo13AZNAhzK/3IYuIiOhmwACkorKaBtQ22AEAA3s5CwwAooPbLoaY32oXeJ2OU+CJiIgABiBVyTPAQnyNMHvpe32+9obACrgFBhERURsMQCpy1QwwWXsBiDPAiIiI2mIAUpGyC7wLCqCBlgBUWFaLBlvT0JoyAyyUPUBEREQyBiAVWZVtMFwTgAb4mWAy6GCzCxRebwpX5+UaoGD2ABEREckYgFRU6KI1gGSSJCkzwQpKq2G3C5wvlYfA2ANEREQkYwBSkbIGkAumwMta1wEVVdSivtEOg05CZCCnwBMREckYgFTkqlWgW2sdgORd4KOCvGHQ81dNREQk419FFbl6FhjQelf4am6BQURE1AEGIJVU1Dagoq4RgOtmgQFAdFDLYogt9T8sgCYiImrNoHYDPFVRc/2Pv9kAX5Prfg0xIS1DYFHyIojsASIiInLAHiCVtMwAc10BNABEBzUFoOvVDTh5qQwAe4CIiIhuxACkkkI31P8AgK/JgFCLEQBwoZS7wBMREbWHAUglVhevAt1aVFBL4JEkx+dERETEAKQad/UAAS1T4QEgMsDbJRutEhER3UwYgFQib4PhyjWAZK0DUAy3wCAiImqDAUglfdUDFBfKAERERHQjBiCVyNPgXT0LDGhZDBHgIohERETtYQBSQW2DDdeqGwC4pwcoOrglVHEXeCIiorYYgFQgzwDzMerhb3b9WpQRAd7w0ksA2ANERETUHq4ErYLW9T+SJLn8/HqdhGenD0felSqMCPdz+fmJiIj6OwYgFVjL3TcDTLb47qFuOzcREVF/xyEwFSg9QP6uL4AmIiKirmkiAG3cuBFxcXEwm81ISkrCkSNHOjz23XffxZ133omgoCAEBQUhJSWl0+MXL14MSZLw5ptvuqPpPaKsAh1gUrklREREnkkX/msJAAAWqklEQVT1ALRjxw4sX74caWlp+P777zF27FikpqaiuLi43eMzMjLwi1/8Avv370dmZiaio6Mxffp0XLp0qc2xu3btwrfffovIyEh3f41uaakBYg8QERGRGlQPQBs2bMCiRYuwYMECxMfHY/PmzfDx8cH777/f7vFbt27Fk08+iXHjxmHEiBF47733YLfbsW/fPofjLl26hN/85jfYunUrvLy8+uKrOE3uAYpwwz5gRERE1DVVA1B9fT2OHTuGlJQU5TWdToeUlBRkZmY6dY7q6mo0NDQgODhYec1ut+NXv/oVnn/+eYwaNarLc9TV1aG8vNzh4U7WcvetAk1ERERdUzUAXb16FTabDWFhYQ6vh4WFwWq1OnWOF154AZGRkQ4hat26dTAYDHjqqaecOkd6ejoCAgKUR3R0tPNfopvqG+24WlkHwL2zwIiIiKhjqg+B9cbatWuxfft27Nq1C2ZzU5g4duwY3nrrLWzZssXpNXZWrFiBsrIy5XHhwgW3tbm4ohZCAEa9DsG+Rrf9HCIiIuqYqgEoNDQUer0eRUVFDq8XFRUhPDy808++9tprWLt2Lfbs2YOEhATl9YMHD6K4uBgxMTEwGAwwGAw4f/48nn32WcTFxbV7LpPJBH9/f4eHu1jdvAgiERERdU3VAGQ0GjFhwgSHAma5oDk5ObnDz61fvx6vvPIKdu/ejYkTJzq896tf/QrZ2dnIyspSHpGRkXj++efxv//7v277Ls5y5y7wRERE5BzVV4Jevnw55s2bh4kTJ2Ly5Ml48803UVVVhQULFgAAHn/8cQwaNAjp6ekAmup7Vq1ahQ8//BBxcXFKrZDFYoHFYkFISAhCQkIcfoaXlxfCw8MxfPjwvv1y7VBmgDEAERERqUb1ADR37lxcuXIFq1atgtVqxbhx47B7926lMLqgoAA6XUtH1aZNm1BfX49HHnnE4TxpaWlYvXp1Xza9R1pWgWYAIiIiUovqAQgAli1bhmXLlrX7XkZGhsPz/Pz8bp+/J59xF3kfMA6BERERqadfzwLrjwo5BEZERKQ6BqA+ZuU2GERERKpjAOpDNrtAcQUXQSQiIlIbA1AfulpZB5tdQK+TEGrhTvBERERqYQDqQ3L9T5ifCXodF0EkIiJSCwNQH7KWcQYYERGRFjAA9aGWGWAsgCYiIlITA1Afqm2ww+ylYw8QERGRyiQhhFC7EVpTXl6OgIAAlJWVuXxjVCEEGmwCRgOzJxERkSt15++3JlaC9iSSJMFoYAE0ERGRmtgNQURERB6HAYiIiIg8DgMQEREReRwGICIiIvI4DEBERETkcRiAiIiIyOMwABEREZHHYQAiIiIij8MARERERB6HAYiIiIg8DgMQEREReRwGICIiIvI4DEBERETkcbgbfDuEEACA8vJylVtCREREzpL/bst/xzvDANSOiooKAEB0dLTKLSEiIqLuqqioQEBAQKfHSMKZmORh7HY7Ll++DD8/P0iS5NJzl5eXIzo6GhcuXIC/v79Lz92f8Do04XVowWvRhNehCa9DE16HFs5cCyEEKioqEBkZCZ2u8yof9gC1Q6fTISoqyq0/w9/f3+NvZoDXQcbr0ILXogmvQxNehya8Di26uhZd9fzIWARNREREHocBiIiIiDyOfvXq1avVboSn0ev1mDp1KgwGzx6B5HVowuvQgteiCa9DE16HJrwOLVx5LVgETURERB6HQ2BERETkcRiAiIiIyOMwABEREZHHYQAiIiIij8MA1Ic2btyIuLg4mM1mJCUl4ciRI2o3qc+tXr0akiQ5PEaMGKF2s9zun//8J2bOnInIyEhIkoRPP/3U4X0hBFatWoWIiAh4e3sjJSUFZ8+eVam17tPVdZg/f36b+2PGjBkqtdZ90tPTMWnSJPj5+WHgwIF46KGHkJOT43BMbW0tli5dipCQEFgsFsyZMwdFRUUqtdg9nLkOU6dObXNPLF68WKUWu8+mTZuQkJCgLPKXnJyMr776SnnfE+4HoOvr4Mr7gQGoj+zYsQPLly9HWloavv/+e4wdOxapqakoLi5Wu2l9btSoUSgsLFQe33zzjdpNcruqqiqMHTsWGzdubPf99evX4+2338bmzZvx3XffwdfXF6mpqaitre3jlrpXV9cBAGbMmOFwf2zbtq0PW9g3Dhw4gKVLl+Lbb7/FP/7xDzQ0NGD69OmoqqpSjnnmmWfwxRdfYOfOnThw4AAuX76Mhx9+WMVWu54z1wEAFi1a5HBPrF+/XqUWu09UVBTWrl2LY8eO4ejRo7j33nsxa9Ys/PjjjwA8434Aur4OgAvvB0F9YvLkyWLp0qXKc5vNJiIjI0V6erqKrep7aWlpYuzYsWo3Q1UAxK5du5TndrtdhIeHi//+7/9WXrt+/bowmUxi27ZtajSxT9x4HYQQYt68eWLWrFkqtUg9xcXFAoA4cOCAEKLp9+/l5SV27typHHPq1CkBQGRmZqrVTLe78ToIIcTdd98tnn76aRVbpZ6goCDx3nvveez9IJOvgxCuvR/YA9QH6uvrcezYMaSkpCiv6XQ6pKSkIDMzU8WWqePs2bOIjIzEkCFD8Nhjj6GgoEDtJqkqLy8PVqvV4f4ICAhAUlKSR94fGRkZGDhwIIYPH44lS5agpKRE7Sa5XVlZGQAgODgYAHDs2DE0NDQ43BMjRoxATEzMTX1P3HgdZFu3bkVoaChGjx6NFStWoLq6Wo3m9RmbzYbt27ejqqoKycnJHns/3HgdZK66H7isZB+4evUqbDYbwsLCHF4PCwvD6dOnVWqVOpKSkrBlyxYMHz4chYWFWLNmDe6880788MMP8PPzU7t5qrBarQDQ7v0hv+cpZsyYgYcffhiDBw9Gbm4uXnzxRdx///3IzMyEXq9Xu3luYbfb8R//8R+4/fbbMXr0aABN94TRaERgYKDDsTfzPdHedQCARx99FLGxsYiMjER2djZeeOEF5OTk4JNPPlGxte5x8uRJJCcno7a2FhaLBbt27UJ8fDyysrI86n7o6DoArr0fGICoT91///3KvxMSEpCUlITY2Fh89NFH+PWvf61iy0gLfv7znyv/HjNmDBISEjB06FBkZGRg2rRpKrbMfZYuXYoffvjBI2rhOtPRdXjiiSeUf48ZMwYRERGYNm0acnNzMXTo0L5uplsNHz4cWVlZKCsrw8cff4x58+bhwIEDajerz3V0HeLj4116P3AIrA+EhoZCr9e3qdgvKipCeHi4Sq3ShsDAQAwbNgznzp1Tuymqke8B3h9tDRkyBKGhoTft/bFs2TL87W9/w/79+xEVFaW8Hh4ejvr6ely/ft3h+Jv1nujoOrQnKSkJAG7Ke8JoNOKWW27BhAkTkJ6ejrFjx+Ktt97yuPuho+vQnt7cDwxAfcBoNGLChAnYt2+f8prdbse+ffscxjU9UWVlJXJzcxEREaF2U1QzePBghIeHO9wf5eXl+O677zz+/rh48SJKSkpuuvtDCIFly5Zh165d+PrrrzF48GCH9ydMmAAvLy+HeyInJwcFBQU31T3R1XVoT1ZWFgDcdPdEe+x2O+rq6jzmfuiIfB3a06v7wSWl1NSl7du3C5PJJLZs2SJ++ukn8cQTT4jAwEBhtVrVblqfevbZZ0VGRobIy8sThw4dEikpKSI0NFQUFxer3TS3qqioEMePHxfHjx8XAMSGDRvE8ePHxfnz54UQQqxdu1YEBgaKzz77TGRnZ4tZs2aJwYMHi5qaGpVb7lqdXYeKigrx3HPPiczMTJGXlyf27t0rxo8fL2699VZRW1urdtNdasmSJSIgIEBkZGSIwsJC5VFdXa0cs3jxYhETEyO+/vprcfToUZGcnCySk5NVbLXrdXUdzp07J15++WVx9OhRkZeXJz777DMxZMgQcdddd6ncctf77W9/Kw4cOCDy8vJEdna2+O1vfyskSRJ79uwRQnjG/SBE59fB1fcDA1Af+t3vfidiYmKE0WgUkydPFt9++63aTepzc+fOFREREcJoNIpBgwaJuXPninPnzqndLLfbv3+/ANDmMW/ePCFE01T4lStXirCwMGEymcS0adNETk6Ouo12g86uQ3V1tZg+fboYMGCA8PLyErGxsWLRokU35X8ktHcNAIgPPvhAOaampkY8+eSTIigoSPj4+IjZs2eLwsJC9RrtBl1dh4KCAnHXXXeJ4OBgYTKZxC233CKef/55UVZWpm7D3WDhwoUiNjZWGI1GMWDAADFt2jQl/AjhGfeDEJ1fB1ffD5IQQnS/34iIiIio/2INEBEREXkcBiAiIiLyOAxARERE5HEYgIiIiMjjMAARERGRx2EAIiIiIo/DAEREREQehwGIiIiIPA4DEBFpUn19PW655RYcPnwYAJCfnw9JkpS9f1xp/vz5eOihh1x+Xi248br99NNPiIqKQlVVlcotI1IXAxCRh7hy5QqWLFmCmJgYmEwmhIeHIzU1FYcOHVKOkSQJn376qYqtbLF582YMHjwYU6ZMAQBER0ejsLAQo0eP7vE53Rmi+ov4+Hjcdttt2LBhg9pNIVIVAxCRh5gzZw6OHz+OP/7xjzhz5gw+//xzTJ06FSUlJWo3rQ0hBH7/+9/j17/+tfKaXq9HeHg4DAaDii27OSxYsACbNm1CY2Oj2k0hUg0DEJEHuH79Og4ePIh169bhnnvuQWxsLCZPnowVK1bgwQcfBADExcUBAGbPng1JkpTnAPDZZ59h/PjxMJvNGDJkCNasWePwx1OSJGzatAn3338/vL29MWTIEHz88cfK+/X19Vi2bBkiIiJgNpsRGxuL9PT0Dtt77Ngx5Obm4l/+5V+U127svcnIyIAkSdi3bx8mTpwIHx8fTJkyBTk5OR2ed/DgwQCAxMRESJKEqVOnOrz/2muvISIiAiEhIVi6dCkaGhqU965du4bHH38cQUFB8PHxwf3334+zZ88q769evRrjxo1zON+bb77pcB0zMjIwefJk+Pr6IjAwELfffjvOnz8PAMjNzcWsWbMQFhYGi8WCSZMmYe/evQ7ni4uLw6uvvoqFCxfCz88PMTEx+J//+R+HY44cOYLExESYzWZMnDgRx48fb3Md7rvvPpSWluLAgQMdXiuimx0DEJEHsFgssFgs+PTTT1FXV9fuMf/3f/8HAPjggw9QWFioPD948CAef/xxPP300/jpp5/wzjvvYMuWLfiv//ovh8+vXLkSc+bMwYkTJ/DYY4/h5z//OU6dOgUAePvtt/H555/jo48+Qk5ODrZu3eoQDG508OBBDBs2DH5+fl1+t5deegmvv/46jh49CoPBgIULF3Z47JEjRwAAe/fuRWFhIT755BPlvf379yM3Nxf79+/HH//4R2zZsgVbtmxR3p8/fz6OHj2Kzz//HJmZmRBC4IEHHnAISZ1pbGzEQw89hLvvvhvZ2dnIzMzEE088AUmSAACVlZV44IEHsG/fPhw/fhwzZszAzJkzUVBQ4HCe119/XQk2Tz75JJYsWaKEvsrKSvzsZz9DfHw8jh07htWrV+O5555r0xaj0Yhx48bh4MGDTrWd6Kbkqi3siUjbPv74YxEUFCTMZrOYMmWKWLFihThx4oTDMQDErl27HF6bNm2aePXVVx1e+/Of/ywiIiIcPrd48WKHY5KSksSSJUuEEEL85je/Effee6+w2+1OtfXpp58W9957r8NreXl5AoA4fvy4EEKI/fv3CwBi7969yjF///vfBQBRU1PT7nlvPIds3rx5IjY2VjQ2Niqv/eu//quYO3euEEKIM2fOCADi0KFDyvtXr14V3t7e4qOPPhJCCJGWlibGjh3rcN433nhDxMbGCiGEKCkpEQBERkaGU9dACCFGjRolfve73ynPY2NjxS9/+Uvlud1uFwMHDhSbNm0SQgjxzjvviJCQEIfvv2nTpna/8+zZs8X8+fOdbgvRzYY9QEQeYs6cObh8+TI+//xzzJgxAxkZGRg/frxDL0d7Tpw4gZdfflnpRbJYLFi0aBEKCwtRXV2tHJecnOzwueTkZKUHaP78+cjKysLw4cPx1FNPYc+ePZ3+zJqaGpjNZqe+V0JCgvLviIgIAEBxcbFTn21t1KhR0Ov1DueSz3Pq1CkYDAYkJSUp74eEhGD48OHKd+xKcHAw5s+fj9TUVMycORNvvfUWCgsLlfcrKyvx3HPPYeTIkQgMDITFYsGpU6fa9AC1/r6SJCE8PNyhnQkJCQ7X7sbfi8zb29vh90fkaRiAiDyI2WzGfffdh5UrV+Lw4cOYP38+0tLSOv1MZWUl1qxZg6ysLOVx8uRJnD171umQMn78eOTl5eGVV15BTU0N/u3f/g2PPPJIh8eHhobi2rVrTp3by8tL+bc8nGS32536bEfnkc/VnfPodDoIIRxeu3F47IMPPkBmZiamTJmCHTt2YNiwYfj2228BAM899xx27dqFV199FQcPHkRWVhbGjBmD+vp6l7ZTVlpaigEDBnT7c0Q3CwYgIg8WHx/vsB6Ml5cXbDabwzHjx49HTk4ObrnlljYPna7l/0LkP+Stn48cOVJ57u/vj7lz5+Ldd9/Fjh078Ne//hWlpaXttisxMRGnT59uEyh6y2g0AkCb79iVkSNHorGxEd99953yWklJCXJychAfHw8AGDBgAKxWq0Ob25tun5iYiBUrVuDw4cMYPXo0PvzwQwDAoUOHMH/+fMyePRtjxoxBeHg48vPzu93O7Oxs1NbWKq/d+HuR/fDDD0hMTOzW+YluJgxARB6gpKQE9957L/7yl78gOzsbeXl52LlzJ9avX49Zs2Ypx8XFxWHfvn2wWq1KD8yqVavwpz/9CWvWrMGPP/6IU6dOYfv27fjP//xPh5+xc+dOvP/++zhz5gzS0tJw5MgRLFu2DACwYcMGbNu2DadPn8aZM2ewc+dOhIeHIzAwsN323nPPPaisrMSPP/7o0uswcOBAeHt7Y/fu3SgqKkJZWZlTn7v11lsxa9YsLFq0CN988w1OnDiBX/7ylxg0aJBy/aZOnYorV65g/fr1yM3NxcaNG/HVV18p58jLy8OKFSuQmZmJ8+fPY8+ePTh79qwSEm+99VZ88sknyMrKwokTJ/Doo492u2fn0UcfhSRJWLRoEX766Sd8+eWXeO2119ocl5+fj0uXLiElJaVb5ye6mTAAEXkAi8WCpKQkvPHGG7jrrrswevRorFy5EosWLcLvf/975bjXX38d//jHPxAdHa30DqSmpuJvf/sb9uzZg0mTJuG2227DG2+8gdjYWIefsWbNGmzfvh0JCQn405/+hG3btim9I35+fli/fj0mTpyISZMmIT8/H19++aVDD1JrISEhmD17NrZu3erS62AwGPD222/jnXfeQWRkpEP468oHH3yACRMm4Gc/+xmSk5MhhMCXX36pDEmNHDkSf/jDH7Bx40aMHTsWR44ccZiB5ePjg9OnT2POnDkYNmwYnnjiCSxduhT//u//DqApJAYFBWHKlCmYOXMmUlNTMX78+G59P4vFgi+++AInT55EYmIiXnrpJaxbt67Ncdu2bcP06dPb/A6JPIkkXN3HTEQeR5Ik7Nq1y6XbSWRnZ+O+++5Dbm4uLBaLy87r6err63Hrrbfiww8/xO233652c4hUwx4gItKkhIQErFu3Dnl5eWo35aZSUFCAF198keGHPB57gIio19zRA0RE5E7cVIeIeo3/HUVE/Q2HwIiIiMjjMAARERGRx2EAIiIiIo/DAEREREQehwGIiIiIPA4DEBEREXkcBiAiIiLyOAxARERE5HH+PzLoyn1BkE7nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpFt_Fo2TdN0"
      },
      "source": [
        "# Deep CBOW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZanOMesTfEZ"
      },
      "source": [
        "To see if we can squeeze some more performance out of the CBOW model, we can make it deeper and non-linear by adding more layers and, e.g., tanh-activations.\n",
        "By using more parameters we can learn more aspects of the data, and by using more layers and non-linearities, we can try to learn a more complex function. \n",
        "This is not something that always works. If the input-output mapping of your data is simple, then a complicated function could easily overfit on your training set, thereby leading to poor generalization. \n",
        "\n",
        "#### Exercise: write Deep CBOW class and train it\n",
        "\n",
        "Write a class `DeepCBOW`.\n",
        "\n",
        "In your code, make sure that your `output_layer` consists of the following:\n",
        "- A linear transformation from E units to D units.\n",
        "- A Tanh activation\n",
        "- A linear transformation from D units to D units\n",
        "- A Tanh activation\n",
        "- A linear transformation from D units to 5 units (our output classes).\n",
        "\n",
        "E is the size of the word embeddings (please use E=300) and D for the size of a hidden layer (please use D=100).\n",
        "\n",
        "We recommend using [nn.Sequential](https://pytorch.org/docs/stable/nn.html?highlight=sequential#torch.nn.Sequential) to implement this exercise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8Z1igvpTrZq"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "class DeepCBOW(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim, output_dim, num_hidden, vocab):\n",
        "        super(DeepCBOW, self).__init__()\n",
        "        \n",
        "        self.vocab = vocab\n",
        "\n",
        "        # this is a trainable look-up table with word embeddings\n",
        "        self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # sequential model\n",
        "        self.net = nn.Sequential(self.embed, nn.Linear(embedding_dim, num_hidden), nn.Tanh(),\\\n",
        "                                 nn.Linear(num_hidden,num_hidden), nn.Tanh(), nn.Linear(num_hidden,output_dim))\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        \n",
        "        # pass through net\n",
        "        out = self.net(inputs)\n",
        "        \n",
        "        # the output is the sum across the time dimension (1)\n",
        "        # with the bias term added\n",
        "        logits = out.sum(1)\n",
        "\n",
        "        return logits\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9okYsC4FqnU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "b7ce02d5-c460-49ea-fe1a-f676012a24fd"
      },
      "source": [
        "NUM_HIDDEN = 100\n",
        "# train DeepCBOW\n",
        "\n",
        "deep_cbow_losses, deep_cbow_accuracies, deep_cbow_best_accs = train_loop(\n",
        "    lambda: DeepCBOW(len(v.w2i), EMBED_DIM, output_dim=NUM_CLASSES, num_hidden=NUM_HIDDEN, vocab=v).to(device),\n",
        "    lambda model: optim.Adam(model.parameters(), lr=0.0005),\n",
        "    num_iterations=0, print_every=1000, eval_every=1000, patience=20)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DeepCBOW(\n",
            "  (embed): Embedding(18280, 300)\n",
            "  (net): Sequential(\n",
            "    (0): Embedding(18280, 300)\n",
            "    (1): Linear(in_features=300, out_features=100, bias=True)\n",
            "    (2): Tanh()\n",
            "    (3): Linear(in_features=100, out_features=100, bias=True)\n",
            "    (4): Tanh()\n",
            "    (5): Linear(in_features=100, out_features=5, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-49e87d90239f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep_cbow_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0005\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m deep_cbow_losses, deep_cbow_accuracies = train_model(\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mdeep_cbow_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     print_every=1000, eval_every=1000)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MY2E4UCXD-6G"
      },
      "source": [
        "plot_results(deep_cbow_losses, deep_cbow_accuracies, \"fig/deep_cbow.pdf\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQZ5flHwiiHY"
      },
      "source": [
        "# Pre-trained word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NX35vecmHy6"
      },
      "source": [
        "The Stanford Sentiment Treebank is a rather small data set, since it required fine-grained manual annotatation. This makes it difficult for the Deep CBOW model to learn good word embeddings, i.e. to learn good word representations for the words in our vocabulary.\n",
        "In fact, the only error signal that the network receives is from predicting the sentiment of entire sentences!\n",
        "\n",
        "To start off with better word representations, we can download **pre-trained word embeddings**. \n",
        "You can choose which pre-trained word embeddings to use:\n",
        "\n",
        "- **GloVe**. The \"original\" Stanford Sentiment classification [paper](http://aclweb.org/anthology/P/P15/P15-1150.pdf) used Glove embeddings, which are just another method (like *word2vec*) to get word embeddings from unannotated text. Glove is described in the following paper which you should cite if you use them:\n",
        "> Jeffrey Pennington, Richard Socher, and Christopher Manning. [\"Glove: Global vectors for word representation.\"](https://nlp.stanford.edu/pubs/glove.pdf) EMNLP 2014. \n",
        "\n",
        "- **Word2Vec**. This is the method that you learned about in class, described in:\n",
        "> Mikolov, Tomas, et al. [\"Distributed representations of words and phrases and their compositionality.\"](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) Advances in neural information processing systems. 2013.\n",
        "\n",
        "Using these pre-trained word embeddings, we can initialize our word embedding lookup table and start form a point where similar words are already close to one another in the distributional semantic space. \n",
        "\n",
        "You can choose to keep the word embeddings **fixed** or to train them further, specialising them to the task at hand.\n",
        "We will keep them fixed for now.\n",
        "\n",
        "For the purposes of this lab, it is enough if you understand how word2vec works (whichever vectors you use), but if you are interested, we encourage you to also check out the GloVe paper.\n",
        "\n",
        "You can either download the word2vec vectors, or the Glove vectors.\n",
        "If you want to compare your results to the Stanford paper later on, then you should use Glove. \n",
        "**At the end of this lab you have the option to compare which vectors give you the best performance. For now, simply choose one of them and continue with that.**\n",
        "\n",
        "[**OPTIONAL in case you don't want to mount Google Drive:** instead of running all the 5 boxes below, you can 1) download the GloVe and word2vec in your local machine, 2) upload them on your Drive folder (\"My Drive\"). Then, uncomment the first 2 lines in box 6 before writing your code!]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGYr02WWO993"
      },
      "source": [
        "# This downloads the Glove 840B 300d embeddings.\n",
        "# The original file is at http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
        "# Since that file is 2GB, we provide you with a *filtered version*\n",
        "# which contains all the words you need for this data set.\n",
        "\n",
        "# You only need to do this once.\n",
        "# Please comment this cell out after downloading.\n",
        "\n",
        "!wget https://gist.githubusercontent.com/bastings/b094de2813da58056a05e8e7950d4ad1/raw/3fbd3976199c2b88de2ae62afc0ecc6f15e6f7ce/glove.840B.300d.sst.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NLsgFGiTjmI"
      },
      "source": [
        "# This downloads the word2vec 300D Google News vectors \n",
        "# The file has been truncated to only contain words that appear in our data set.\n",
        "# You can find the original file here: https://code.google.com/archive/p/word2vec/\n",
        "\n",
        "# You only need to do this once.\n",
        "# Please comment this out after downloading.\n",
        "#!wget https://gist.githubusercontent.com/bastings/4d1c346c68969b95f2c34cfbc00ba0a0/raw/76b4fefc9ef635a79d0d8002522543bc53ca2683/googlenews.word2vec.300d.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "GXBITzPRQUQb"
      },
      "source": [
        "# Mount Google Drive (to save the downloaded files)\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFvzPuiKSCbl"
      },
      "source": [
        "# Copy word vectors *to* Google Drive\n",
        "\n",
        "# You only need to do this once.\n",
        "# Please comment this out after running it. \n",
        "!cp \"glove.840B.300d.sst.txt\" \"/gdrive/My Drive/\"\n",
        "!cp \"googlenews.word2vec.300d.txt\" \"/gdrive/My Drive/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUMH0bM6BuY9"
      },
      "source": [
        "# If you copied the word vectors to your Drive before,\n",
        "# here is where you copy them back to the Colab notebook.\n",
        "\n",
        "# Copy Glove vectors *from* Google Drive\n",
        "!cp \"/gdrive/My Drive/glove.840B.300d.sst.txt\" .\n",
        "!cp \"/gdrive/My Drive/googlenews.word2vec.300d.txt\" ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcpkoh6PIjfe"
      },
      "source": [
        "# Uncomment these 2 lines below if went for the OPTIONAL method described above\n",
        "# !cp \"glove.840B.300d.sst.txt\" \"./\"\n",
        "# !cp \"googlenews.word2vec.300d.txt\" \"./\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MX2GJVHILM8n"
      },
      "source": [
        "At this point you have the pre-trained word embedding files, but what do they look like?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFaNNSp2FqnW"
      },
      "source": [
        "W2V_PATH = './googlenews.word2vec.300d.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChsChH14Ruxn"
      },
      "source": [
        "# Exercise: Print the first 4 lines of the files that you downloaded.\n",
        "# What do you see?\n",
        "f = open(W2V_PATH)\n",
        "for i in range(4):\n",
        "    print(f.readline())\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIVCkUkE_IjR"
      },
      "source": [
        "#### Exercise: New Vocabulary\n",
        "\n",
        "Since we now use pre-trained word embeddings, we need to create a new vocabulary. \n",
        "This is because of two reasons:\n",
        "\n",
        "1. We do not have pre-trained word embeddings for all words in our SST training set, and we do not want words in our vocabulary for which we have no word embeddings.\n",
        "2. We should be able to look up the pre-trained word embedding for words in the validation and test set, even if these words are unseen in training. \n",
        "\n",
        "Now, create a new vocabulary object `v` based on the word set of pre-trained embeddings, and load the corresponding embeddings into a list `vectors`.\n",
        "\n",
        "The vocabulary `v` should consist of:\n",
        " - a  `<unk>` token at position 0,\n",
        " - a  `<pad>` token at position 1, \n",
        " - and then all words in the pre-trained embedding set.\n",
        " \n",
        "\n",
        "After storing each vector in a list `vectors`, turn the list into a numpy matrix like this:\n",
        "```python\n",
        " vectors = np.stack(vectors, axis=0)\n",
        "```\n",
        "\n",
        "Remember to add new embeddings for the `<unk>` and `<pad>` tokens, as they're not part of the word2vec/GloVe embeddings. These embeddings can be randomly initialized or 0-valued, think about what makes sense and see what the effects are.\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITyyCvDnCL4U"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "# v = ...\n",
        "# vectors = ...\n",
        "\n",
        "vectors = [[0]*300, [0]*300] # first two vectors are embedding of <unk> and <pad>\n",
        "\n",
        "f = open(W2V_PATH)\n",
        "v = Vocabulary()\n",
        "for line in f:\n",
        "    line_list = line.split()\n",
        "    word = line_list[0]\n",
        "    vec = line_list[1:]\n",
        "    vec = [float(val) for val in vec]\n",
        "    vectors.append(vec)\n",
        "    v.count_token(word)\n",
        "    \n",
        "vectors = np.stack(vectors, axis=0)\n",
        "v.build()\n",
        "print(\"Vocabulary size:\", len(v.w2i))\n",
        "print('Vectors shape:',vectors.shape)\n",
        "print('Third vector:', vectors[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xC-7mRyYNG9b"
      },
      "source": [
        "#### Exercise: words not in our pre-trained set\n",
        "\n",
        "How many words in the training, dev, and test set are also in your vector set?\n",
        "How many words are not there?\n",
        "\n",
        "Store the words that are not in the word vector set in the set below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6MA3-wF_X5M"
      },
      "source": [
        "words_not_found = set()\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# get all words which are in the data we have\n",
        "known_words = set()\n",
        "all_sets = (train_data, dev_data, test_data)\n",
        "for data in all_sets:\n",
        "    for data_set in (data,):\n",
        "        for ex in data_set:\n",
        "            for token in ex.tokens:\n",
        "                known_words.add(token)\n",
        "\n",
        "# check for each word in v if it is known\n",
        "words_not_found = {word for word in v.w2i.keys() if word not in known_words}\n",
        "\n",
        "print(words_not_found)\n",
        "print(len(known_words))\n",
        "print(len(v.w2i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfEd38W0NnAI"
      },
      "source": [
        "#### Exercise: train Deep CBOW with (fixed) pre-trained embeddings\n",
        "\n",
        "Now train Deep CBOW again using the pre-trained word vectors.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_6ooqgEsB20"
      },
      "source": [
        "# We define a new class that inherits from DeepCBOW.\n",
        "class PTDeepCBOW(DeepCBOW):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, vocab):\n",
        "        super(PTDeepCBOW, self).__init__(\n",
        "            vocab_size, embedding_dim, output_dim, hidden_dim, vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfIh4Ni6yuAh"
      },
      "source": [
        "# Create a Deep CBOW model with pre-trained embeddings\n",
        "# YOUR CODE HERE\n",
        "# pt_deep_cbow_model = ..\n",
        "pt_deep_cbow_model = PTDeepCBOW(len(v.i2w), EMBED_DIM, NUM_CLASSES, NUM_HIDDEN, v)\n",
        "\n",
        "# copy pre-trained word vectors into embeddings table\n",
        "pt_deep_cbow_model.embed.weight.data.copy_(torch.from_numpy(vectors))\n",
        "\n",
        "# disable training the pre-trained embeddings\n",
        "pt_deep_cbow_model.embed.weight.requires_grad = False\n",
        "\n",
        "# move model to specified device\n",
        "pt_deep_cbow_model = pt_deep_cbow_model.to(device)\n",
        "\n",
        "# train the model\n",
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cs9VivO7FqnX"
      },
      "source": [
        "# train PTDeepCBOW\n",
        "PTdeep_cbow_model = PTDeepCBOW(len(v.w2i), EMBED_DIM, NUM_CLASSES, NUM_HIDDEN, v)\n",
        "print(PTdeep_cbow_model)\n",
        "\n",
        "PTdeep_cbow_model = PTdeep_cbow_model.to(device)\n",
        "\n",
        "optimizer = optim.Adam(PTdeep_cbow_model.parameters(), lr=0.0005)\n",
        "PTdeep_cbow_losses, PTdeep_cbow_accuracies = train_model(\n",
        "    PTdeep_cbow_model, optimizer, num_iterations=10000, \n",
        "    print_every=1000, eval_every=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ufujv3x31ufD"
      },
      "source": [
        "# plot dev accuracies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTJtKBzd7Qjr"
      },
      "source": [
        "# plot train loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFu8xzCy9XDW"
      },
      "source": [
        "**It looks like we've hit what is possible with just using words.**\n",
        "Let's move on by incorporating word order!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g41yW4PL9jG0"
      },
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODzXEH0MaGpa"
      },
      "source": [
        "It is time to get more serious. Even with pre-trained word embeddings and multiple layers, we still seem to do pretty badly at sentiment classification. \n",
        "The next step we can take is to introduce word order again, dropping our independence assumptions. In this way, we can get a representation of the sentence as an ordered set of tokens.\n",
        "\n",
        "We will get this representation using a **Long Short-Term Memory** (LSTM). As an exercise, we will code our own LSTM cell, so that we get comfortable with its inner workings.\n",
        "Once we have an LSTM cell, we can call it repeatedly, updating its hidden state one word at a time:\n",
        "\n",
        "```python\n",
        "rnn = MyLSTMCell(input_size, hidden_size)\n",
        "\n",
        "hx = torch.zeros(1, hidden_size)  # initial hidden state\n",
        "cx = torch.zeros(1, hidden_size)  # initial memory cell\n",
        "output = []                       # to save intermediate LSTM states\n",
        "\n",
        "# feed one word at a time\n",
        "for i in range(n_timesteps):\n",
        "  hx, cx = rnn(input[i], (hx, cx))\n",
        "  output.append(hx)\n",
        "```\n",
        "\n",
        "If you need some more help understanding LSTMs, you can check out these resources:\n",
        "- Blog post (highly recommended): http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
        "- Paper covering LSTM formulas in detail: https://arxiv.org/abs/1503.04069 \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9f4b45BXKFC"
      },
      "source": [
        "#### Exercise: Finish the LSTM cell below. \n",
        "You will need to implement the LSTM formulas:\n",
        "\n",
        "$$\n",
        "\\begin{array}{ll}\n",
        "        i = \\sigma(W_{ii} x + b_{ii} + W_{hi} h + b_{hi}) \\\\\n",
        "        f = \\sigma(W_{if} x + b_{if} + W_{hf} h + b_{hf}) \\\\\n",
        "        g = \\tanh(W_{ig} x + b_{ig} + W_{hg} h + b_{hg}) \\\\\n",
        "        o = \\sigma(W_{io} x + b_{io} + W_{ho} h + b_{ho}) \\\\\n",
        "        c' = f * c + i * g \\\\\n",
        "        h' = o \\tanh(c') \\\\\n",
        "\\end{array}\n",
        " $$\n",
        "\n",
        "where $\\sigma$ is the sigmoid function.\n",
        "\n",
        "*Note that the LSTM formulas can differ slightly between different papers. We use the PyTorch LSTM formulation here.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJ9m5kLMd7-v"
      },
      "source": [
        "class MyLSTMCell(nn.Module):\n",
        "    \"\"\"Our own LSTM cell\"\"\"\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, bias=True):\n",
        "        \"\"\"Creates the weights for this LSTM\"\"\"\n",
        "        super(MyLSTMCell, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.bias = bias\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        self.i = nn.Linear(input_size+hidden_size, hidden_size)\n",
        "        self.f = nn.Linear(input_size+hidden_size, hidden_size)\n",
        "        self.g = nn.Linear(input_size+hidden_size, hidden_size)\n",
        "        self.o = nn.Linear(input_size+hidden_size, hidden_size)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        \"\"\"This is PyTorch's default initialization method\"\"\"\n",
        "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
        "        for weight in self.parameters():\n",
        "            weight.data.uniform_(-stdv, stdv)  \n",
        "\n",
        "    def forward(self, input_, hx, mask=None):\n",
        "        \"\"\"\n",
        "        input is (batch, input_size)\n",
        "        hx is ((batch, hidden_size), (batch, hidden_size))\n",
        "        \"\"\"\n",
        "        prev_h, prev_c = hx\n",
        "\n",
        "        # project input and prev state\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        #???\n",
        "        \n",
        "        # stack hidden and input\n",
        "        x_stack = torch.cat((prev_h, input_), dim=1)\n",
        "        \n",
        "        # main LSTM computation    \n",
        "        i = torch.sigmoid(self.i(x_stack))\n",
        "        f = torch.sigmoid(self.f(x_stack))\n",
        "        o = torch.sigmoid(self.o(x_stack))\n",
        "        g = torch.tanh(self.g(x_stack))\n",
        "        \n",
        "        c = f * prev_c + i * g\n",
        "        h = torch.tanh(c) * o\n",
        "\n",
        "        return h, c\n",
        "  \n",
        "    def __repr__(self):\n",
        "        return \"{}({:d}, {:d})\".format(\n",
        "            self.__class__.__name__, self.input_size, self.hidden_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JM7xPhkQeE5"
      },
      "source": [
        "#### Optional: Efficient Matrix Multiplication\n",
        "\n",
        "It is more efficient to do a few big matrix multiplications than to do many smaller ones. So we will implement the above cell using just **two** linear layers.\n",
        "\n",
        "This is possible because the eight linear transformations contained in one forward pass through an LSTM cell can be reduced to just two:\n",
        "$$W_h h + b_h$$\n",
        "$$W_i x + b_i $$ \n",
        "\n",
        "with $h = $ `prev_h` and $x = $ `input_`.\n",
        "\n",
        "and where: \n",
        "\n",
        "$W_h =  \\begin{pmatrix}\n",
        "W_{hi}\\\\ \n",
        "W_{hf}\\\\ \n",
        "W_{hg}\\\\ \n",
        "W_{ho}\n",
        "\\end{pmatrix}$, $b_h = \\begin{pmatrix}\n",
        "b_{hi}\\\\ \n",
        "b_{hf}\\\\ \n",
        "b_{hg}\\\\ \n",
        "b_{ho}\n",
        "\\end{pmatrix}$,  $W_i = \\begin{pmatrix}\n",
        "W_{ii}\\\\ \n",
        "W_{if}\\\\ \n",
        "W_{ig}\\\\ \n",
        "W_{io}\n",
        "\\end{pmatrix}$ and $b_i = \\begin{pmatrix}\n",
        "b_{ii}\\\\ \n",
        "b_{if}\\\\ \n",
        "b_{ig}\\\\ \n",
        "b_{io}\n",
        "\\end{pmatrix}$.\n",
        "\n",
        "Convince yourself that, after chunking with [torch.chunk](https://pytorch.org/docs/stable/torch.html?highlight=chunk#torch.chunk), the output of those two linear transformations is equivalent to the output of the eight linear transformations in the LSTM cell calculations above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9gA-UcqSBe0"
      },
      "source": [
        "#### LSTM Classifier\n",
        "\n",
        "Having an LSTM cell is not enough: we still need some code that calls it repeatedly, and then makes a prediction from the final hidden state. \n",
        "You will find that code below. Make sure that you understand it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iuYZm5poEn5"
      },
      "source": [
        "class LSTMClassifier(nn.Module):\n",
        "    \"\"\"Encodes sentence with an LSTM and projects final hidden state\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, vocab):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.vocab = vocab\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embed = nn.Embedding(vocab_size, embedding_dim, padding_idx=1)\n",
        "        self.rnn = MyLSTMCell(embedding_dim, hidden_dim)\n",
        "\n",
        "        self.output_layer = nn.Sequential(     \n",
        "            nn.Dropout(p=0.5),  # explained later\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "    \n",
        "        B = x.size(0)  # batch size (this is 1 for now, i.e. 1 single example)\n",
        "        T = x.size(1)  # timesteps (the number of words in the sentence)\n",
        "\n",
        "        input_ = self.embed(x)\n",
        "\n",
        "        # here we create initial hidden states containing zeros\n",
        "        # we use a trick here so that, if input is on the GPU, then so are hx and cx\n",
        "        hx = input_.new_zeros(B, self.rnn.hidden_size)\n",
        "        cx = input_.new_zeros(B, self.rnn.hidden_size)\n",
        "\n",
        "        # process input sentences one word/timestep at a time\n",
        "        # input is batch-major (i.e., batch size is the first dimension)\n",
        "        # so the first word(s) is (are) input_[:, 0]\n",
        "        outputs = []   \n",
        "        for i in range(T):\n",
        "            hx, cx = self.rnn(input_[:, i], (hx, cx))\n",
        "            outputs.append(hx)\n",
        "    \n",
        "        # if we have a single example, our final LSTM state is the last hx\n",
        "        if B == 1:\n",
        "            final = hx\n",
        "        else:\n",
        "            #\n",
        "            # This part is explained in next section, ignore this else-block for now.\n",
        "            #\n",
        "            # We processed sentences with different lengths, so some of the sentences\n",
        "            # had already finished and we have been adding padding inputs to hx.\n",
        "            # We select the final state based on the length of each sentence.\n",
        "\n",
        "            # two lines below not needed if using LSTM from pytorch\n",
        "            outputs = torch.stack(outputs, dim=0)           # [T, B, D]\n",
        "            outputs = outputs.transpose(0, 1).contiguous()  # [B, T, D]\n",
        "\n",
        "            # to be super-sure we're not accidentally indexing the wrong state\n",
        "            # we zero out positions that are invalid\n",
        "            pad_positions = (x == 1).unsqueeze(-1)\n",
        "\n",
        "            outputs = outputs.contiguous()      \n",
        "            outputs = outputs.masked_fill_(pad_positions, 0.)\n",
        "\n",
        "            mask = (x != 1)  # true for valid positions [B, T]\n",
        "            lengths = mask.sum(dim=1)                 # [B, 1]\n",
        "\n",
        "            indexes = (lengths - 1) + torch.arange(B, device=x.device, dtype=x.dtype) * T\n",
        "            final = outputs.view(-1, self.hidden_dim)[indexes]  # [B, D]\n",
        "    \n",
        "        # we use the last hidden state to classify the sentence\n",
        "        logits = self.output_layer(final)\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxFoVpvMPB6g"
      },
      "source": [
        "#### Dropout\n",
        "\n",
        "Data sparsity and a small data set can cause *overfitting*. This is a phenomenom that is very likely to occur when training strong and expressive models, like LSTMs, on small data. In practice, if your model overfits, this means that it will be very good at predicting (or \"remembering\") the sentiment of the training set, but unable to generalise to new, unseen data in the test set. This is undesirable and one technique to mitigate this issue is *dropout*. \n",
        "\n",
        "A dropout layer is defined by the following formula, which can be applied, for example, to a linear layer:\n",
        "\n",
        "$$\\text{tanh}(W(\\mathbf{h}\\odot \\mathbf{d}) + \\mathbf{b})$$\n",
        "\n",
        "where $\\mathbf{d} \\in \\{0, 1\\}^n$, with $d_j \\sim \\text{Bernoulli}(p)$, \n",
        "\n",
        "These formula simply means that we *drop* certain parameters during training (by setting them to zero). Which parameters we drop is stochastically determined by a Bernoulli distribution and the probability of each parameter being dropped is set to $p = 0.5$ in our experiments (see the previous cell of code where we define our output layer). A dropout layer can be applied at many different places in our models. This technique helps against the undesirable effect that a model relies on single parameters for prediction (e.g. if $h^{\\prime}_j$ is large, always predict positive). If we use dropout, the model needs to learn to rely on different parameters, which is desirable to obtain better generalisation to unseen data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQjEjLt9z0XW"
      },
      "source": [
        "**Let's train our LSTM!** Note that is will be a lot slower than previous models because we need to do many more computations per sentence.\n",
        "\n",
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgZoSPD4fsf_"
      },
      "source": [
        "lstm_model = LSTMClassifier(len(v.w2i), 300, 168, len(t2i), v)\n",
        "\n",
        "# copy pre-trained word vectors into embeddings table\n",
        "with torch.no_grad():\n",
        "    lstm_model.embed.weight.data.copy_(torch.from_numpy(vectors))\n",
        "    lstm_model.embed.weight.requires_grad = False\n",
        "\n",
        "print(lstm_model)\n",
        "print_parameters(lstm_model)\n",
        "\n",
        "lstm_model = lstm_model.to(device)\n",
        "optimizer = optim.Adam(lstm_model.parameters(), lr=3e-4)\n",
        "\n",
        "lstm_losses, lstm_accuracies = train_model(\n",
        "    lstm_model, optimizer, num_iterations=25000, \n",
        "    print_every=250, eval_every=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BKVnyg0Hq5E"
      },
      "source": [
        "# plot validation accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZowTV0EBTb3z"
      },
      "source": [
        "# plot training loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEw6XHQY_AAQ"
      },
      "source": [
        "# Mini-batching\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPf96wGzBTQJ"
      },
      "source": [
        "**Why is the LSTM so slow?** Despite our best efforts, we still need to make a lot of matrix multiplications per example (linear in the length of the example) just to get a single classification, and we can only process the 2nd word once we have computed the hidden state for the 1st word (sequential computation).\n",
        "\n",
        "GPUs are more efficient if we do a few big matrix multiplications, rather than lots of small ones. If we could process multiple examples at the same time, then we could exploit that. That is, we could still process the input sequentially, but doing so for multiple sentences at the same time.\n",
        "\n",
        "Up to now our \"mini-batches\" consisted of a single example. This was for a reason: the sentences in our data sets have **different lengths**, and this makes it difficult to process them at the same time.\n",
        "\n",
        "Consider a batch of 2 sentences:\n",
        "\n",
        "```\n",
        "this movie is bad\n",
        "this movie is super cool !\n",
        "```\n",
        "\n",
        "Let's say the IDs for these sentences are:\n",
        "\n",
        "```\n",
        "2 3 4 5\n",
        "2 3 4 6 7 8\n",
        "```\n",
        "\n",
        "We cannot feed PyTorch an object with rows of variable length! We need to turn this into a matrix.\n",
        "\n",
        "The solution is to add **padding values** to our mini-batch:\n",
        "\n",
        "```\n",
        "2 3 4 5 1 1\n",
        "2 3 4 6 7 8\n",
        "```\n",
        "\n",
        "Whenever a sentence is shorter than the longest sentence in a mini-batch, we just use a padding value (here: 1) to fill the matrix.\n",
        "\n",
        "In our computation, we should **ignore** the padding positions (e.g. mask them out) so that paddings do not contribute to the loss.\n",
        "\n",
        "#### Mini-batch feed\n",
        "We will now implement a `get_minibatch` function which will replace `get_example` and returns a mini-batch of the requested size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoAE2JBiXJ3P"
      },
      "source": [
        "def get_minibatch(data, batch_size=25, shuffle=True):\n",
        "    \"\"\"Return minibatches, optional shuffling\"\"\"\n",
        "  \n",
        "    if shuffle:\n",
        "        print(\"Shuffling training data\")\n",
        "        random.shuffle(data)  # shuffle training data each epoch\n",
        "  \n",
        "    batch = []\n",
        "  \n",
        "    # yield minibatches\n",
        "    for example in data:\n",
        "        batch.append(example)\n",
        "    \n",
        "        if len(batch) == batch_size:\n",
        "            yield batch\n",
        "            batch = []\n",
        "      \n",
        "    # in case there is something left\n",
        "    if len(batch) > 0:\n",
        "        yield batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwZM-XYkT8Zx"
      },
      "source": [
        "#### Padding function\n",
        "We will need a function that adds padding 1s to a sequence of IDs so that\n",
        "it becomes as long as the longest sequence in the minibatch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sp0sK1ghw4Ft"
      },
      "source": [
        "def pad(tokens, length, pad_value=1):\n",
        "    \"\"\"add padding 1s to a sequence to that it has the desired length\"\"\"\n",
        "    return tokens + [pad_value] * (length - len(tokens))\n",
        "\n",
        "# example\n",
        "tokens = [2, 3, 4]\n",
        "pad(tokens, 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SL2iixMYUgfh"
      },
      "source": [
        "#### New `prepare` function\n",
        "\n",
        "We will also need a new function that turns a mini-batch into PyTorch tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZID0cqozWks8"
      },
      "source": [
        "def prepare_minibatch(mb, vocab):\n",
        "    \"\"\"\n",
        "    Minibatch is a list of examples.\n",
        "    This function converts words to IDs and returns\n",
        "    torch tensors to be used as input/targets.\n",
        "    \"\"\"\n",
        "    batch_size = len(mb)\n",
        "    maxlen = max([len(ex.tokens) for ex in mb])\n",
        "\n",
        "    # vocab returns 0 if the word is not there\n",
        "    x = [pad([vocab.w2i.get(t, 0) for t in ex.tokens], maxlen) for ex in mb]\n",
        "\n",
        "    x = torch.LongTensor(x)\n",
        "    x = x.to(device)\n",
        "\n",
        "    y = [ex.label for ex in mb]\n",
        "    y = torch.LongTensor(y)\n",
        "    y = y.to(device)\n",
        "\n",
        "    return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwDAtCv1x2hB"
      },
      "source": [
        "# Let's test our new function.\n",
        "# This should give us 3 examples.\n",
        "mb = next(get_minibatch(train_data, batch_size=3, shuffle=False))\n",
        "for ex in mb:\n",
        "    print(ex)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dg8zEK8zyUCH"
      },
      "source": [
        "# We should find padding 1s at the end\n",
        "x, y = prepare_minibatch(mb, v)\n",
        "print(\"x\", x)\n",
        "print(\"y\", y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYBJEoSNUwI0"
      },
      "source": [
        "#### Evaluate (mini-batch version)\n",
        "\n",
        "We can now update our evaluation function to use mini-batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiZZpEghzqou"
      },
      "source": [
        "def evaluate(model, data, \n",
        "             batch_fn=get_minibatch, prep_fn=prepare_minibatch,\n",
        "             batch_size=16):\n",
        "    \"\"\"Accuracy of a model on given data set (using mini-batches)\"\"\"\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.eval()  # disable dropout\n",
        "\n",
        "    for mb in batch_fn(data, batch_size=batch_size, shuffle=False):\n",
        "        x, targets = prep_fn(mb, model.vocab)\n",
        "        with torch.no_grad():\n",
        "            logits = model(x)\n",
        "      \n",
        "        predictions = logits.argmax(dim=-1).view(-1)\n",
        "    \n",
        "        # add the number of correct predictions to the total correct\n",
        "        correct += (predictions == targets.view(-1)).sum().item()\n",
        "        total += targets.size(0)\n",
        "\n",
        "    return correct, total, correct / float(total)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23wAZomozh_2"
      },
      "source": [
        "# LSTM (Mini-batched)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-gkPU7jzBe2"
      },
      "source": [
        "With this, let's run the LSTM again but now using mini-batches!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "226Xg9OPzFbA"
      },
      "source": [
        "lstm_model = LSTMClassifier(\n",
        "    len(v.w2i), 300, 168, len(t2i), v)\n",
        "\n",
        "# copy pre-trained vectors into embeddings table\n",
        "with torch.no_grad():\n",
        "    lstm_model.embed.weight.data.copy_(torch.from_numpy(vectors))\n",
        "    lstm_model.embed.weight.requires_grad = False\n",
        "\n",
        "print(lstm_model)\n",
        "print_parameters(lstm_model)  \n",
        "  \n",
        "lstm_model = lstm_model.to(device)\n",
        "\n",
        "batch_size = 25\n",
        "optimizer = optim.Adam(lstm_model.parameters(), lr=2e-4)\n",
        "\n",
        "lstm_losses, lstm_accuracies = train_model(\n",
        "    lstm_model, optimizer, num_iterations=30000, \n",
        "    print_every=250, eval_every=250,\n",
        "    batch_size=batch_size,\n",
        "    batch_fn=get_minibatch, \n",
        "    prep_fn=prepare_minibatch,\n",
        "    eval_fn=evaluate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymj1rLDMvyhp"
      },
      "source": [
        "# plot validation accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1je5S1RHVC5R"
      },
      "source": [
        "# plot training loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7WjcxXntMi5"
      },
      "source": [
        "# Tree-LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyj_UD6GtO5M"
      },
      "source": [
        "In the final part of this lab we will exploit the tree structure of the SST data. \n",
        "Until now we only used the surface tokens, but remember that our data examples include binary trees with a sentiment score at every node.\n",
        "\n",
        "In particular, we will implement **N-ary Tree-LSTMs** which are described in:\n",
        "\n",
        "> Kai Sheng Tai, Richard Socher, and Christopher D. Manning. [Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks](http://aclweb.org/anthology/P/P15/P15-1150.pdf) ACL 2015.\n",
        "\n",
        "Since our trees are binary (i.e., N=2), we can refer to these as *Binary Tree-LSTMs*. If you study equations (9) to (14) in the paper, you will find that they are not all too different from the original LSTM that you already have.\n",
        "\n",
        "You should read this paper carefully and make sure that you understand the approach. You will also find our LSTM baseline there.\n",
        "Note however that Tree-LSTMs were proposed around the same time by two other groups:\n",
        "\n",
        "> Phong Le and Willem Zuidema. [Compositional distributional semantics with long short term memory](http://anthology.aclweb.org/S/S15/S15-1002.pdf). *SEM 2015.\n",
        "\n",
        "> Xiaodan Zhu, Parinaz Sobihani,  and Hongyu Guo. [Long short-term memory over recursive structures](http://proceedings.mlr.press/v37/zhub15.pdf). ICML 2015.\n",
        "\n",
        "It is good scientific practice to cite all three papers in your report.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rDzvSos3JFp"
      },
      "source": [
        "## Computation\n",
        "\n",
        "Do you remember the `transitions_from_treestring` function all the way in the beginning of this lab? Every example contains a **transition sequence** produced by this function. Let's look at it again:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pg0Xumc3ZUS"
      },
      "source": [
        "ex = next(examplereader(\"trees/dev.txt\"))\n",
        "print(TreePrettyPrinter(ex.tree))\n",
        "print(\"Transitions:\")\n",
        "print(ex.transitions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceBFe9fU4BI_"
      },
      "source": [
        "Note that the tree is **binary**. Every node has two children, except for pre-terminal nodes.\n",
        "\n",
        "A tree like this can be described by a sequence of **SHIFT (0)** and **REDUCE (1)** actions.\n",
        "\n",
        "To construct a tree, we can use the transitions as follows:\n",
        "- **reverse** the sentence (a list of tokens) and call this the **buffer**\n",
        "   - the first word is now on top (last in the list), and we would get it when calling pop() on the buffer\n",
        "- create an empty list and call it the **stack**\n",
        "- iterate through the transition sequence:\n",
        "  - if it says SHIFT(0), we pop a word from the buffer, and push it to the stack\n",
        "  - if it says REDUCE(1), we pop the **top two items** from the stack, and combine them (e.g. with a Tree-LSTM!), creating a new node that we push back on the stack\n",
        "  \n",
        "Convince yourself that going through the transition sequence above will result in the tree that you see.\n",
        "For example, we would start by putting the following words on the stack (by shifting 5 times, starting with `It`):\n",
        "\n",
        "```\n",
        "Top of the stack:\n",
        "-----------------\n",
        "film\n",
        "lovely\n",
        "a \n",
        "'s  \n",
        "It\n",
        "```\n",
        "Now we find a REDUCE in the transition sequence, so we get the top two words (film and lovely), and combine them, so our new stack becomes:\n",
        "```\n",
        "Top of the stack:\n",
        "-----------------\n",
        "lovely film\n",
        "a \n",
        "'s  \n",
        "It\n",
        "```\n",
        "\n",
        "We will use this approach when encoding sentences with our Tree-LSTM.\n",
        "Now, our sentence is a reversed list of word embeddings.\n",
        "When we shift, we move a word embedding to the stack.\n",
        "When we reduce, we apply the Tree-LSTM to the top two vectors, and the result is a single vector that we put back on the stack.\n",
        "After going through the whole transition sequence, we will have the root node on our stack! We can use that to classify the sentence.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDWKShm1AfmR"
      },
      "source": [
        "## Obtaining the transition sequence\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fO7VKWVpAbWj"
      },
      "source": [
        "\n",
        "So what goes on in the `transitions_from_treestring` function?\n",
        "\n",
        "The idea ([explained in this blog post](https://devblogs.nvidia.com/recursive-neural-networks-pytorch/)) is that, if we had a tree, we could traverse through the tree, and every time that we find a node containing only a word, we output a SHIFT.\n",
        "Every time **after** we have finished visiting the children of a node, we output a REDUCE.\n",
        "(What is this tree traversal called?)\n",
        "\n",
        "However, our `transitions_from_treestring` function operates directly on the string representation. It works as follows.\n",
        "\n",
        "We start with the representation:\n",
        "\n",
        "```\n",
        "(3 (2 It) (4 (4 (2 's) (4 (3 (2 a) (4 (3 lovely) (2 film))) (3 (2 with) (4 (3 (3 lovely) (2 performances)) (2 (2 by) (2 (2 (2 Buy) (2 and)) (2 Accorsi))))))) (2 .)))\n",
        "```\n",
        "\n",
        "First we remove pre-terminal nodes (and add spaces before closing brackets):\n",
        "\n",
        "```\n",
        "(3 It (4 (4 's (4 (3 a (4 lovely film ) ) (3 with (4 (3 lovely performances ) (2 by (2 (2 Buy and )  Accorsi ) ) ) ) ) ) . ) )\n",
        "```\n",
        "\n",
        "Then we remove node labels:\n",
        "\n",
        "```\n",
        "( It ( ( 's ( ( a ( lovely film ) ) ( with ( ( lovely performances) ( by ( ( Buy and )  Accorsi ) ) ) ) ) ) . ) )\n",
        "```\n",
        "\n",
        "Then we remove opening brackets:\n",
        "\n",
        "```\n",
        "It 's a lovely film ) ) with lovely performances ) by Buy and ) Accorsi ) ) ) ) ) ) . ) )\n",
        "```\n",
        "\n",
        "Now we replace words by S (for SHIFT), and closing brackets by R (for REDUCE):\n",
        "\n",
        "```\n",
        "S S S S S R R S S S R S S S R S R R R R R R S R R\n",
        "0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1 \n",
        "```\n",
        "\n",
        "Et voilà. We just obtained the transition sequence!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1y069gM4_v64"
      },
      "source": [
        "# for comparison\n",
        "seq = ex.transitions\n",
        "s = \" \".join([\"S\" if t == 0 else \"R\" for t in seq])\n",
        "print(s)\n",
        "print(\" \".join(map(str, seq)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-qOuKbDAiBn"
      },
      "source": [
        "## Coding the Tree-LSTM\n",
        "\n",
        "The code below contains a Binary Tree-LSTM cell.\n",
        "It is used in the TreeLSTM class below it, which in turn is used in the TreeLSTMClassifier.\n",
        "The job of the TreeLSTM class is to encode a complete sentence and return the root node.\n",
        "The job of the TreeLSTMCell is to return a new state when provided with two children (a reduce action). By repeatedly calling the TreeLSTMCell, the TreeLSTM will encode a sentence. This can be done for multiple sentences at the same time.\n",
        "\n",
        "\n",
        "#### Exercise \n",
        "Check the `forward` function and complete the Tree-LSTM formulas.\n",
        "You can see that we defined a large linear layer for you, that projects the *concatenation* of the left and right child into the input gate, left forget gate, right forget gate, candidate, and output gate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9b9mjMlN7Pb"
      },
      "source": [
        "class TreeLSTMCell(nn.Module):\n",
        "    \"\"\"A Binary Tree LSTM cell\"\"\"\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, bias=True):\n",
        "        \"\"\"Creates the weights for this LSTM\"\"\"\n",
        "        super(TreeLSTMCell, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.bias = bias\n",
        "\n",
        "        self.reduce_layer = nn.Linear(2 * hidden_size, 5 * hidden_size)\n",
        "        self.dropout_layer = nn.Dropout(p=0.25)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        \"\"\"This is PyTorch's default initialization method\"\"\"\n",
        "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
        "        for weight in self.parameters():\n",
        "            weight.data.uniform_(-stdv, stdv)  \n",
        "\n",
        "    def forward(self, hx_l, hx_r, mask=None):\n",
        "        \"\"\"\n",
        "        hx_l is ((batch, hidden_size), (batch, hidden_size))\n",
        "        hx_r is ((batch, hidden_size), (batch, hidden_size))    \n",
        "        \"\"\"\n",
        "        prev_h_l, prev_c_l = hx_l  # left child\n",
        "        prev_h_r, prev_c_r = hx_r  # right child\n",
        "\n",
        "        B = prev_h_l.size(0)\n",
        "\n",
        "        # we concatenate the left and right children\n",
        "        # you can also project from them separately and then sum\n",
        "        children = torch.cat([prev_h_l, prev_h_r], dim=1)\n",
        "\n",
        "        # project the combined children into a 5D tensor for i,fl,fr,g,o\n",
        "        # this is done for speed, and you could also do it separately\n",
        "        proj = self.reduce_layer(children)  # shape: B x 5D\n",
        "\n",
        "        # each shape: B x D\n",
        "        i, f_l, f_r, g, o = torch.chunk(proj, 5, dim=-1)\n",
        "\n",
        "        # main Tree LSTM computation\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        # You only need to complete the commented lines below.\n",
        "        \n",
        "        # WHAT TO DO WITH THE DROPOUT LAYER?\n",
        "        # in LSTMClassifier this was only applied after the LSTM Cell\n",
        "        # should we use dropout here after all operations, or only on h and c ?\n",
        "        \n",
        "        # The shape of each of these is [batch_size, hidden_size]\n",
        "        i = torch.sigmoid(i)\n",
        "        f_l = torch.sigmoid(f_l)\n",
        "        f_r = torch.sigmoid(f_r)\n",
        "        g = torch.tanh(g)\n",
        "        o = torch.sigmoid(o)\n",
        "        \n",
        "        c = i * g + f_r * prev_c_r + f_l * prev_c_l\n",
        "        h = o * torch.tanh(c)\n",
        "        \n",
        "        return h, c\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"{}({:d}, {:d})\".format(\n",
        "                self.__class__.__name__, self.input_size, self.hidden_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dj5dYSGh_643"
      },
      "source": [
        "## Explanation of the TreeLSTM class\n",
        "\n",
        "\n",
        "The code below contains the TreeLSTM class, which implements everything we need in order to encode a sentence from word embeddings. The calculations are the same as in the paper, implemented such that the class `TreeLSTMCell` above is as general as possible and only takes two children to reduce them into a parent. \n",
        "\n",
        "\n",
        "**Initialize $\\mathbf{h}$ and $\\mathbf{c}$ outside of the cell for the leaves**\n",
        "\n",
        "At the leaves of each tree the children nodes are **empty**, whereas in higher levels the nodes are binary tree nodes that *do* have a left and right child (but no input $x$). By initializing the leaf nodes outside of the cell class (`TreeLSTMCell`), we avoid if-else statements in the forward pass.\n",
        "\n",
        "The `TreeLSTM` class (among other things) pre-calculates an initial $h$ and $c$ for every word in the sentence. Since the initial left and right child are 0, the only calculations we need to do are based on $x$, and we can drop the forget gate calculation (`prev_c_l` and `prev_c_r` are zero). The calculations we do in order to initalize $h$ and $c$ are then:\n",
        "\n",
        "$$\n",
        "c_1 =  W^{(u)}x_1 \\\\\n",
        "o_1 = \\sigma (W^{(i)}x_1) \\\\\n",
        "h_1 = o_1 \\odot \\text{tanh}(c_1)$$\n",
        "*NB: note that these equations are chosen as initializations of $c$ and $h$, other initializations are possible and might work equally well.*\n",
        "\n",
        "**Sentence Representations**\n",
        "\n",
        "All our leaf nodes are now initialized, so we can start processing the sentence in its tree form. Each sentence is represented by a buffer (initially a list with a concatenation of $[h_1, c_1]$ for every word in the reversed sentence), a stack (initially an empty list) and a transition sequence. To encode our sentence, we construct the tree from its transition sequence as explained earlier. \n",
        "\n",
        "*A short example that constructs a tree:*\n",
        "\n",
        "We loop over the time dimension of the batched transition sequences (i.e. row by row), which contain values of 0's, 1's and 2's (representing SHIFT, REDUCE and padding respectively). If we have a batch of size 2 where the first example has a transition sequence given by [0, 0, 1, 0, 0, 0, 1] and the second by [0, 0, 1, 0, 0, 1], our transition batch will be given by the following two-dimensional numpy array:\n",
        "\n",
        "$$\n",
        "\\text{transitions} = \n",
        "\\begin{pmatrix}\n",
        "0 & 0\\\\ \n",
        "0 & 0\\\\ \n",
        "1 & 1\\\\ \n",
        "0 & 0\\\\ \n",
        "0 & 0\\\\ \n",
        "0 & 1\\\\ \n",
        "1 & 2\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "The inner loop (`for transition, buffer, stack in zip(t_batch, buffers, stacks)`) goes over each example in the batch and updates its buffer and stack. The nested loop for this example will then do roughy the following:\n",
        "\n",
        "```\n",
        "Time = 0:  t_batch = [0, 0], the inner loop performs 2 SHIFTs. \n",
        "\n",
        "Time = 1:  t_batch = [0, 0], \"..\"\n",
        "\n",
        "Time = 2:  t_batch = [1, 1], causing the inner loop to fill the list child_l and child_r for both examples in the batch. Now the statement if child_l will return True, triggering a REDUCE action to be performed by our Tree LSTM cell with a batch size of 2. \n",
        "\n",
        "Time = 3:  t_batch = [0, 0], \"..\".\n",
        "\n",
        "Time = 4:  t_batch = [0, 0], \"..\"\n",
        "\n",
        "Time = 5:  t_batch = [0, 1], one SHIFT will be done and another REDUCE action will be performed by our Tree LSTM, this time of batch size 1.  \n",
        "\n",
        "Time = 6:  t_batch = [1, 2], triggering another REDUCE action with batch size 1.\n",
        "```\n",
        "*NB: note that this was an artificial example for the purpose of demonstrating parts of the code, the transition sequences do not necessarily represent actual trees.*\n",
        "\n",
        "**Batching and Unbatching**\n",
        "\n",
        "Within the body of the outer loop over time, we use the functions for batching and unbatching. \n",
        "\n",
        "*Batching*\n",
        "\n",
        "Before passing two lists of children to the reduce layer (an instance of `TreeLSTMCell`), we batch the children as they are at this point a list of tensors of variable length based on how many REDUCE actions there are to perform at a certain time step across the batch (let's call the length `L`). To do an efficient forward pass we want to transform the list to a pair of tensors of shape `([L, D], [L, D])`, which the function `batch` achieves. \n",
        "\n",
        "*Unbatching*\n",
        "\n",
        "In the same line where we batched the children, we unbatch the output of the forward pass to become a list of states of length `L` again. We do this because we need to loop over each example's transition at the current time step and push the children that are reduced into a parent to the stack.\n",
        "\n",
        "*The batch and unbatch functions let us switch between the \"PyTorch world\" (Tensors) and the Python world (easy to manipulate lists).*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PixvTd4AqsQ"
      },
      "source": [
        "# Helper functions for batching and unbatching states\n",
        "# For speed we want to combine computations by batching, but \n",
        "# for processing logic we want to turn the output into lists again\n",
        "# to easily manipulate.\n",
        "\n",
        "def batch(states):\n",
        "  \"\"\"\n",
        "  Turns a list of states into a single tensor for fast processing. \n",
        "  This function also chunks (splits) each state into a (h, c) pair\"\"\"\n",
        "  return torch.cat(states, 0).chunk(2, 1)\n",
        "\n",
        "def unbatch(state):\n",
        "  \"\"\"\n",
        "  Turns a tensor back into a list of states.\n",
        "  First, (h, c) are merged into a single state.\n",
        "  Then the result is split into a list of sentences.\n",
        "  \"\"\"\n",
        "  return torch.split(torch.cat(state, 1), 1, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CynltDasaLPt"
      },
      "source": [
        "Take some time to understand the class below, having read the explanation above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQOqMXG4gX5G"
      },
      "source": [
        "class TreeLSTM(nn.Module):\n",
        "  \"\"\"Encodes a sentence using a TreeLSTMCell\"\"\"\n",
        "\n",
        "  def __init__(self, input_size, hidden_size, bias=True):\n",
        "    \"\"\"Creates the weights for this LSTM\"\"\"\n",
        "    super(TreeLSTM, self).__init__()\n",
        "\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.bias = bias\n",
        "    self.reduce = TreeLSTMCell(input_size, hidden_size)\n",
        "\n",
        "    # project word to initial c\n",
        "    self.proj_x = nn.Linear(input_size, hidden_size)\n",
        "    self.proj_x_gate = nn.Linear(input_size, hidden_size)\n",
        "    \n",
        "    self.buffers_dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "  def forward(self, x, transitions):\n",
        "    \"\"\"\n",
        "    WARNING: assuming x is reversed!\n",
        "    :param x: word embeddings [B, T, E]\n",
        "    :param transitions: [2T-1, B]\n",
        "    :return: root states\n",
        "    \"\"\"\n",
        "\n",
        "    B = x.size(0)  # batch size\n",
        "    T = x.size(1)  # time\n",
        "\n",
        "    # compute an initial c and h for each word\n",
        "    # Note: this corresponds to input x in the Tai et al. Tree LSTM paper.\n",
        "    # We do not handle input x in the TreeLSTMCell itself.\n",
        "    buffers_c = self.proj_x(x)\n",
        "    buffers_h = buffers_c.tanh()\n",
        "    buffers_h_gate = self.proj_x_gate(x).sigmoid()\n",
        "    buffers_h = buffers_h_gate * buffers_h\n",
        "    \n",
        "    # concatenate h and c for each word\n",
        "    buffers = torch.cat([buffers_h, buffers_c], dim=-1)\n",
        "\n",
        "    D = buffers.size(-1) // 2\n",
        "\n",
        "    # we turn buffers into a list of stacks (1 stack for each sentence)\n",
        "    # first we split buffers so that it is a list of sentences (length B)\n",
        "    # then we split each sentence to be a list of word vectors\n",
        "    buffers = buffers.split(1, dim=0)  # Bx[T, 2D]\n",
        "    buffers = [list(b.squeeze(0).split(1, dim=0)) for b in buffers]  # BxTx[2D]\n",
        "\n",
        "    # create B empty stacks\n",
        "    stacks = [[] for _ in buffers]\n",
        "\n",
        "    # t_batch holds 1 transition for each sentence\n",
        "    for t_batch in transitions:\n",
        "\n",
        "      child_l = []  # contains the left child for each sentence with reduce action\n",
        "      child_r = []  # contains the corresponding right child\n",
        "\n",
        "      # iterate over sentences in the batch\n",
        "      # each has a transition t, a buffer and a stack\n",
        "      for transition, buffer, stack in zip(t_batch, buffers, stacks):\n",
        "        if transition == SHIFT:\n",
        "          stack.append(buffer.pop())\n",
        "        elif transition == REDUCE:\n",
        "          assert len(stack) >= 2, \\\n",
        "            \"Stack too small! Should not happen with valid transition sequences\"\n",
        "          child_r.append(stack.pop())  # right child is on top\n",
        "          child_l.append(stack.pop())\n",
        "\n",
        "      # if there are sentences with reduce transition, perform them batched\n",
        "      if child_l:\n",
        "        reduced = iter(unbatch(self.reduce(batch(child_l), batch(child_r))))\n",
        "        for transition, stack in zip(t_batch, stacks):\n",
        "          if transition == REDUCE:\n",
        "            stack.append(next(reduced))\n",
        "\n",
        "    final = [stack.pop().chunk(2, -1)[0] for stack in stacks]\n",
        "    final = torch.cat(final, dim=0)  # tensor [B, D]\n",
        "\n",
        "    return final"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4EzbVzqaXkw"
      },
      "source": [
        "Just like the LSTM before, we will need an extra class that does the classifications."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLxpYRvtQKge"
      },
      "source": [
        "class TreeLSTMClassifier(nn.Module):\n",
        "  \"\"\"Encodes sentence with a TreeLSTM and projects final hidden state\"\"\"\n",
        "\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, vocab):\n",
        "    super(TreeLSTMClassifier, self).__init__()\n",
        "    self.vocab = vocab\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.embed = nn.Embedding(vocab_size, embedding_dim, padding_idx=1)\n",
        "    self.treelstm = TreeLSTM(embedding_dim, hidden_dim)\n",
        "    self.output_layer = nn.Sequential(     \n",
        "        nn.Dropout(p=0.5),\n",
        "        nn.Linear(hidden_dim, output_dim, bias=True)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    \n",
        "    # x is a pair here of words and transitions; we unpack it here.\n",
        "    # x is batch-major: [B, T], transitions is time major [2T-1, B]\n",
        "    x, transitions = x\n",
        "    emb = self.embed(x)\n",
        "    \n",
        "    # we use the root/top state of the Tree LSTM to classify the sentence\n",
        "    root_states = self.treelstm(emb, transitions)\n",
        "\n",
        "    # we use the last hidden state to classify the sentence\n",
        "    logits = self.output_layer(root_states)\n",
        "    return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gh9RbhGwaiLg"
      },
      "source": [
        "## Special `prepare` function for Tree-LSTM\n",
        "\n",
        "We need yet another `prepare` function. For our implementation, sentences need to be *reversed*. We will do that here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiqH-_2xdm9H"
      },
      "source": [
        "def prepare_treelstm_minibatch(mb, vocab):\n",
        "  \"\"\"\n",
        "  Returns sentences reversed (last word first)\n",
        "  Returns transitions together with the sentences.  \n",
        "  \"\"\"\n",
        "  batch_size = len(mb)\n",
        "  maxlen = max([len(ex.tokens) for ex in mb])\n",
        "    \n",
        "  # vocab returns 0 if the word is not there\n",
        "  # NOTE: reversed sequence!\n",
        "  x = [pad([vocab.w2i.get(t, 0) for t in ex.tokens], maxlen)[::-1] for ex in mb]\n",
        "  \n",
        "  x = torch.LongTensor(x)\n",
        "  x = x.to(device)\n",
        "  \n",
        "  y = [ex.label for ex in mb]\n",
        "  y = torch.LongTensor(y)\n",
        "  y = y.to(device)\n",
        "  \n",
        "  maxlen_t = max([len(ex.transitions) for ex in mb])\n",
        "  transitions = [pad(ex.transitions, maxlen_t, pad_value=2) for ex in mb]\n",
        "  transitions = np.array(transitions)\n",
        "  transitions = transitions.T  # time-major\n",
        "  \n",
        "  return (x, transitions), y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMUsrlL9ayVe"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpOYUdg2D3v0"
      },
      "source": [
        "# Now let's train the Tree LSTM!\n",
        "\n",
        "tree_model = TreeLSTMClassifier(\n",
        "    len(v.w2i), 300, 150, len(t2i), v)\n",
        "\n",
        "with torch.no_grad():\n",
        "  tree_model.embed.weight.data.copy_(torch.from_numpy(vectors))\n",
        "  tree_model.embed.weight.requires_grad = False\n",
        "  \n",
        "def do_train(model):\n",
        "  \n",
        "  print(model)\n",
        "  print_parameters(model)\n",
        "\n",
        "  model = model.to(device)\n",
        "\n",
        "  optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
        "  \n",
        "  return train_model(\n",
        "      model, optimizer, num_iterations=30000, \n",
        "      print_every=250, eval_every=250,\n",
        "      prep_fn=prepare_treelstm_minibatch,\n",
        "      eval_fn=evaluate,\n",
        "      batch_fn=get_minibatch,\n",
        "      batch_size=25, eval_batch_size=25)\n",
        "  \n",
        "results = do_train(tree_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHcHHaLtguUg"
      },
      "source": [
        "# plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7QZZH86eHqu"
      },
      "source": [
        "# Further experiments and report\n",
        "\n",
        "For your report, you are expected to answer research questions by doing further experiments.\n",
        "\n",
        "## Research Questions\n",
        "\n",
        "Make sure you cover at least the following:\n",
        "\n",
        "- How important is word order for this task?\n",
        "- Does the tree structure help to get a better accuracy?\n",
        "- How does performance depend on the sentence length? Compare the various models. Is there a model that does better on longer sentences? If so, why?\n",
        "- Do you get better performance if you supervise the sentiment **at each node in the tree**? You can extract more training examples by treating every node in each tree as a separate tree. You will need to write a function that extracts all subtrees given a treestring. \n",
        "    - Warning: NLTK's Tree function seems to result in invalid trees in some cases, so be careful if you want to parse the string to a tree structure before extraction the phrases.\n",
        "\n",
        "**To obtain a full grade, you should conduct further investigations.** For example, you can also investigate the following:\n",
        "\n",
        "- When making a wrong prediction, can you figure out at what point in the tree (sentence) the model fails? You can make a prediction at each node to investigate.\n",
        "- How does N-ary Tree LSTM compare to the Child-Sum Tree LSTM? \n",
        "- How do the Tai et al. Tree LSTMs compare to Le & Zuidema's formulation?\n",
        "- Or... your own research question!\n",
        "\n",
        "In general:\n",
        "\n",
        "- ***When you report numbers, please report the mean accuracy across 3 (or more) runs with different random seed, together with the standard deviation.*** This is because the final performance may vary per random seed. \n",
        "More precisely, you should run each model with 3 different seeds, and for each of these 3 runs, evaluate the best model (according to the validation) on the test dataset. The validation dataset is used for finding the best model over iterations, but the accuracy you report should be on the test dataset.\n",
        "\n",
        "## Report instructions\n",
        "\n",
        "Your report needs to be written in LaTeX. You are required to use the ACL 2020 template which you can download from or edit directly on [Overleaf](https://www.overleaf.com/latex/templates/instructions-for-acl-2018-proceedings/xzmhqgnmkppc). Make sure your names and student numbers are visible at the top. (Tip: you need to uncomment `\\aclfinalcopy`).\n",
        "You can find some general tips about writing a research paper [here](https://www.microsoft.com/en-us/research/academic-program/write-great-research-paper/), but note that you need to make your own judgment about what is appropriate for this project. \n",
        "\n",
        "We expect you to use the following structure:\n",
        "1. Introduction (~1 page) - describe the problem, your research questions and goals, a summary of your findings and contributions. Please cite related work (models, data set) as part of your introduction here, since this is a short paper.\n",
        "    - Introduce the task and the main goal\n",
        "    - Clear research questions\n",
        "    - Motivating the importance of the questions and explaining the expectations\n",
        "    - How are these addressed or not addressed in the literature\n",
        "    - What is your approach\n",
        "    - Short summary of your findings\n",
        "2. Background (~1/2-1 page) -\n",
        "cover the main techniques (\"building blocks\") used in your project (e.g. word embeddings, LSTM, Tree-LSTM) and intuitions behind them. Be accurate and concise.\n",
        "    - How each technique that you use works (don't just copy the formulas)\n",
        "    - The relation between the techniques\n",
        "3. Models (~1/2 page) - Cover the models that you used.\n",
        "    - The architecture of the final models (How do you use LSTM or Tree-LSTM for the sentiment classification task? What layers do you have, how do you do classification? What is your loss function?)\n",
        "4. Experiments (~1/2 page) - Describe your experimental setup. The information here should allow someone else to reproduce your experiments. Describe how you evaluate the models.\n",
        "    - Explain the task and the data\n",
        "    - Training the models (model, data, parameters and hyper parameters if the models, training algorithms, what supervision signals you use, etc.)\n",
        "    - Evaluation (e.g. metrics)\n",
        "5. Results and Analysis (~1 page). Go over the results and analyse your findings.\n",
        "    - Answer each of the research questions you raised in the introduction.\n",
        "    - Plots and figures highlighting interesting patterns\n",
        "    - What are the factors that make model A better than model B in task C? Investigate to prove their effect!\n",
        "6. Conclusion (~1/4 page). The main conclusions of your experiments.\n",
        "    - What have you learned from you experiments? How does it relate to what is already known in the literature?\n",
        "    - Were the results as expected? Any surprising results? Why?\n",
        "    - Based on what you learned what would you suggest doing next?\n",
        "\n",
        "\n",
        "General Tips:\n",
        "\n",
        "- Math notation – define each variable (either in running text, or in a pseudo-legenda after or before the equation).\n",
        "- Define technical terminology you need.\n",
        "- Avoid colloquial language – everything can be said in a scientific-sounding way.\n",
        "- Avoid lengthy sentences, stay to the point.\n",
        "- Do not spend space on \"obvious\" things.\n",
        "- Do not go over the page limit. (We will deduct points for that.)\n",
        "- The page limit is 4 pages excluding references and appendix. There is no strict limit to references and appendix. However, the report needs to remain fully self-contained: the appendix should only include content that is not necessary to understand your work. For example, preprocessing decisions, model parameters, pseudocode, sample system inputs/outputs, and other details that are necessary for the exact replication of your work can be put into the appendix. However, \n",
        "\n",
        "\n",
        "An ideal report:\n",
        "- Precise, scientific-sounding, technical, to the point \n",
        "  - Little general “waffle”/chit-chat\n",
        "- Not boring – because you don’t explain obvious things too much\n",
        "- Efficient delivery of (only) the facts that we need to know to understand/reimplement\n",
        "- Results visually well-presented and described with the correct priority of importance of sub-results\n",
        "- Insightful analysis – speculation should connect to something interesting and not be too much; the reader “learns something new”\n",
        "- No typos, no colloquialisms – well-considered language\n",
        "- This normally means several re-draftings (re-orderings of information)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCINIXV1q1oe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}